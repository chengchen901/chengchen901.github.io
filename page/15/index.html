<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta property="og:type" content="website">
<meta property="og:title" content="Hexo">
<meta property="og:url" content="http://example.com/page/15/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/typeface-source-code-pro@0.0.71/index.min.css">

  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
  
<meta name="generator" content="Hexo 6.3.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hexo</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"><span class="fa fa-bars"></span></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
        
          <a class="nav-icon" href="/atom.xml" title="RSS Feed"><span class="fa fa-rss"></span></a>
        
        <a class="nav-icon nav-search-btn" title="Search"><span class="fa fa-search"></span></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://example.com"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main">
  
    <article id="post-08-消息队列/03-Kafka/01-Kafka入门" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2020/11/29/08-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/03-Kafka/01-Kafka%E5%85%A5%E9%97%A8/" class="article-date">
  <time class="dt-published" datetime="2020-11-29T07:40:00.000Z" itemprop="datePublished">2020-11-29</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Kafka/">Kafka</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2020/11/29/08-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/03-Kafka/01-Kafka%E5%85%A5%E9%97%A8/">Kafka入门</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>使用版本：2.3</p>
<h1 id="学习目标"><a href="#学习目标" class="headerlink" title="学习目标"></a>学习目标</h1><ol>
<li>掌握kafka是什么，主要用途是什么，了解kafka的特性</li>
<li>掌握kafka集群安装</li>
<li>掌握kafka核心概念、工作原理</li>
<li>掌握kafka的使用</li>
</ol>
<h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><p>官网：<a target="_blank" rel="noopener" href="http://kafka.apache.org/">http://kafka.apache.org/</a></p>
<p>中文文档介绍：<a target="_blank" rel="noopener" href="https://kafka.apachecn.org/">https://kafka.apachecn.org/</a></p>
<h2 id="Kafka是什么？"><a href="#Kafka是什么？" class="headerlink" title="Kafka是什么？"></a>Kafka是什么？</h2><ul>
<li>一个分布式的流式数据处理平台。</li>
<li>可以用它来发布和订阅流式的记录。这一方面与消息队列或者企业消息系统类似</li>
<li>它将流式的数据安全地存储在分布式、有副本备份、容错的集群上</li>
<li>可以用来做流式计算</li>
</ul>
<h2 id="Kafka适用于什么场景？"><a href="#Kafka适用于什么场景？" class="headerlink" title="Kafka适用于什么场景？"></a>Kafka适用于什么场景？</h2><p>它可以用于两大类别的应用: </p>
<ol>
<li>构造实时流数据管道，它可以在系统或应用之间可靠地获取数据。 (相当于message queue)</li>
<li>构建实时流式应用程序，对这些流数据进行转换或者影响。 (就是流处理，通过kafka stream topic和topic之间内部进行变化)</li>
</ol>
<h2 id="Kafka的架构体系"><a href="#Kafka的架构体系" class="headerlink" title="Kafka的架构体系"></a>Kafka的架构体系</h2><p><strong>分布式集群</strong></p>
<ul>
<li>Kafka作为一个集群，运行在一台或者多台服务器上.</li>
<li>Kafka 通过 topic 对存储的流数据进行分类。</li>
</ul>
<p><strong>Kafka有四个核心的API：</strong></p>
<ul>
<li>The <a target="_blank" rel="noopener" href="https://kafka.apachecn.org/documentation.html#producerapi">Producer API</a> 允许一个应用程序发布一串流式的数据到一个或者多个Kafka topic。</li>
<li>The <a target="_blank" rel="noopener" href="https://kafka.apachecn.org/documentation.html#consumerapi">Consumer API</a> 允许一个应用程序订阅一个或多个 topic ，并且对发布给他们的流式数据进行处理。</li>
<li>The <a target="_blank" rel="noopener" href="https://kafka.apachecn.org/documentation/streams">Streams API</a> 允许一个应用程序作为一个<em>流处理器</em>，消费一个或者多个topic产生的输入流，然后生产一个输出流到一个或多个topic中去，在输入输出流中进行有效的转换。</li>
<li>The <a target="_blank" rel="noopener" href="https://kafka.apachecn.org/documentation.html#connect">Connector API</a> 允许构建并运行可重用的生产者或者消费者，将Kafka topics连接到已存在的应用程序或者数据系统。比如，连接到一个关系型数据库，捕捉表（table）的所有变更内容。</li>
</ul>
<p>在Kafka中，客户端和服务器使用一个简单、高性能、支持多语言的 <a target="_blank" rel="noopener" href="https://kafka.apache.org/protocol.html">TCP 协议</a>.此协议版本化并且向下兼容老版本， 我们为Kafka提供了Java客户端，也支持许多<a target="_blank" rel="noopener" href="https://cwiki.apache.org/confluence/display/KAFKA/Clients">其他语言的客户端</a>。</p>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211030144959.png"></p>
<h1 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h1><h2 id="环境要求"><a href="#环境要求" class="headerlink" title="环境要求"></a>环境要求</h2><ul>
<li>生产环境，强烈要求 linux ，学习可以windows</li>
<li>java1.8 或以上</li>
</ul>
<h2 id="安装-1"><a href="#安装-1" class="headerlink" title="安装"></a>安装</h2><ol>
<li><p>下载安装包：<a target="_blank" rel="noopener" href="https://www.apache.org/dyn/closer.cgi?path=/kafka/2.3.0/kafka_2.12-2.3.0.tgz">https://www.apache.org/dyn/closer.cgi?path=/kafka/2.3.0/kafka_2.12-2.3.0.tgz</a></p>
<p>windows 和 linux都是同一个安装包，window命令在 bin&#x2F;windows&#x2F; 下。</p>
</li>
<li><p>安装</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mkdir /usr/kafka</span><br><span class="line">tar -xzf kafka_2.12-2.3.0.tgz -C /usr/kafka/</span><br><span class="line">ln -s /usr/kafka/kafka_2.12-2.3.0 /usr/kafka/latest</span><br></pre></td></tr></table></figure>
</li>
<li><p>了解目录结构</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">ll /usr/kafka/latest</span><br><span class="line"></span><br><span class="line">drwxr-xr-x. 3 root root  4096 6月  20 2019 bin</span><br><span class="line">drwxr-xr-x. 2 root root  4096 12月  6 2020 config</span><br><span class="line">drwxr-xr-x. 2 root root  4096 12月  6 2020 libs</span><br><span class="line">-rw-r--r--. 1 root root 32216 6月  20 2019 LICENSE</span><br><span class="line">drwxr-xr-x. 2 root root  4096 12月  6 2020 logs</span><br><span class="line">-rw-r--r--. 1 root root   337 6月  20 2019 NOTICE</span><br><span class="line">drwxr-xr-x. 2 root root    44 6月  20 2019 site-docs</span><br></pre></td></tr></table></figure>

<p>了解 bin 、config 、libs 下都有些什么。</p>
</li>
<li><p>开启zookeeper服务</p>
<p>Kafka集群使用zookeeper来存储元信息，在生产环境下你应该独立安装一个zookeeper集群。学习可以使用Kafka安装包中内置的zookeeper，启动一个单实例的zookeeper</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /usr/kafka/latest/</span><br><span class="line">bin/zookeeper-server-start.sh config/zookeeper.properties &amp;</span><br></pre></td></tr></table></figure>
</li>
<li><p>配置Kafka，kafka的配置文件 config&#x2F;server.properties</p>
<p>要掌握的基本配置项：</p>
<figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Licensed to the Apache Software Foundation (ASF) under one or more</span></span><br><span class="line"><span class="comment"># contributor license agreements.  See the NOTICE file distributed with</span></span><br><span class="line"><span class="comment"># this work for additional information regarding copyright ownership.</span></span><br><span class="line"><span class="comment"># The ASF licenses this file to You under the Apache License, Version 2.0</span></span><br><span class="line"><span class="comment"># (the &quot;License&quot;); you may not use this file except in compliance with</span></span><br><span class="line"><span class="comment"># the License.  You may obtain a copy of the License at</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment">#    http://www.apache.org/licenses/LICENSE-2.0</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># Unless required by applicable law or agreed to in writing, software</span></span><br><span class="line"><span class="comment"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span></span><br><span class="line"><span class="comment"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span></span><br><span class="line"><span class="comment"># See the License for the specific language governing permissions and</span></span><br><span class="line"><span class="comment"># limitations under the License.</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment"># see kafka.server.KafkaConfig for additional details and defaults</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">############################# Server Basics #############################</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment"># The id of the broker. This must be set to a unique integer for each broker.</span></span><br><span class="line"><span class="attr">broker.id</span>=<span class="string">0	# 集群中每个broker的唯一整数 id</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">############################# Socket Server Settings #############################</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment"># The address the socket server listens on. It will get the value returned from </span></span><br><span class="line"><span class="comment"># java.net.InetAddress.getCanonicalHostName() if not configured.</span></span><br><span class="line"><span class="comment">#   FORMAT:</span></span><br><span class="line"><span class="comment">#     listeners = listener_name://host_name:port</span></span><br><span class="line"><span class="comment">#   EXAMPLE:</span></span><br><span class="line"><span class="comment">#     listeners = PLAINTEXT://your.host.name:9092</span></span><br><span class="line"><span class="comment"># 服务端口，默认9092</span></span><br><span class="line"><span class="comment">#listeners==PLAINTEXT://:9092</span></span><br><span class="line"><span class="comment"># 这里ip修改成自己的ip</span></span><br><span class="line"><span class="attr">listeners</span>=<span class="string">PLAINTEXT://192.168.254.159:9092</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment"># Hostname and port the broker will advertise to producers and consumers. If not set, </span></span><br><span class="line"><span class="comment"># it uses the value for &quot;listeners&quot; if configured.  Otherwise, it will use the value</span></span><br><span class="line"><span class="comment"># returned from java.net.InetAddress.getCanonicalHostName().</span></span><br><span class="line"><span class="comment"># broker发布自己的地址给生产者和消费用，不配做则使用配置的listeners值，如果没有配置listeners则取主机名。这里建议配置IP，否则客户端通过主机名连接可能连不通。</span></span><br><span class="line"><span class="comment">#advertised.listeners=PLAINTEXT://your.host.name:9092</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment"># Maps listener names to security protocols, the default is for them to be the same. See the config documentation for more details</span></span><br><span class="line"><span class="comment">#listener.security.protocol.map=PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment"># The number of threads that the server uses for receiving requests from the network and sending responses to the network</span></span><br><span class="line"><span class="attr">num.network.threads</span>=<span class="string">3</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment"># The number of threads that the server uses for processing requests, which may include disk I/O</span></span><br><span class="line"><span class="attr">num.io.threads</span>=<span class="string">8</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment"># The send buffer (SO_SNDBUF) used by the socket server</span></span><br><span class="line"><span class="attr">socket.send.buffer.bytes</span>=<span class="string">102400</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment"># The receive buffer (SO_RCVBUF) used by the socket server</span></span><br><span class="line"><span class="attr">socket.receive.buffer.bytes</span>=<span class="string">102400</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment"># The maximum size of a request that the socket server will accept (protection against OOM)</span></span><br><span class="line"><span class="attr">socket.request.max.bytes</span>=<span class="string">104857600</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">############################# Log Basics #############################</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment"># A comma separated list of directories under which to store log files</span></span><br><span class="line"><span class="attr">log.dirs</span>=<span class="string">/usr/kafka/kafka-logs	# 数据存储目录，逗号间隔的多个目录，一定要修改为你想要存 放的目录。</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment"># The default number of log partitions per topic. More partitions allow greater</span></span><br><span class="line"><span class="comment"># parallelism for consumption, but this will also result in more files across</span></span><br><span class="line"><span class="comment"># the brokers.</span></span><br><span class="line"><span class="comment"># 当创建Topic(主题)未指定分区数时的默认分区数，</span></span><br><span class="line"><span class="attr">num.partitions</span>=<span class="string">1</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment"># The number of threads per data directory to be used for log recovery at startup and flushing at shutdown.</span></span><br><span class="line"><span class="comment"># This value is recommended to be increased for installations with data dirs located in RAID array.</span></span><br><span class="line"><span class="attr">num.recovery.threads.per.data.dir</span>=<span class="string">1</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">############################# Internal Topic Settings  #############################</span></span><br><span class="line"><span class="comment"># The replication factor for the group metadata internal topics &quot;__consumer_offsets&quot; and &quot;__transaction_state&quot;</span></span><br><span class="line"><span class="comment"># For anything other than development testing, a value greater than 1 is recommended for to ensure availability such as 3.</span></span><br><span class="line"><span class="attr">offsets.topic.replication.factor</span>=<span class="string">1</span></span><br><span class="line"><span class="attr">transaction.state.log.replication.factor</span>=<span class="string">1</span></span><br><span class="line"><span class="attr">transaction.state.log.min.isr</span>=<span class="string">1</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">############################# Log Flush Policy #############################</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment"># Messages are immediately written to the filesystem but by default we only fsync() to sync</span></span><br><span class="line"><span class="comment"># the OS cache lazily. The following configurations control the flush of data to disk.</span></span><br><span class="line"><span class="comment"># There are a few important trade-offs here:</span></span><br><span class="line"><span class="comment">#    1. Durability: Unflushed data may be lost if you are not using replication.</span></span><br><span class="line"><span class="comment">#    2. Latency: Very large flush intervals may lead to latency spikes when the flush does occur as there will be a lot of data to flush.</span></span><br><span class="line"><span class="comment">#    3. Throughput: The flush is generally the most expensive operation, and a small flush interval may lead to excessive seeks.</span></span><br><span class="line"><span class="comment"># The settings below allow one to configure the flush policy to flush data after a period of time or</span></span><br><span class="line"><span class="comment"># every N messages (or both). This can be done globally and overridden on a per-topic basis.</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment"># The number of messages to accept before forcing a flush of data to disk</span></span><br><span class="line"><span class="comment">#log.flush.interval.messages=10000</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment"># The maximum amount of time a message can sit in a log before we force a flush</span></span><br><span class="line"><span class="comment">#log.flush.interval.ms=1000</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">############################# Log Retention Policy #############################</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment"># The following configurations control the disposal of log segments. The policy can</span></span><br><span class="line"><span class="comment"># be set to delete segments after a period of time, or after a given size has accumulated.</span></span><br><span class="line"><span class="comment"># A segment will be deleted whenever *either* of these criteria are met. Deletion always happens</span></span><br><span class="line"><span class="comment"># from the end of the log.</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment"># The minimum age of a log file to be eligible for deletion due to age</span></span><br><span class="line"><span class="attr">log.retention.hours</span>=<span class="string">168</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment"># A size-based retention policy for logs. Segments are pruned from the log unless the remaining</span></span><br><span class="line"><span class="comment"># segments drop below log.retention.bytes. Functions independently of log.retention.hours.</span></span><br><span class="line"><span class="comment">#log.retention.bytes=1073741824</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment"># The maximum size of a log segment file. When this size is reached a new log segment will be created.</span></span><br><span class="line"><span class="attr">log.segment.bytes</span>=<span class="string">1073741824</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment"># The interval at which log segments are checked to see if they can be deleted according</span></span><br><span class="line"><span class="comment"># to the retention policies</span></span><br><span class="line"><span class="attr">log.retention.check.interval.ms</span>=<span class="string">300000</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">############################# Zookeeper #############################</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment"># Zookeeper connection string (see zookeeper docs for details).</span></span><br><span class="line"><span class="comment"># This is a comma separated host:port pairs, each corresponding to a zk</span></span><br><span class="line"><span class="comment"># server. e.g. &quot;127.0.0.1:3000,127.0.0.1:3001,127.0.0.1:3002&quot;.</span></span><br><span class="line"><span class="comment"># You can also append an optional chroot string to the urls to specify the</span></span><br><span class="line"><span class="comment"># root directory for all kafka znodes.</span></span><br><span class="line"><span class="attr">zookeeper.connect</span>=<span class="string">localhost:2181</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment"># Timeout in ms for connecting to zookeeper</span></span><br><span class="line"><span class="attr">zookeeper.connection.timeout.ms</span>=<span class="string">6000</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">############################# Group Coordinator Settings #############################</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment"># The following configuration specifies the time, in milliseconds, that the GroupCoordinator will delay the initial consumer rebalance.</span></span><br><span class="line"><span class="comment"># The rebalance will be further delayed by the value of group.initial.rebalance.delay.ms as new members join the group, up to a maximum of max.poll.interval.ms.</span></span><br><span class="line"><span class="comment"># The default value for this is 3 seconds.</span></span><br><span class="line"><span class="comment"># We override this to 0 here as it makes for a better out-of-the-box experience for development and testing.</span></span><br><span class="line"><span class="comment"># However, in production environments the default value of 3 seconds is more suitable as this will help to avoid unnecessary, and potentially expensive, rebalances during application startup.</span></span><br><span class="line"><span class="comment"># 消费组的重平衡延时（单位毫秒），【注意】生产环境请恢复为默认值3秒，或根据实际需要增大</span></span><br><span class="line"><span class="attr">group.initial.rebalance.delay.ms</span>=<span class="string">0</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>启动Kafka broker 实例</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-server-start.sh config/server.properties &amp;</span><br></pre></td></tr></table></figure>

<p>接下来我们可以通过zookeeper客户端去查看kafka集群信息</p>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211030150619.png"></p>
</li>
</ol>
<h2 id="集群搭建"><a href="#集群搭建" class="headerlink" title="集群搭建"></a>集群搭建</h2><p>【生产集群】在其他机器上同样安装kafka，配置它们连接到同一个zookeeper集群、它们的唯一id，数据目录，启动Broker实例即加入集群。</p>
<p>【学习用集群】在同一台机器上启动多个broker实例，按如下步骤操作来搭建一个3节点的集群：</p>
<h3 id="学习用集群搭建"><a href="#学习用集群搭建" class="headerlink" title="学习用集群搭建"></a>学习用集群搭建</h3><p>拷贝配置文件</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cp config/server.properties config/server-1.properties</span><br><span class="line">cp config/server.properties config/server-2.properties</span><br></pre></td></tr></table></figure>

<p>修改配置文件：</p>
<p>config&#x2F;server-1.properties</p>
<figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">broker.id</span>=<span class="string">1 </span></span><br><span class="line"><span class="comment"># 这里ip修改成自己的ip</span></span><br><span class="line"><span class="attr">listeners</span>=<span class="string">PLAINTEXT://192.168.254.159:9093</span></span><br><span class="line"><span class="attr">log.dirs</span>=<span class="string">/usr/kafka/kafka-logs-1</span></span><br></pre></td></tr></table></figure>

<p>config&#x2F;server-2.properties</p>
<figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">broker.id</span>=<span class="string">2 </span></span><br><span class="line"><span class="comment"># 这里ip修改成自己的ip</span></span><br><span class="line"><span class="attr">listeners</span>=<span class="string">PLAINTEXT://192.168.254.159:9094</span></span><br><span class="line"><span class="attr">log.dirs</span>=<span class="string">/usr/kafka/kafka-logs-2</span></span><br></pre></td></tr></table></figure>

<p>启动这两个broker实例</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-server-start.sh config/server-1.properties &amp;</span><br><span class="line">bin/kafka-server-start.sh config/server-2.properties &amp;</span><br></pre></td></tr></table></figure>

<p><strong>【启动失败说明】</strong> 如果启动第二个或第三个broker时提示内存不够用，可以做如下调整：</p>
<p>1、调大你的虚拟机的内存（1G 或更多）</p>
<p>2、调小<strong>Kafka</strong>的堆大小，默认是<strong>1G</strong>，生产用时可以调大。这里学习用可以调为256M（不能太小</p>
<p>了，启动时会heap OOM)</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vi bin/kafka-server-start.sh</span><br></pre></td></tr></table></figure>

<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211030151536.png"></p>
<p>启动好后，可以查看启动的java进程，将看到3个Kafka，一个zookeeper</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">jps</span><br><span class="line"></span><br><span class="line">2324 QuorumPeerMain</span><br><span class="line">3335 Kafka</span><br><span class="line">2633 Kafka</span><br><span class="line">2985 Kafka</span><br><span class="line">3710 Jps</span><br></pre></td></tr></table></figure>

<p>再看看zookeeper上的kafka集群信息：<br><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211030151744.png"></p>
<p>可以玩了，创建一个只有1个分片，3个备份的Topic</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-topics.sh --create --bootstrap-server 192.168.254.159:9092 --replication-factor 3 --partitions 1 --topic my-replicated-topic</span><br></pre></td></tr></table></figure>

<p>查看主题信息：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost latest]# bin/kafka-topics.sh --describe --bootstrap-server 192.168.254.159:9092 --topic my-replicated-topic</span><br><span class="line">Topic:my-replicated-topic	PartitionCount:1	ReplicationFactor:3	Configs:segment.bytes=1073741824</span><br><span class="line">	Topic: my-replicated-topic	Partition: 0	Leader: 1	Replicas: 1,2,0	Isr: 1,2,0</span><br></pre></td></tr></table></figure>

<p>也可在zookeeper客户端上看：</p>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211030152359.png"></p>
<p><strong>【掌握】状态信息的含义</strong></p>
<ul>
<li><p>“leader” is the node responsible for all reads and writes for the given partition. Each node will be the leader for a randomly selected portion of the partitions.</p>
</li>
<li><p>“replicas” is the list of nodes that replicate the log for this partition regardless of whether they are the leader or even if they are currently alive.</p>
</li>
<li><p>“isr” is the set of “in-sync” replicas. This is the subset of the replicas list that is currently alive and caught-up to the leader.</p>
</li>
<li><p>leader是负责指定分区所有读写的节点。每个节点将是随机选择的分区部分的领导者。</p>
</li>
<li><p>“replicas”是复制该分区日志的节点列表，无论它们是leader还是当前处于活动状态。</p>
</li>
<li><p>“isr”是“同步”副本的集合。这是当前活动的副本列表的子集，并赶上了leader。</p>
</li>
</ul>
<ol>
<li><p>现在让我们用Kafka安装包中提供的客户端程序来发布消息：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost latest]# bin/kafka-console-producer.sh --broker-list 192.168.254.159:9092 --topic my-replicated-topic</span><br><span class="line">...</span><br><span class="line"><span class="meta prompt_">&gt;</span><span class="language-bash">my <span class="built_in">test</span> message 1</span> </span><br><span class="line"><span class="meta prompt_">&gt;</span><span class="language-bash">my <span class="built_in">test</span> message 2</span> </span><br><span class="line"><span class="meta prompt_">&gt;</span><span class="language-bash">^C</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>来消费消息</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost latest]# bin/kafka-console-consumer.sh --bootstrap-server 192.168.254.159:9092 --from-beginning --topic my-replicated-topic</span><br><span class="line">...</span><br><span class="line">my test message 1 </span><br><span class="line">my test message 2</span><br><span class="line">^C</span><br></pre></td></tr></table></figure>
</li>
<li><p>怎么停止Broker</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost latest]# bin/kafka-server-stop.sh</span><br></pre></td></tr></table></figure>

<p>这会把我们刚才启动的三个broker都停掉。</p>
</li>
<li><p>让我们来测试一下集群容错，现在主题 my-replicated-topic 的唯一分片的leader备份是1号节点，我们把1号broker停掉看看，找到它的进程号，直接kill。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost latest]# ps aux | grep server-1.properties</span><br><span class="line"> </span><br><span class="line">2633  1.4 18.9 2800140 352408 pts/0  Sl   15:16   0:15 /usr/jdk/latest//bin/java -Xmx256M -Xms256M -server...</span><br><span class="line"></span><br><span class="line">[root@localhost latest]# kill 2633</span><br></pre></td></tr></table></figure>

<p>再看看 主题 my-replicated-topic 的信息：【注意 不能连0号节点了】</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost latest]# bin/kafka-topics.sh --describe --bootstrap-server 192.168.254.159:9092 --topic my-replicated-topic</span><br><span class="line">Topic:my-replicated-topic	PartitionCount:1	ReplicationFactor:3	Configs:segment.bytes=1073741824</span><br><span class="line">	Topic: my-replicated-topic	Partition: 0	Leader: 2	Replicas: 1,2,0	Isr: 2,0</span><br></pre></td></tr></table></figure>

<p>集群搭建完成。</p>
</li>
</ol>
<h1 id="监控管理"><a href="#监控管理" class="headerlink" title="监控管理"></a><strong>监控管理</strong></h1><p>Kakfa自身未提供图形化的监控管理工具，市面上有很多开源的监控管理工具，但都不怎么成熟可靠。这里给介绍一款稍可靠的工具。</p>
<h2 id="Kafka-Offset-Monitor"><a href="#Kafka-Offset-Monitor" class="headerlink" title="Kafka Offset Monitor"></a>Kafka Offset Monitor</h2><p><a target="_blank" rel="noopener" href="https://github.com/sylar88/kafkaOffsetMonitor-0.3.0/blob/master/KafkaOffsetMonitor-assembly-0.3.0-SNAPSHOT.jar">https://github.com/sylar88/kafkaOffsetMonitor-0.3.0/blob/master/KafkaOffsetMonitor-assembly-0.3.0-SNAPSHOT.jar</a></p>
<p>可以实时监控：</p>
<ul>
<li>Kafka集群状态</li>
<li>Topic、Consumer Group列表</li>
<li>图形化展示topic和consumer之间的关系</li>
<li>图形化展示consumer的Offset、Lag等信息</li>
</ul>
<p>它是一个jar 包，使用很简单</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">java -cp KafkaOffsetMonitor-assembly-0.3.0-SNAPSHOT.jar \</span><br><span class="line">    com.quantifind.kafka.offsetapp.OffsetGetterWeb \</span><br><span class="line">    --offsetStorage kafka \</span><br><span class="line">    --zk 192.168.254.159:2181 \</span><br><span class="line">    --port 8080 \</span><br><span class="line">    --refresh 10.seconds \</span><br><span class="line">    --retain 2.days</span><br></pre></td></tr></table></figure>

<h2 id="0-2-0-版本启动命令"><a href="#0-2-0-版本启动命令" class="headerlink" title="0.2.0 版本启动命令"></a>0.2.0 版本启动命令</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">java -cp KafkaOffsetMonitor-assembly-0.2.0.jar \ </span><br><span class="line">    com.quantifind.kafka.offsetapp.OffsetGetterWeb \ </span><br><span class="line">    --zk zk-server1,zk-server2 \ </span><br><span class="line">    --port 8080 \</span><br><span class="line">    --refresh 10.seconds \ </span><br><span class="line">    --retain 2.days</span><br></pre></td></tr></table></figure>

<p>The arguments are:</p>
<ul>
<li><strong>offsetStorage</strong> valid options are ‘’zookeeper’’, ‘’kafka’’ or ‘’storm’’. Anything else falls back to ‘’zookeeper’’ 【说明】0.2.1版本才有这个参数</li>
<li><strong>zk</strong> the ZooKeeper hosts</li>
<li><strong>port</strong> on what port will the app be available</li>
<li><strong>refresh</strong> how often should the app refresh and store a point in the DB</li>
<li><strong>retain</strong> how long should points be kept in the DB</li>
<li><strong>dbName</strong> where to store the history (default ‘offsetapp’)</li>
<li><strong>kafkaOffsetForceFromStart</strong> only applies to ‘’kafka’’ format. Force KafkaOffsetMonitor to scan the commit messages from start (see notes below)</li>
<li><strong>stormZKOffsetBase</strong> only applies to ‘’storm’’ format. Change the offset storage base in zookeeper, default to ‘’&#x2F;stormconsumers’’ (see notes below)</li>
<li><strong>pluginsArgs</strong> additional arguments used by extensions (see below)</li>
</ul>
<p>启动后就可以在浏览器中访问了：<a target="_blank" rel="noopener" href="http://localhost:8080/">http://localhost:8080</a></p>
<h1 id="Spring-中使用"><a href="#Spring-中使用" class="headerlink" title="Spring 中使用"></a>Spring 中使用</h1><p>spring 官网学习文档：<a target="_blank" rel="noopener" href="https://docs.spring.io/spring-kafka/docs/2.2.8.RELEASE/reference/html/#introduction">https://docs.spring.io/spring-kafka/docs/2.2.8.RELEASE/reference/html/#introduction</a></p>
<h1 id="java客户端"><a href="#java客户端" class="headerlink" title="java客户端"></a>java客户端</h1><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span> </span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.kafka<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span> </span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>kafka-clients<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span> </span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.3.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span> </span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h2 id="核心API"><a href="#核心API" class="headerlink" title="核心API"></a>核心API</h2><h3 id="Topic及管理"><a href="#Topic及管理" class="headerlink" title="Topic及管理"></a>Topic及管理</h3><ul>
<li>AdminClient</li>
<li>NewTopic</li>
</ul>
<h3 id="Client"><a href="#Client" class="headerlink" title="Client"></a>Client</h3><ul>
<li>CommonClientConfigs</li>
</ul>
<h3 id="Producer"><a href="#Producer" class="headerlink" title="Producer"></a>Producer</h3><ul>
<li><p>KafkaProducer</p>
</li>
<li><p>ProducerRecord</p>
</li>
<li><p>ProducerConfifig</p>
</li>
<li><p>Serializer</p>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211031094613.png"></p>
</li>
</ul>
<h3 id="Consumer"><a href="#Consumer" class="headerlink" title="Consumer"></a>Consumer</h3><ul>
<li><p>KafkaConsumer</p>
</li>
<li><p>ConsumerConfifig</p>
</li>
<li><p>ConsumerRecord</p>
</li>
<li><p>Deserializer</p>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211031094838.png"></p>
</li>
</ul>
<h1 id="核心概念"><a href="#核心概念" class="headerlink" title="核心概念"></a>核心概念</h1><h2 id="Topic"><a href="#Topic" class="headerlink" title="Topic"></a>Topic</h2><h3 id="Topic说明"><a href="#Topic说明" class="headerlink" title="Topic说明"></a>Topic说明</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost latest]# bin/kafka-topics.sh --create --bootstrap-server 192.168.254.159:9092 --replication-factor 3 --partitions 3 --topic my-33-topic</span><br><span class="line"></span><br><span class="line">[root@localhost latest]# bin/kafka-topics.sh --describe --bootstrap-server 192.168.254.159:9092 --topic my-33-topic</span><br><span class="line">Topic:my-33-topic	PartitionCount:3	ReplicationFactor:3	Configs:segment.bytes=1073741824</span><br><span class="line">	Topic: my-33-topic	Partition: 0	Leader: 0	Replicas: 0,2,1	Isr: 0,2,1</span><br><span class="line">	Topic: my-33-topic	Partition: 1	Leader: 2	Replicas: 2,1,0	Isr: 2,1,0</span><br><span class="line">	Topic: my-33-topic	Partition: 2	Leader: 1	Replicas: 1,0,2	Isr: 1,0,2</span><br></pre></td></tr></table></figure>

<ul>
<li><strong>Topic</strong> ：主题 ，一类消息（数据）</li>
<li><strong>partition</strong>：一个Topic可以分成多个分片来分布式存放数据，分片以顺序号来编号。</li>
<li><strong>Replicas</strong>：为保证数据存储的可靠性，一个分片可以存储多个副本（一般3个），副本被自动均衡分布在集群节点上</li>
<li><strong>Leader</strong>：一个分片的多个副本中自动选举一个作为Leader，通过Leader操作数据，Leader同步给其他副本，以此来保证一致性。当Leader挂了时，自动选择一个做Leader。 </li>
<li>Topic的这些元信息存储在Zookeeper上。</li>
<li>Replicas: 0,2,1 副本在哪些broker上</li>
<li>isr: 0,2,1 副本 存活且同步的broker</li>
</ul>
<h3 id="leader选举"><a href="#leader选举" class="headerlink" title="leader选举"></a>leader选举</h3><p>【问题】每个分片的Leader如何产生？</p>
<p>怎么进行Leader选举？</p>
<p>zk leader 选举的原理是什么？</p>
<p>临时节点 + watch</p>
<p>因为惊群效应，Kafka没有直接使用zk来进行分片的Leader选举</p>
<p>Kafka中增加一个角色： Controller ， 由集群的一个broker来担任这个角色</p>
<p>谁来当Controller 怎么定？ 挂了怎么办？</p>
<p>这里就是真正用的zk 来进行 Controller的选举。</p>
<p>然后所有的 主题的分片的副本的分布、leader的选定都由Controller来完成。</p>
<p>controller_epoch：controller选举的次数</p>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211031095659.png"></p>
<h3 id="消息的分片选择"><a href="#消息的分片选择" class="headerlink" title="消息的分片选择"></a>消息的分片选择</h3><p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211031095829.png"></p>
<p>消息该发往哪个分片？</p>
<p><strong>Producer</strong>客户端负责消息的分发</p>
<ul>
<li>kafka集群中的任何一个broker都可以向producer提供metadata信息,这些metadata中包含”集群中存活的servers列表”&#x2F;”partitions leader列表”等信息；</li>
<li>当producer获取到metadata信息之后, producer将会和Topic下所有partition leader保持socket连接；</li>
<li>消息由producer直接通过socket发送到broker，中间不会经过任何”路由层”，事实上，消息被路由到哪个partition上由producer客户端决定；比如可以采用”random”“key-hash”“轮询”等,如果一个topic中有多个partitions，那么在producer端实现”消息均衡分发”是必要的。</li>
</ul>
<p><strong>消息的分片选择规则：</strong></p>
<ul>
<li>用户给定了分片号且正确有效，则发到给定分片；</li>
<li>未指定分片，指定了Key，则对Key取Hash 求余决定目标分片</li>
<li>未指定分片，也未提供key，则采用轮询</li>
</ul>
<p>ProducerRecord&lt;K, V&gt;</p>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211031100807.png"></p>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211031100901.png"></p>
<h3 id="分片数据持久化原理"><a href="#分片数据持久化原理" class="headerlink" title="分片数据持久化原理"></a>分片数据持久化原理</h3><p>【问题】 副本怎么存储消息数据？</p>
<p>Kafka 是采用文件来存储数据</p>
<p>数据量大</p>
<h4 id="磁盘文件组织方式"><a href="#磁盘文件组织方式" class="headerlink" title="磁盘文件组织方式"></a>磁盘文件组织方式</h4><p>Kafka是一个分布式的流数据存储平台，它<strong>将流数据以日志的方式顺序存储在磁盘文件</strong>中。数据文件的数据存放目录下的组织方式为：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">drwxr-xr-x. 2 root root 141 10月 31 09:53 my-33-topic-0</span><br><span class="line">drwxr-xr-x. 2 root root 141 10月 31 09:53 my-33-topic-1</span><br><span class="line">drwxr-xr-x. 2 root root 141 10月 31 09:53 my-33-topic-2</span><br><span class="line"></span><br><span class="line">[root@localhost kafka-logs]# ll my-33-topic-0</span><br><span class="line">总用量 4</span><br><span class="line">-rw-r--r--. 1 root root 10485760 10月 31 09:53 00000000000000000000.index</span><br><span class="line">-rw-r--r--. 1 root root        0 10月 31 09:53 00000000000000000000.log</span><br><span class="line">-rw-r--r--. 1 root root 10485756 10月 31 09:53 00000000000000000000.timeindex</span><br><span class="line">-rw-r--r--. 1 root root        8 10月 31 09:53 leader-epoch-checkpoint</span><br><span class="line"></span><br><span class="line">[root@localhost kafka-logs]# ll my-33-topic-1</span><br><span class="line">总用量 0</span><br><span class="line">-rw-r--r--. 1 root root 10485760 10月 31 09:53 00000000000000000000.index</span><br><span class="line">-rw-r--r--. 1 root root        0 10月 31 09:53 00000000000000000000.log</span><br><span class="line">-rw-r--r--. 1 root root 10485756 10月 31 09:53 00000000000000000000.timeindex</span><br><span class="line">-rw-r--r--. 1 root root        0 10月 31 09:53 leader-epoch-checkpoint</span><br></pre></td></tr></table></figure>

<p><strong>【说明】消息数据是顺序追加到.log文件中，这用写入速度非常快。</strong></p>
<p>像写日志一样，追加流数据。</p>
<p>文件的顺序写 远快于 随机写</p>
<p>【问题1】日志文件名 这串 00000000000000000000 表示什么意思？ 为什么这么命名？</p>
<p>【问题2】在这个日志文件中怎么存储数据？怎么知道一条消息的结尾。</p>
<p>存得都是字节</p>
<p>offset 偏移量 记录</p>
<p>4个字节（消息内容的长度） + 消息内容（字节序列）</p>
<h4 id="日志文件数据存储格式"><a href="#日志文件数据存储格式" class="headerlink" title="日志文件数据存储格式"></a>日志文件数据存储格式</h4><p>每个日志文件都是“log entries”序列，每一个log entry包含一个4字节整型数（值为N），其后跟N个字节的消息体。每条消息都有一个当前partition下唯一的64字节的offset，它指明了这条消息的起始位置。磁盘上存储的消息格式如下：</p>
<p><strong>消息长度: 4 bytes (value: 1 + 4 + n)</strong><br><strong>版本号: 1 byte</strong><br><strong>CRC 校验码: 4 bytes</strong><br><strong>具体的消息: n bytes</strong></p>
<p>【问题】 每条消息都有一个当前partition下唯一的64字节的offset，它指明了这条消息的起始位置。那这个offset值存哪里？</p>
<p>下面是一个消息的偏移量图示</p>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211031101841.png"></p>
<p>这个信息存储到.index索引文件中</p>
<h4 id="索引"><a href="#索引" class="headerlink" title="索引"></a>索引</h4><p>【思考】索引中存储的数据的结构是怎样的？</p>
<p>{消息序号，存储偏移地址}</p>
<p>【思考】有必要在索引中存储每一条消息的偏移地址吗？</p>
<p>稀疏索引</p>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211031101948.png"></p>
<p>kafka用的稀疏索引。</p>
<p>【思考】 kafka作为一个分布式的流数据存储平台，它能存储海量的消息数据，那一个分片的数据可能会很大吗？</p>
<p>一个很大的分片，也即一个很大的文件，操作方便吗</p>
<h4 id="Segment-段"><a href="#Segment-段" class="headerlink" title="Segment 段"></a>Segment 段</h4><p>分片分成多个段（一个.log文件）来存储，段的大小固定（可以指定）</p>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211031102057.png"></p>
<p>每个partion(目录)相当于一个巨型文件被平均分配到多个大小相等segment(段)数据文件中，每个segment文件名为该segment第一条消息的offset和“.log”组成。但每个段segment file消息数量不一定相等，这种特性方便old segment fifile快速被删除。（默认情况下每个文件大小为1G）</p>
<p>每个partiton只需要支持顺序读写就行了，segment文件生命周期由服务端配置参数决定。</p>
<p>这样做的好处就是能快速删除无用文件，有效提高磁盘利用率。</p>
<h4 id="消息什么时候删除"><a href="#消息什么时候删除" class="headerlink" title="消息什么时候删除"></a>消息什么时候删除</h4><ul>
<li>通过在server.properties文件中配置全局默认的日志保留策略来控制：</li>
</ul>
<p>支持两种策略：时间 和 大小 。 可多策略，哪个达到即哪个生效。</p>
<figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">############################# Log Retention Policy #############################</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment"># The following configurations control the disposal of log segments. The policy can</span></span><br><span class="line"><span class="comment"># be set to delete segments after a period of time, or after a given size has accumulated.</span></span><br><span class="line"><span class="comment"># A segment will be deleted whenever *either* of these criteria are met. Deletion always happens</span></span><br><span class="line"><span class="comment"># from the end of the log.</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment"># The minimum age of a log file to be eligible for deletion due to age</span></span><br><span class="line"><span class="comment"># 策略1 留存多长时间</span></span><br><span class="line"><span class="attr">log.retention.hours</span>=<span class="string">168</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment"># A size-based retention policy for logs. Segments are pruned from the log unless the remaining</span></span><br><span class="line"><span class="comment"># segments drop below log.retention.bytes. Functions independently of log.retention.hours.</span></span><br><span class="line"><span class="comment"># 策略2 留存多少字节</span></span><br><span class="line"><span class="comment">#log.retention.bytes=1073741824</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment"># The maximum size of a log segment file. When this size is reached a new log segment will be created.</span></span><br><span class="line"><span class="attr">log.segment.bytes</span>=<span class="string">1073741824</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment"># The interval at which log segments are checked to see if they can be deleted according</span></span><br><span class="line"><span class="comment"># to the retention policies</span></span><br><span class="line"><span class="attr">log.retention.check.interval.ms</span>=<span class="string">300000</span></span><br></pre></td></tr></table></figure>

<ul>
<li><p>在定义Topic时指定对应参数</p>
<p>文档：<a target="_blank" rel="noopener" href="https://kafka.apachecn.org/documentation.html#topicconfigs">https://kafka.apachecn.org/documentation.html#topicconfigs</a></p>
</li>
</ul>
<table>
<thead>
<tr>
<th>名称</th>
<th>描述</th>
<th>类型</th>
<th>默认值</th>
<th>有效值</th>
<th>服务器默认属性</th>
<th>重要性</th>
</tr>
</thead>
<tbody><tr>
<td>retention.bytes</td>
<td>如果使用“delete”保留策略，此配置控制分区(由日志段组成)在放弃旧日志段以释放空间之前的最大大小。默认情况下，没有大小限制，只有时间限制。由于此限制是在分区级别强制执行的，因此，将其乘以分区数，计算出topic保留值，以字节为单位。</td>
<td>long</td>
<td>-1</td>
<td></td>
<td>log.retention.bytes</td>
<td>medium</td>
</tr>
<tr>
<td>retention.ms</td>
<td>如果使用“delete”保留策略，此配置控制保留日志的最长时间，然后将旧日志段丢弃以释放空间。这代表了用户读取数据的速度的SLA。</td>
<td>long</td>
<td>604800000</td>
<td></td>
<td>log.retention.ms</td>
<td>medium</td>
</tr>
<tr>
<td>segment.bytes</td>
<td>此配置控制日志的段文件大小。保留和清理总是一次完成一个文件，所以更大的段大小意味着更少的文件，但对保留的粒度控制更少。</td>
<td>int</td>
<td>1073741824</td>
<td>[14,…]</td>
<td>log.segment.bytes</td>
<td>medium</td>
</tr>
</tbody></table>
<h4 id="消息的序号"><a href="#消息的序号" class="headerlink" title="消息的序号"></a>消息的序号</h4><p>在每个分片中会给每条消息一个<strong>递增的序号</strong></p>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211113172943.png"></p>
<p>【重点】消费者的offset （序号偏移量）</p>
<h4 id="TimeIndex-时间索引"><a href="#TimeIndex-时间索引" class="headerlink" title="TimeIndex 时间索引"></a>TimeIndex 时间索引</h4><p>【问题】如果我们想要消费从某时刻开始的消息，该怎么办？</p>
<ul>
<li>消息要有时间戳</li>
<li>建立时间索引 .timeindex 数据结构 {时间戳，序号}</li>
</ul>
<p>当要获取某时刻开始的消息时，根据时间戳到时间索引中获得 &gt;&#x3D; 该时刻的第一个消息序号；然后从该序号开始拉取数据。</p>
<p><strong>Broker中关于Timestamp的全局默认配置参数：</strong></p>
<table>
<thead>
<tr>
<th>名称</th>
<th>描述</th>
<th>类型</th>
<th>默认值</th>
<th>有效值</th>
<th>重要性</th>
</tr>
</thead>
<tbody><tr>
<td>log.message.timestamp.difference.max.ms</td>
<td>broker收到消息时的时间戳和消息中指定的时间戳之间允许的最大差异。当log.message.timestamp.type&#x3D;CreateTime,如果时间差超过这个阈值，消息将被拒绝。如果log.message.timestamp.type &#x3D; logappendtime，则该配置将被忽略。允许的最大时间戳差值，不应大于log.retention.ms，以避免不必要的频繁日志滚动。</td>
<td>long</td>
<td>9223372036854775807</td>
<td></td>
<td>中</td>
</tr>
<tr>
<td>log.message.timestamp.type</td>
<td>定义消息中的时间戳是消息创建时间还是日志追加时间。 该值应该是“createtime”或“logappendtime”。</td>
<td>string</td>
<td>CreateTime</td>
<td>[CreateTime, LogAppendTime]</td>
<td>中</td>
</tr>
</tbody></table>
<p><strong>Topic中关于Timestamp的配置参数：</strong></p>
<table>
<thead>
<tr>
<th>名称</th>
<th>描述</th>
<th>类型</th>
<th>默认值</th>
<th>有效值</th>
<th></th>
<th>重要性</th>
</tr>
</thead>
<tbody><tr>
<td>message.timestamp.difference.max.ms</td>
<td>broker接收消息时所允许的时间戳与消息中指定的时间戳之间的最大差异。如果message.timestamp.type&#x3D;CreateTime，则如果时间戳的差异超过此阈值，则将拒绝消息。如果message.timestamp.type&#x3D;LogAppendTime，则忽略此配置。</td>
<td>long</td>
<td>9223372036854775807</td>
<td>[0,…]</td>
<td>log.message.timestamp.difference.max.ms</td>
<td>medium</td>
</tr>
<tr>
<td>message.timestamp.type</td>
<td>定义消息中的时间戳是消息创建时间还是日志附加时间。值应该是“CreateTime”或“LogAppendTime”</td>
<td>string</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody></table>
<h4 id="Topic的配置参数-【了解】"><a href="#Topic的配置参数-【了解】" class="headerlink" title="Topic的配置参数 【了解】"></a>Topic的配置参数 【了解】</h4><h5 id="参数列表"><a href="#参数列表" class="headerlink" title="参数列表"></a>参数列表</h5><p><a target="_blank" rel="noopener" href="https://kafka.apachecn.org/documentation.html#topicconfigs">https://kafka.apachecn.org/documentation.html#topicconfigs</a></p>
<h5 id="参数修改"><a href="#参数修改" class="headerlink" title="参数修改"></a>参数修改</h5><p>与Topic相关的配置既包含服务器默认值，也包含可选的每个Topic覆盖值。 如果没有给出每个Topic的配置，那么服务器默认值就会被使用。 通过提供一个或多个 <code>--config</code> 选项，可以在创建Topic时设置覆盖值。 本示例使用自定义的最大消息大小和刷新率创建了一个名为 <em>my-topic</em> 的topic:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt; bin/kafka-topics.sh --zookeeper localhost:2181 --create --topic my-topic --partitions 1 --replication-factor 1 --config max.message.bytes=64000 --config flush.messages=1</span><br></pre></td></tr></table></figure>

<p>也可以在使用alter configs命令稍后更改或设置覆盖值. 本示例重置<em>my-topic</em>的最大消息的大小:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt; bin/kafka-configs.sh --zookeeper localhost:2181 --entity-type topics --entity-name my-topic --alter --add-config max.message.bytes=128000</span><br></pre></td></tr></table></figure>

<p>您可以执行如下操作来检查topic设置的覆盖值</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt; bin/kafka-configs.sh --zookeeper localhost:2181 --entity-type topics --entity-name my-topic --describe</span><br></pre></td></tr></table></figure>

<p>您可以执行如下操作来删除一个覆盖值</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt; bin/kafka-configs.sh --zookeeper localhost:2181  --entity-type topics --entity-name my-topic --alter --delete-config max.message.bytes</span><br></pre></td></tr></table></figure>

<h4 id="删除Topic"><a href="#删除Topic" class="headerlink" title="删除Topic"></a>删除Topic</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-topics.sh --create --bootstrap-server localhost:9092 --replication-factor 1 --partitions 1 --topic test-1</span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-console-producer.sh --broker-list localhost:9092 --topic test-1</span><br></pre></td></tr></table></figure>

<p>删除Topic</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-topics.sh --delete --bootstrap-server localhost:9092 --topic test-1</span><br></pre></td></tr></table></figure>

<p><strong>说明：</strong></p>
<p>当Topic是新创建的空Topic时，元信息和Topic的存储目录都会删除。</p>
<p>当Topic已有数据时，元信息在zookeeper上被删除，数据存储目录被标识为delete</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">drwxr-xr-x. 2 root root 141 11月 13 23:33 test-1-0.ba94171b0a724f36ab7d85af5ab8d18a-delete</span><br></pre></td></tr></table></figure>

<h2 id="Producer-1"><a href="#Producer-1" class="headerlink" title="Producer"></a>Producer</h2><p>生产者可以将数据发布到所选择的topic（主题）中。生产者负责将记录分配到topic的哪一个partition（分片）中。</p>
<h3 id="生产者消息发布确认机制"><a href="#生产者消息发布确认机制" class="headerlink" title="生产者消息发布确认机制"></a>生产者消息发布确认机制</h3><p>设置发送数据是否需要服务端的反馈,有三个值 [all,0,1,-1 ] ，默认 1</p>
<ul>
<li>0: producer不会等待broker发送ack</li>
<li>1: 当leader接收到消息之后发送ack</li>
<li>all：leader接收到消息后，等待所有in-sync的副本同步完成之后 发送ack。这样可以提供最好的可靠性。This means the leader will wait for the full set of in-sync replicas to acknowledge the record. This guarantees that the record will not be lost as long as at least one in-sync replica remains alive. This is the strongest available guarantee. This is equivalent to the acks&#x3D;-1 setting.</li>
<li>-1: 等价于 all</li>
</ul>
<h4 id="Callback异步处理的确认结果"><a href="#Callback异步处理的确认结果" class="headerlink" title="Callback异步处理的确认结果"></a>Callback异步处理的确认结果</h4><p>在发送时，你可以完全非阻塞，通过指定回调来处理发送结果，但当使用事务时，则没必要使用Callback，因为结果在事务方法中处理。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> Future&lt;RecordMetadata&gt; <span class="title function_">send</span><span class="params">(ProducerRecord&lt;K, V&gt; record, Callback callback)</span></span><br></pre></td></tr></table></figure>

<p>发送到同一分片的结果处理将按顺序执行回调。请详细阅读它的注释。</p>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211114114620.png"></p>
<h4 id="spring中的处理方式"><a href="#spring中的处理方式" class="headerlink" title="spring中的处理方式"></a>spring中的处理方式</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Set a &#123;<span class="doctag">@link</span> ProducerListener&#125; which will be invoked when Kafka acknowledges</span></span><br><span class="line"><span class="comment"> * a send operation. By default a &#123;<span class="doctag">@link</span> LoggingProducerListener&#125; is configured</span></span><br><span class="line"><span class="comment"> * which logs errors only.</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> producerListener the listener; may be &#123;<span class="doctag">@code</span> null&#125;.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">setProducerListener</span><span class="params">(<span class="meta">@Nullable</span> ProducerListener&lt;K, V&gt; producerListener)</span> &#123;</span><br><span class="line">	<span class="built_in">this</span>.producerListener = producerListener;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="ProducerInterceptor"><a href="#ProducerInterceptor" class="headerlink" title="ProducerInterceptor"></a>ProducerInterceptor</h4><p>如果想对消息的发送过程增加额外的统一处理逻辑可以提供 ProducerInterceptor 实现</p>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211114115059.png"></p>
<p>请详细了解它的注释说明。</p>
<p><strong>使用方式</strong> ，通过interceptor.classes指定实现类名：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 配置 ProducerInterceptor</span></span><br><span class="line">props.put(ProducerConfig.INTERCEPTOR_CLASSES_CONFIG,</span><br><span class="line">                <span class="string">&quot;com.study.kafka.client.s02_pub_ack.MyProducerInterceptor1,&quot;</span> + <span class="string">&quot;com.study.kafka.client.s02_pub_ack.MyProducerInterceptor2&quot;</span>);</span><br></pre></td></tr></table></figure>

<p>参数的说明</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">String</span> <span class="variable">INTERCEPTOR_CLASSES_CONFIG</span> <span class="operator">=</span> <span class="string">&quot;interceptor.classes&quot;</span>;</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">String</span> <span class="variable">INTERCEPTOR_CLASSES_DOC</span> <span class="operator">=</span> <span class="string">&quot;A list of classes to use as interceptors. &quot;</span></span><br><span class="line">                                                        + <span class="string">&quot;Implementing the &lt;code&gt;org.apache.kafka.clients.producer.ProducerInterceptor&lt;/code&gt; interface allows you to intercept (and possibly mutate) the records &quot;</span></span><br><span class="line">                                                        + <span class="string">&quot;received by the producer before they are published to the Kafka cluster. By default, there are no interceptors.&quot;</span>;</span><br></pre></td></tr></table></figure>

<h3 id="幂等模式-【了解】"><a href="#幂等模式-【了解】" class="headerlink" title="幂等模式 【了解】"></a>幂等模式 【了解】</h3><p>Kafka 0.11.0 支持幂等和事务性能力。幂等传递确保消息在单个生产者的生命周期内仅给特定的主题分区传递一次。事务交付允许生产者给多个分区发送数据，这样所有的消息都会被传递成功或失败。这些功能使Kafka符合“恰好一次语义”。</p>
<p>配置生产者参数enable.idempotence 为 true 。 retries acks 都不要配置了，因为会有自动的默认值：Integer.MAX_VALUE all 。 不可以业务上重发相同的业务数据。</p>
<h3 id="事务模式"><a href="#事务模式" class="headerlink" title="事务模式"></a>事务模式</h3><p>事务模式是为了让发往多个分片、多个Topic的多条消息具有原子性。</p>
<p><strong>事务模式要求：</strong></p>
<ul>
<li>要使用事务模式和对应的api，必须设置 transactional.id 属性。 transactional.id 配置后，将自动启用幂等性，同时启用幂等性所依赖的生成器配置。transactional.id 是事务标识，用于跨单个生产者实例的多个会话启用事务恢复。对于分区应用程序中运行的每个生成器实例，它应该是惟一的。</li>
<li>Kafka集群需要有至少3个节点</li>
<li>为了从端到端实现事务保证，还必须将消费者配置为只<strong>读取提交的消息</strong>。</li>
</ul>
<p>所有事务API是同步阻塞的。</p>
<p>Producer是线程安全。</p>
<h2 id="Consumer-1"><a href="#Consumer-1" class="headerlink" title="Consumer"></a>Consumer</h2><p>Kafka中消费者是采用 poll 拉模式来获取消息。</p>
<h3 id="Consumer示例"><a href="#Consumer示例" class="headerlink" title="Consumer示例"></a>Consumer示例</h3><p>【注意】 Consumer是非线程安全的。 多线程中的情况下，一个线程一个Consumer。</p>
<h3 id="消费组"><a href="#消费组" class="headerlink" title="消费组"></a>消费组</h3><h4 id="消费组的说明"><a href="#消费组的说明" class="headerlink" title="消费组的说明"></a>消费组的说明</h4><p>消费者使用一个 消费组 名称来进行标识，发布到topic中的每条记录被分配给订阅消费组中的一个消费者实例.消费者实例可以分布在多个进程中或者多个机器上。</p>
<p>如果所有的消费者实例在同一消费组中，消息记录会负载平衡到每一个消费者实例.</p>
<p>如果所有的消费者实例在不同的消费组中，每条消息记录会广播到所有的消费者进程.</p>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211119200722.png"></p>
<p>通常情况下，每个 topic 都会有一些消费组，一个消费组对应一个”逻辑订阅者”。一个消费组由许多消费者实例组成，便于扩展和容错。这就是发布和订阅的概念，只不过订阅者是一组消费者而不是单个的进程。</p>
<p>在Kafka中实现消费的方式是将日志中的分片划分到每一个消费者实例上，以便在任何时间，每个实例都是分片唯一的消费者。维护消费组中的消费关系由Kafka协议动态处理。如果新的实例加入组，他们将从组中其他成员处接管一些 partition 分区;如果一个实例消失，拥有的分区将被分发到剩余的实例。</p>
<p>Kafka 只保证分区内的记录是有序的，而不保证主题中不同分区的顺序。如果你希望所有消息都有序消费，可使用仅有一个分区的主题来实现，这意味着每个消费者组只有一个消费者进程。</p>
<h4 id="group-rebalance-消费组重平衡"><a href="#group-rebalance-消费组重平衡" class="headerlink" title="group rebalance 消费组重平衡"></a>group rebalance 消费组重平衡</h4><p>【注意】消费组的重平衡延时参数设置</p>
<figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">############################# Group Coordinator Settings #############################</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment"># The following configuration specifies the time, in milliseconds, that the GroupCoordinator will delay the initial consumer rebalance.</span></span><br><span class="line"><span class="comment"># The rebalance will be further delayed by the value of group.initial.rebalance.delay.ms as new members join the group, up to a maximum of max.poll.interval.ms.</span></span><br><span class="line"><span class="comment"># The default value for this is 3 seconds.</span></span><br><span class="line"><span class="comment"># We override this to 0 here as it makes for a better out-of-the-box experience for development and testing.</span></span><br><span class="line"><span class="comment"># However, in production environments the default value of 3 seconds is more suitable as this will help to avoid unnecessary, and potentially expensive, rebalances during application startup.</span></span><br><span class="line"><span class="comment"># 消费组的重平衡延时（单位毫秒），【注意】生产环境请恢复为默认值3秒，或根据实际需要增大</span></span><br><span class="line"><span class="attr">group.initial.rebalance.delay.ms</span>=<span class="string">0</span></span><br></pre></td></tr></table></figure>

<p>触发重平衡的事件：</p>
<ul>
<li>消费者数量变化</li>
<li>分片的增减</li>
</ul>
<p>【思考】Kafka怎么知道消费者离线了，需要rebalance了？</p>
<p>消费者与partition leader保持长连接，通过心跳机制检测消费者是否离线</p>
<h4 id="Beatheart-Session"><a href="#Beatheart-Session" class="headerlink" title="Beatheart Session"></a>Beatheart Session</h4><table>
<thead>
<tr>
<th>Name</th>
<th>Description</th>
<th>Type</th>
<th>Default</th>
<th>Valid Values</th>
<th>Importance</th>
</tr>
</thead>
<tbody><tr>
<td>heartbeat.interval.ms</td>
<td>The expected time between heartbeats to the consumer coordinator when using Kafka’s group management facilities. Heartbeats are used to ensure that the consumer’s session stays active and to facilitate rebalancing when new consumers join or leave the group. The value must be set lower than <code>session.timeout.ms</code>, but typically should be set no higher than 1&#x2F;3 of that value. It can be adjusted even lower to control the expected time for normal rebalances.</td>
<td>int</td>
<td>3000</td>
<td></td>
<td>high</td>
</tr>
<tr>
<td>session.timeout.ms</td>
<td>The timeout used to detect consumer failures when using Kafka’s group management facility. The consumer sends periodic heartbeats to indicate its liveness to the broker. If no heartbeats are received by the broker before the expiration of this session timeout, then the broker will remove this consumer from the group and initiate a rebalance. Note that the value must be in the allowable range as configured in the broker configuration by <code>group.min.session.timeout.ms</code> and <code>group.max.session.timeout.ms</code>.</td>
<td>int</td>
<td>10000</td>
<td></td>
<td>high</td>
</tr>
<tr>
<td>max.poll.interval.ms</td>
<td>The maximum delay between invocations of poll() when using consumer group management. This places an upper bound on the amount of time that the consumer can be idle before fetching more records. If poll() is not called before expiration of this timeout, then the consumer is considered failed and the group will rebalance in order to reassign the partitions to another member.</td>
<td>int</td>
<td>300000</td>
<td>[1,…]</td>
<td>medium</td>
</tr>
</tbody></table>
<h3 id="消费offset"><a href="#消费offset" class="headerlink" title="消费offset"></a>消费offset</h3><p>【思考】消费者启动时从哪里开始消费消息？</p>
<h4 id="auto-offset-reset"><a href="#auto-offset-reset" class="headerlink" title="auto.offset.reset"></a>auto.offset.reset</h4><p>如果zookeeper上没有消费者的offset，或保存的消费者offset被删除了，消费者启动时从哪里开始消费？</p>
<table>
<thead>
<tr>
<th>PROPERTY</th>
<th>DEFAULT</th>
<th>DESCRIPTION</th>
</tr>
</thead>
<tbody><tr>
<td>auto.offset.reset</td>
<td>largest</td>
<td>What to do when there is no initial offset in ZooKeeper or if an offset is out of range:<br/>* smallest(最小偏移量) : automatically reset the offset to the smallest offset<br/>* largest(最大偏移量) : automatically reset the offset to the largest offset<br/>* anything else(抛异常): throw exception to the consumer</td>
</tr>
</tbody></table>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//设置创建的消费者从何处开始消费消息 </span></span><br><span class="line">props.put(<span class="string">&quot;auto.offset.reset&quot;</span>, <span class="string">&quot;smallest&quot;</span>);</span><br></pre></td></tr></table></figure>

<p>【思考】每次启动都从头开始消费吗？如何从上次结束的位置开始？</p>
<h4 id="自动提交"><a href="#自动提交" class="headerlink" title="自动提交"></a>自动提交</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 开启自动消费offset提交</span></span><br><span class="line"><span class="comment">// 如果此值设置为true，consumer会周期性的把当前消费的offset值保存到zookeeper。当consumer失败重启之后将会使用此值作为新开始消费的值。</span></span><br><span class="line">props.put(<span class="string">&quot;enable.auto.commit&quot;</span>, <span class="string">&quot;true&quot;</span>);</span><br><span class="line"><span class="comment">// 自动消费offset提交的间隔时间</span></span><br><span class="line">props.put(<span class="string">&quot;auto.commit.interval.ms&quot;</span>, <span class="string">&quot;1000&quot;</span>);</span><br></pre></td></tr></table></figure>

<p>存在问题：</p>
<ul>
<li>重复消费</li>
<li>丢数据（丢消息，只是拉取到数据还没处理完成就自动提交offset了）</li>
</ul>
<h4 id="手动提交"><a href="#手动提交" class="headerlink" title="手动提交"></a>手动提交</h4><h5 id="代码示例"><a href="#代码示例" class="headerlink" title="代码示例"></a>代码示例</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//设置手动提交</span></span><br><span class="line">props.put(<span class="string">&quot;enable.auto.commit&quot;</span>, <span class="string">&quot;false&quot;</span>);</span><br><span class="line">...</span><br><span class="line"><span class="comment">//手动同步提交消费者offset到zookeeper</span></span><br><span class="line">consumer.commitSync();</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerRecord;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerRecords;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.KafkaConsumer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.time.Duration;</span><br><span class="line"><span class="keyword">import</span> java.util.ArrayList;</span><br><span class="line"><span class="keyword">import</span> java.util.Arrays;</span><br><span class="line"><span class="keyword">import</span> java.util.List;</span><br><span class="line"><span class="keyword">import</span> java.util.Properties;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">ManualCommitConsumerDemo</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        <span class="type">Properties</span> <span class="variable">props</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Properties</span>();</span><br><span class="line">        props.put(<span class="string">&quot;bootstrap.servers&quot;</span>, <span class="string">&quot;192.168.254.159:9092&quot;</span>);</span><br><span class="line">        props.put(<span class="string">&quot;group.id&quot;</span>, <span class="string">&quot;test&quot;</span>);</span><br><span class="line">        <span class="comment">// 设置手动提交消费offset</span></span><br><span class="line">        props.put(<span class="string">&quot;enable.auto.commit&quot;</span>, <span class="string">&quot;false&quot;</span>);</span><br><span class="line">        props.put(<span class="string">&quot;key.deserializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringDeserializer&quot;</span>);</span><br><span class="line">        props.put(<span class="string">&quot;value.deserializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringDeserializer&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">try</span> (KafkaConsumer&lt;String, String&gt; consumer = <span class="keyword">new</span> <span class="title class_">KafkaConsumer</span>&lt;&gt;(props);) &#123;</span><br><span class="line">            consumer.subscribe(Arrays.asList(<span class="string">&quot;test&quot;</span>, <span class="string">&quot;test-group&quot;</span>));</span><br><span class="line">            <span class="keyword">final</span> <span class="type">int</span> <span class="variable">minBatchSize</span> <span class="operator">=</span> <span class="number">10</span>;</span><br><span class="line">            List&lt;ConsumerRecord&lt;String, String&gt;&gt; buffer = <span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;&gt;();</span><br><span class="line">            <span class="keyword">while</span> (<span class="literal">true</span>) &#123;</span><br><span class="line">                ConsumerRecords&lt;String, String&gt; records = consumer.poll(Duration.ofMillis(<span class="number">200L</span>));</span><br><span class="line">                <span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; record : records) &#123;</span><br><span class="line">                    System.out.printf(<span class="string">&quot;offset = %d, key = %s, value = %s%n&quot;</span>, record.offset(), record.key(), record.value());</span><br><span class="line">                    buffer.add(record);</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="keyword">if</span> (buffer.size() &gt;= minBatchSize) &#123;</span><br><span class="line">                    insertIntoDb(buffer);</span><br><span class="line">                    <span class="comment">// 手动同步提交消费者offset到zookeeper</span></span><br><span class="line">                    consumer.commitSync();</span><br><span class="line">                    buffer.clear();</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">insertIntoDb</span><span class="params">(List&lt;ConsumerRecord&lt;String, String&gt;&gt; buffer)</span> &#123;</span><br><span class="line">        <span class="comment">// Insert into db</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h5 id="Consumer-的提交方法"><a href="#Consumer-的提交方法" class="headerlink" title="Consumer 的提交方法"></a>Consumer 的提交方法</h5><p>有同步、异步的方法，还有获取上次提交的offset数据的方法</p>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211121115638.png"></p>
<h5 id="灵活按分片提交消费offset值"><a href="#灵活按分片提交消费offset值" class="headerlink" title="灵活按分片提交消费offset值"></a>灵活按分片提交消费offset值</h5><p>当你订阅了多个topic的消息，一次拉取可能会返回多个主题多个分片的消息集，上面的手动提交是所有</p>
<p>的分片一起提交，如有需要，我们可以更细粒度地控制提交，下面的代码示例展示了按分片处理消息，</p>
<p>没处理完一个则提交该分片的消费offset值。<strong>【注意】 offset值是下次拉取的起始值（lastOffset + 1)</strong></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerRecord;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerRecords;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.KafkaConsumer;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.OffsetAndMetadata;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.common.TopicPartition;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.time.Duration;</span><br><span class="line"><span class="keyword">import</span> java.util.*;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">ManualPartitionCommitConsumer</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        <span class="type">Properties</span> <span class="variable">props</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Properties</span>();</span><br><span class="line">        props.put(<span class="string">&quot;bootstrap.servers&quot;</span>, <span class="string">&quot;192.168.254.170:9092&quot;</span>);</span><br><span class="line">        props.put(<span class="string">&quot;group.id&quot;</span>, <span class="string">&quot;test&quot;</span>);</span><br><span class="line">        <span class="comment">// 设置手动提交消费offset</span></span><br><span class="line">        props.put(<span class="string">&quot;enable.auto.commit&quot;</span>, <span class="string">&quot;false&quot;</span>);</span><br><span class="line">        props.put(<span class="string">&quot;key.deserializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringDeserializer&quot;</span>);</span><br><span class="line">        props.put(<span class="string">&quot;value.deserializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringDeserializer&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">try</span> (KafkaConsumer&lt;String, String&gt; consumer = <span class="keyword">new</span> <span class="title class_">KafkaConsumer</span>&lt;&gt;(props);) &#123;</span><br><span class="line">            consumer.subscribe(Arrays.asList(<span class="string">&quot;test&quot;</span>, <span class="string">&quot;test-group&quot;</span>));</span><br><span class="line">            <span class="keyword">final</span> <span class="type">int</span> <span class="variable">minBatchSize</span> <span class="operator">=</span> <span class="number">10</span>;</span><br><span class="line">            List&lt;ConsumerRecord&lt;String, String&gt;&gt; buffer = <span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;&gt;();</span><br><span class="line">            <span class="keyword">while</span> (<span class="literal">true</span>) &#123;</span><br><span class="line">                ConsumerRecords&lt;String, String&gt; records = consumer.poll(Duration.ofMillis(<span class="number">200L</span>));</span><br><span class="line">                <span class="comment">// 按分片来处理得到的数据</span></span><br><span class="line">                <span class="keyword">for</span> (TopicPartition partition : records.partitions()) &#123;</span><br><span class="line">                    System.out.println(<span class="string">&quot;************* partition:&quot;</span> + partition);</span><br><span class="line">                    <span class="comment">// 遍历处理分片的数据</span></span><br><span class="line">                    List&lt;ConsumerRecord&lt;String, String&gt;&gt; partitionRecords = records.records(partition);</span><br><span class="line">                    <span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; record : partitionRecords) &#123;</span><br><span class="line">                        System.out.printf(<span class="string">&quot;offset = %d, key = %s, value = %s%n&quot;</span>, record.offset(), record.key(), record.value());</span><br><span class="line">                    &#125;</span><br><span class="line">                    <span class="comment">// 取到的分片的最后一条数据的offset值</span></span><br><span class="line">                    <span class="type">long</span> <span class="variable">lastOffset</span> <span class="operator">=</span> partitionRecords.get(partitionRecords.size() - <span class="number">1</span>).offset();</span><br><span class="line">                    <span class="comment">// 提交该分片的消费offset = lastOffset + 1 .</span></span><br><span class="line">                    consumer.commitSync(Collections.singletonMap(partition, <span class="keyword">new</span> <span class="title class_">OffsetAndMetadata</span>(lastOffset + <span class="number">1</span>)));</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="控制消费的位置"><a href="#控制消费的位置" class="headerlink" title="控制消费的位置"></a>控制消费的位置</h4><p>有两种情况需要控制消费的位置：</p>
<ul>
<li>消费者程序在某个时刻停止了运行，重启后继续消费从那个时刻以来的消息。</li>
<li>消费者在本地可能存储了消费的消息，但这份本地存储坏了，想重新取一份到本地。</li>
</ul>
<p>控制消费位置的情况的下一般使用 指定消费的分片方式来进行消费</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">String</span> <span class="variable">topic</span> <span class="operator">=</span> <span class="string">&quot;foo&quot;</span>; </span><br><span class="line"><span class="type">TopicPartition</span> <span class="variable">partition0</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">TopicPartition</span>(topic, <span class="number">0</span>); </span><br><span class="line"><span class="type">TopicPartition</span> <span class="variable">partition1</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">TopicPartition</span>(topic, <span class="number">1</span>);</span><br><span class="line">consumer.assign(Arrays.asList(partition0, partition1));</span><br></pre></td></tr></table></figure>

<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211121121020.png"></p>
<h3 id="subscribe-vs-assign"><a href="#subscribe-vs-assign" class="headerlink" title="subscribe vs assign"></a>subscribe vs assign</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 不是订阅Topic</span></span><br><span class="line"><span class="comment">// consumer.subscribe(Arrays.asList(&quot;test&quot;, &quot;test-group&quot;));</span></span><br><span class="line"><span class="comment">// 而是直接分配该消费者读取某些分片 subscribe 和 assign 只能用其一，assign 时不受 rebalance影响。</span></span><br><span class="line"><span class="type">TopicPartition</span> <span class="variable">partition</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">TopicPartition</span>(<span class="string">&quot;test&quot;</span>, <span class="number">0</span>);</span><br><span class="line">consumer.assign(Arrays.asList(partition));</span><br></pre></td></tr></table></figure>

<ul>
<li>subscribe ：该消费者消费的是哪些分片是由Kafka根据消费组动态指定，并可以动态rebalance</li>
<li>assign ：则是由用户指定要消费哪些分片，不受rebalance影响。</li>
</ul>
<p>了解 ConsumerRebalanceListener</p>
<h3 id="poll设置"><a href="#poll设置" class="headerlink" title="poll设置"></a>poll设置</h3><p>【思考】 一次拉取，返回多少数据？</p>
<p>可以通过下面的消费者参数来指定。</p>
<table>
<thead>
<tr>
<th>名称</th>
<th>描述</th>
<th>类型</th>
<th>默认值</th>
<th>有效值</th>
<th>重要性</th>
</tr>
</thead>
<tbody><tr>
<td>fetch.min.bytes</td>
<td>The minimum amount of data the server should return for a fetch request. If insufficient data is available the request will wait for that much data to accumulate before answering the request. The default setting of 1 byte means that fetch requests are answered as soon as a single byte of data is available or the fetch request times out waiting for data to arrive. Setting this to something greater than 1 will cause the server to wait for larger amounts of data to accumulate which can improve server throughput a bit at the cost of some additional latency.</td>
<td>int</td>
<td>1</td>
<td>[0,…]</td>
<td>high</td>
</tr>
<tr>
<td>max.partition.fetch.bytes</td>
<td>The maximum amount of data per-partition the server will return. Records are fetched in batches by the consumer. If the first record batch in the first non-empty partition of the fetch is larger than this limit, the batch will still be returned to ensure that the consumer can make progress. The maximum record batch size accepted by the broker is defined via <code>message.max.bytes</code> (broker config) or <code>max.message.bytes</code> (topic config). See fetch.max.bytes for limiting the consumer request size.</td>
<td>int</td>
<td>1048576</td>
<td>[0,…]</td>
<td>high</td>
</tr>
<tr>
<td>fetch.max.bytes</td>
<td>The maximum amount of data the server should return for a fetch request. Records are fetched in batches by the consumer, and if the first record batch in the first non-empty partition of the fetch is larger than this value, the record batch will still be returned to ensure that the consumer can make progress. As such, this is not a absolute maximum. The maximum record batch size accepted by the broker is defined via <code>message.max.bytes</code> (broker config) or <code>max.message.bytes</code> (topic config). Note that the consumer performs multiple fetches in parallel.</td>
<td>int</td>
<td>52428800</td>
<td>[0,…]</td>
<td>medium</td>
</tr>
<tr>
<td>max.poll.records</td>
<td>The maximum number of records returned in a single call to poll().</td>
<td>int</td>
<td>500</td>
<td>[1,…]</td>
<td>medium</td>
</tr>
</tbody></table>
<h3 id="消费的流速控制-Flow-Control"><a href="#消费的流速控制-Flow-Control" class="headerlink" title="消费的流速控制 Flow Control"></a>消费的流速控制 Flow Control</h3><p>当我们给消费者指定从多个分片取数据时，一次poll它会同时从所有指定的分片中拉取数据。但在有些情况下我们可能需要先全速消费指定分片的一个子集，当这个子集只有少量或没有数据时再开始消费其他的分片。</p>
<p>这样的场景如：</p>
<ul>
<li>在流式处理中，程序要对两个Topic中的流数据执行join操作，而一个Topic的生产速度快与另一个，此时就需要降低快的topic的消费速度来匹配慢的。</li>
<li>另一个场景：启动消费者时，已经有大量消息堆积在这些指定的Topic中，程序需要优先处理包含最新数据的topic，再处理老旧数据的topic。</li>
</ul>
<p>Kafka的Consumer支持通过 pause(Collection) 和 resume(Collection) 来动态控制消费流速。</p>
<ul>
<li>pause(Collection partitions) 暂停一个子集的拉取</li>
<li>resume(Collection partitions) 恢复子集的拉取</li>
</ul>
<p>下次调用 poll(Duration) 时它们生效。</p>
<h3 id="事务"><a href="#事务" class="headerlink" title="事务"></a>事务</h3><p>和 生产者使用事务相关。重点就是隔离级别。</p>
<h3 id="消费者的配置参数"><a href="#消费者的配置参数" class="headerlink" title="消费者的配置参数"></a>消费者的配置参数</h3><p><a target="_blank" rel="noopener" href="https://kafka.apachecn.org/documentation.html#newconsumerconfigs">https://kafka.apachecn.org/documentation.html#newconsumerconfigs</a></p>
<p>建议看英文最新版的。</p>
<p><a target="_blank" rel="noopener" href="https://kafka.apache.org/documentation/#consumerconfigs">https://kafka.apache.org/documentation/#consumerconfigs</a></p>
<h3 id="Spring-API-【了解】"><a href="#Spring-API-【了解】" class="headerlink" title="Spring API 【了解】"></a>Spring API 【了解】</h3><ul>
<li>@KafkaListener</li>
<li>@TopicPartition</li>
<li>@PartitionOffset</li>
<li>KafkaListenerErrorHandler</li>
</ul>
<h3 id="ConsumerInterceptor-【知道】"><a href="#ConsumerInterceptor-【知道】" class="headerlink" title="ConsumerInterceptor 【知道】"></a>ConsumerInterceptor 【知道】</h3><p>配置参数：interceptor.classes</p>
<h1 id="Kafka-Streams-process"><a href="#Kafka-Streams-process" class="headerlink" title="Kafka Streams process"></a>Kafka Streams process</h1><h2 id="流式计算说明"><a href="#流式计算说明" class="headerlink" title="流式计算说明"></a>流式计算说明</h2><p>示例：</p>
<ul>
<li>双十一时实时滚动的订单量、成交总金额。</li>
<li>每十分钟的成交额</li>
<li>股票交易看板</li>
</ul>
<p>流式数据 –&gt; 流式计算</p>
<p>流式计算的特点：</p>
<ul>
<li>数据是随时间不断产生的，没有界限，数据是不能变更的。</li>
<li>计算也是不断进行的，是近实时的计算。</li>
<li>计算的结果是不断更新的，每次计算产生最新的结果</li>
</ul>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211121123058.png"></p>
<p>Kafka中提供了 <strong>Kafka-Streams</strong> 客户端库，让我们可以非常轻松地编写流式计算程序，来对Kafka集群中存储的流数据进行实时计算、分析处理。</p>
<p><strong>Kafka-Streams的特点：</strong></p>
<ul>
<li>作为一个简单的轻量级客户端库设计，它可以很容易地嵌入到任何Java应用程序中。</li>
<li>除Apache Kafka本身作为内部消息层外，对系统没有外部依赖;值得注意的是，它使用Kafka的分片模型进行水平伸缩处理，同时保持了强大的顺序保证。</li>
<li>支持容错的本地状态，这支持非常快速和高效的有状态操作，比如窗口连接和聚合。</li>
<li>支持精确的一次处理语义，以确保每条记录将被处理一次，且仅被处理一次，即使流客户机或Kafka代理在处理过程中出现故障也是如此。</li>
<li>使用一次一个记录的处理来实现毫秒级的处理延迟，并支持基于事件时间的窗口操作和记录的延迟到达。</li>
<li>提供必要的流处理原语，以及高级流DSL和低级处理器API。</li>
</ul>
<h2 id="一个流式计算程序示例"><a href="#一个流式计算程序示例" class="headerlink" title="一个流式计算程序示例"></a>一个流式计算程序示例</h2><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span> </span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.kafka<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span> </span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>kafka-streams<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span> </span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.3.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span> </span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>【需求】编写一个统计单词在消息总出现次数统计的流计算程序</p>
<p>【思考】 需要做哪些事情来完成单词统计。</p>
<ul>
<li>不断消费消息</li>
<li>分割消息为单词</li>
<li>累计单词的出现次数</li>
<li>每隔一分钟输出统计结果</li>
</ul>
<h2 id="Kafka-Streams-Low-level-processor-API-和-核心概念"><a href="#Kafka-Streams-Low-level-processor-API-和-核心概念" class="headerlink" title="Kafka Streams Low-level processor API 和 核心概念"></a>Kafka Streams Low-level processor API 和 核心概念</h2><h3 id="Processor-处理器"><a href="#Processor-处理器" class="headerlink" title="Processor 处理器"></a>Processor 处理器</h3><p>我们实现Processor接口提供我们的数据处理逻辑。<strong>要掌握的知识点</strong>：</p>
<ol>
<li>掌握Processor的三个方法的用途</li>
<li>掌握ProcessorContext的用途</li>
<li>重新认识Kafka中的数据为什么是Key 、Value 对结构的。</li>
</ol>
<h3 id="Processor-Topology-处理器拓扑结构"><a href="#Processor-Topology-处理器拓扑结构" class="headerlink" title="Processor Topology 处理器拓扑结构"></a>Processor Topology 处理器拓扑结构</h3><p>处理器拓扑结构，即流计算的流程。Kafka-streams API中提供了Topology这个API来让我们组合多个流处理步骤来构成一个复杂的流计算流程。看下面的Topoloy组合示例代码：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">Topology</span> <span class="variable">topology</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Topology</span>(); </span><br><span class="line"></span><br><span class="line">topology.addSource(<span class="string">&quot;SOURCE&quot;</span>, <span class="string">&quot;src-topic&quot;</span>) </span><br><span class="line"><span class="comment">// add &quot;PROCESS1&quot; node which takes the source processor &quot;SOURCE&quot; as its upstream processor </span></span><br><span class="line">.addProcessor(<span class="string">&quot;PROCESS1&quot;</span>, () -&gt; <span class="keyword">new</span> <span class="title class_">MyProcessor1</span>(), <span class="string">&quot;SOURCE&quot;</span>) </span><br><span class="line"></span><br><span class="line"><span class="comment">// add &quot;PROCESS2&quot; node which takes &quot;PROCESS1&quot; as its upstream processor </span></span><br><span class="line">.addProcessor(<span class="string">&quot;PROCESS2&quot;</span>, () -&gt; <span class="keyword">new</span> <span class="title class_">MyProcessor2</span>(), <span class="string">&quot;PROCESS1&quot;</span>) </span><br><span class="line"></span><br><span class="line"><span class="comment">// add &quot;PROCESS3&quot; node which takes &quot;SOURCE&quot; as its upstream processor </span></span><br><span class="line">.addProcessor(<span class="string">&quot;PROCESS3&quot;</span>, () -&gt; <span class="keyword">new</span> <span class="title class_">MyProcessor3</span>(), <span class="string">&quot;SOURCE&quot;</span>) </span><br><span class="line"></span><br><span class="line"><span class="comment">// add the sink processor node &quot;SINK1&quot; that takes Kafka topic &quot;sink-topic1&quot; </span></span><br><span class="line"><span class="comment">// as output and the &quot;PROCESS1&quot; node as its upstream processor </span></span><br><span class="line">.addSink(<span class="string">&quot;SINK1&quot;</span>, <span class="string">&quot;sink-topic1&quot;</span>, <span class="string">&quot;PROCESS1&quot;</span>) </span><br><span class="line"></span><br><span class="line"><span class="comment">// add the sink processor node &quot;SINK2&quot; that takes Kafka topic &quot;sink-topic2&quot; </span></span><br><span class="line"><span class="comment">// as output and the &quot;PROCESS2&quot; node as its upstream processor </span></span><br><span class="line">.addSink(<span class="string">&quot;SINK2&quot;</span>, <span class="string">&quot;sink-topic2&quot;</span>, <span class="string">&quot;PROCESS2&quot;</span>) </span><br><span class="line"></span><br><span class="line"><span class="comment">// add the sink processor node &quot;SINK3&quot; that takes Kafka topic &quot;sink-topic3&quot; </span></span><br><span class="line"><span class="comment">// as output and the &quot;PROCESS3&quot; node as its upstream processor </span></span><br><span class="line">.addSink(<span class="string">&quot;SINK3&quot;</span>, <span class="string">&quot;sink-topic3&quot;</span>, <span class="string">&quot;PROCESS3&quot;</span>);</span><br></pre></td></tr></table></figure>

<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211121125000.png"></p>
<p><strong>拓扑结构中有两种特殊的处理器：</strong></p>
<ul>
<li><strong>Source Processor</strong> ： Source Processor 是一种没有前置节点的特殊流处理器。它从一个或者多个 Kafka Topic 消费数据并产出一个输入流给到拓扑结构的后续处理节点。</li>
<li><strong>Sink Processor</strong> ： sink processor 是一种特殊的流处理器，没有处理器需要依赖于它。 它从前置流处理器接收数据并传输给指定的 Kafka Topic</li>
</ul>
<h3 id="State-Store"><a href="#State-Store" class="headerlink" title="State Store"></a>State Store</h3><p>【思考】流处理程序启动后能停吗？</p>
<p>重启时怎么恢复到停前的计算状态？</p>
<p>得要能保存计算过程中的状态（结算的结果，特别是中间环节的计算结果）</p>
<p>【思考】为了使流计算过程能容错，我们需要存储计算状态，那可以存储到哪里呢？</p>
<p>内存、磁盘 、db</p>
<p>存储到本机可靠吗？</p>
<p>如果机器故障了，为了容错，需要能将计算迁移到其他机器上继续，存储到本机就不合适了。</p>
<p>那存到哪里合适？</p>
<p>Topic</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">KeyValueBytesStoreSupplier</span> <span class="variable">countStoreSupplier</span> <span class="operator">=</span> Stores.inMemoryKeyValueStore(<span class="string">&quot;Counts&quot;</span>); StoreBuilder&lt;KeyValueStore&lt;String, Long&gt;&gt; builder = Stores.keyValueStoreBuilder(countStoreSupplier, Serdes.String(), Serdes.Long()); </span><br><span class="line"><span class="comment">// add the count store associated with the WordCountProcessor processor </span></span><br><span class="line"><span class="comment">// 在topoly中关联Processor要使用的state store </span></span><br><span class="line">topology.addStateStore(builder, <span class="string">&quot;Process&quot;</span>)</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.kafka.streams.state.StoreBuilder; </span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.streams.state.Stores; </span><br><span class="line"></span><br><span class="line">StoreBuilder&lt;KeyValueStore&lt;String, Long&gt;&gt; countStoreSupplier = Stores.keyValueStoreBuilder( 		Stores.persistentKeyValueStore(<span class="string">&quot;Counts&quot;</span>), </span><br><span class="line">	Serdes.String(), </span><br><span class="line">	Serdes.Long())</span><br><span class="line">    .withLoggingDisabled(); <span class="comment">// disable backing up the store to a changelog topic</span></span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.kafka.streams.state.StoreBuilder; </span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.streams.state.Stores; </span><br><span class="line"><span class="comment">// 存储 state changelog 的topic的配置 </span></span><br><span class="line">Map&lt;String, String&gt; changelogConfig = <span class="keyword">new</span> <span class="title class_">HashMap</span>(); </span><br><span class="line"><span class="comment">// override min.insync.replicas </span></span><br><span class="line">changelogConfig.put(<span class="string">&quot;min.insyc.replicas&quot;</span>, <span class="string">&quot;1&quot;</span>);</span><br><span class="line"></span><br><span class="line">StoreBuilder&lt;KeyValueStore&lt;String, Long&gt;&gt; countStoreSupplier = Stores.keyValueStoreBuilder( 		Stores.persistentKeyValueStore(<span class="string">&quot;Counts&quot;</span>), </span><br><span class="line">	Serdes.String(), </span><br><span class="line">	Serdes.Long())</span><br><span class="line">	.withLoggingEnabled(changlogConfig); <span class="comment">// enable changelogging, with custom changelog settings</span></span><br></pre></td></tr></table></figure>

<p>容错而记录变更日志 (state stroe 变更日志topic )，默认是开启的。</p>
<p>存储变更日志的topic名称为 流计算应用名-存储名-changelog ，如 my-stream-processing-application-Counts-changelog</p>
<h2 id="DSL-High-Level-API"><a href="#DSL-High-Level-API" class="headerlink" title="DSL High-Level API"></a>DSL High-Level API</h2><p>【思考】流计算一般都计算些什么结果？或做些什么计算？</p>
<p>聚合计算 数据转换</p>
<p>kafka-streams 提供了一种更高级的简便DSL，为我们定义好了很多聚合函数，方面我们快速开发流计算程序。</p>
<p>详细的API</p>
<p>KStream KTable</p>
<p><a target="_blank" rel="noopener" href="http://kafka.apache.org/23/documentation/streams/developer-guide/dsl-api.html">http://kafka.apache.org/23/documentation/streams/developer-guide/dsl-api.html</a></p>
<h1 id="Connect"><a href="#Connect" class="headerlink" title="Connect"></a>Connect</h1><p><a target="_blank" rel="noopener" href="https://kafka.apachecn.org/documentation.html#connect">https://kafka.apachecn.org/documentation.html#connect</a></p>
<p>Kafka Connect 是一款可扩展并且可靠地在 Apache Kafka 和其他系统之间进行数据传输的工具。 可以很简单的快速定义 <em>connectors</em> 将大量数据从 Kafka 移入和移出. Kafka Connect 可以摄取数据库数据或者收集应用程序的 metrics 存储到 Kafka topics，使得数据可以用于低延迟的流处理。 一个导出的 job 可以将来自 Kafka topic 的数据传输到二级存储，用于系统查询或者批量进行离线分析。</p>
<p>Kafka Connect 功能包括:</p>
<ul>
<li><strong>Kafka connectors 通用框架：</strong> - Kafka Connect 将其他数据系统和Kafka集成标准化,简化了 connector 的开发,部署和管理</li>
<li><strong>分布式和单机模式</strong> - 可以扩展成一个集中式的管理服务，也可以单机方便的开发,测试和生产环境小型的部署。</li>
<li><strong>REST 接口</strong> - submit and manage connectors to your Kafka Connect cluster via an easy to use REST API</li>
<li><strong>offset 自动管理</strong> - 只需要connectors 的一些信息，Kafka Connect 可以自动管理offset 提交的过程，因此开发人员无需担心开发中offset提交出错的这部分。</li>
<li><strong>分布式的并且可扩展</strong> - Kafka Connect 构建在现有的 group 管理协议上。Kafka Connect 集群可以扩展添加更多的workers。</li>
<li><strong>整合流处理&#x2F;批处理</strong> - 利用 Kafka 已有的功能，Kafka Connect 是一个桥接stream 和批处理系统理想的方式。</li>
</ul>
<h2 id="运行-Kafka-Connect"><a href="#运行-Kafka-Connect" class="headerlink" title="运行 Kafka Connect"></a>运行 Kafka Connect</h2><p>Kafka Connect 当前支持两种执行方式: 单机 (单个进程) 和 分布式.</p>
<p>在单机模式下所有的工作都是在一个进程中运行的。connect的配置项很容易配置和开始使用，当只有一台机器(worker)的时候也是可用的(例如，收集日志文件到kafka)，但是不利于Kafka Connect 的容错。你可以通过下面的命令启动一个单机进程:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt; bin/connect-standalone.sh config/connect-standalone.properties connector1.properties [connector2.properties ...]</span><br></pre></td></tr></table></figure>

<h3 id="connect-standalone-properties"><a href="#connect-standalone-properties" class="headerlink" title="connect-standalone.properties"></a>connect-standalone.properties</h3><figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Licensed to the Apache Software Foundation (ASF) under one or more</span></span><br><span class="line"><span class="comment"># contributor license agreements.  See the NOTICE file distributed with</span></span><br><span class="line"><span class="comment"># this work for additional information regarding copyright ownership.</span></span><br><span class="line"><span class="comment"># The ASF licenses this file to You under the Apache License, Version 2.0</span></span><br><span class="line"><span class="comment"># (the &quot;License&quot;); you may not use this file except in compliance with</span></span><br><span class="line"><span class="comment"># the License.  You may obtain a copy of the License at</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment">#    http://www.apache.org/licenses/LICENSE-2.0</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># Unless required by applicable law or agreed to in writing, software</span></span><br><span class="line"><span class="comment"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span></span><br><span class="line"><span class="comment"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span></span><br><span class="line"><span class="comment"># See the License for the specific language governing permissions and</span></span><br><span class="line"><span class="comment"># limitations under the License.</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment"># These are defaults. This file just demonstrates how to override some settings.</span></span><br><span class="line"><span class="attr">bootstrap.servers</span>=<span class="string">localhost:9092</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment"># The converters specify the format of data in Kafka and how to translate it into Connect data. Every Connect user will</span></span><br><span class="line"><span class="comment"># need to configure these based on the format they want their data in when loaded from or stored into Kafka</span></span><br><span class="line"><span class="attr">key.converter</span>=<span class="string">org.apache.kafka.connect.json.JsonConverter</span></span><br><span class="line"><span class="attr">value.converter</span>=<span class="string">org.apache.kafka.connect.json.JsonConverter</span></span><br><span class="line"><span class="comment"># Converter-specific settings can be passed in by prefixing the Converter&#x27;s setting with the converter we want to apply</span></span><br><span class="line"><span class="comment"># it to</span></span><br><span class="line"><span class="attr">key.converter.schemas.enable</span>=<span class="string">true</span></span><br><span class="line"><span class="attr">value.converter.schemas.enable</span>=<span class="string">true</span></span><br><span class="line"></span><br><span class="line"><span class="attr">offset.storage.file.filename</span>=<span class="string">/tmp/connect.offsets</span></span><br><span class="line"><span class="comment"># Flush much faster than normal, which is useful for testing/debugging</span></span><br><span class="line"><span class="attr">offset.flush.interval.ms</span>=<span class="string">10000</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment"># Set to a list of filesystem paths separated by commas (,) to enable class loading isolation for plugins</span></span><br><span class="line"><span class="comment"># (connectors, converters, transformations). The list should consist of top level directories that include </span></span><br><span class="line"><span class="comment"># any combination of: </span></span><br><span class="line"><span class="comment"># a) directories immediately containing jars with plugins and their dependencies</span></span><br><span class="line"><span class="comment"># b) uber-jars with plugins and their dependencies</span></span><br><span class="line"><span class="comment"># c) directories immediately containing the package directory structure of classes of plugins and their dependencies</span></span><br><span class="line"><span class="comment"># <span class="doctag">Note:</span> symlinks will be followed to discover dependencies or plugins.</span></span><br><span class="line"><span class="comment"># Examples: </span></span><br><span class="line"><span class="comment"># plugin.path=/usr/local/share/java,/usr/local/share/kafka/plugins,/opt/connectors,</span></span><br><span class="line"><span class="comment">#plugin.path=</span></span><br></pre></td></tr></table></figure>

<p>第一个参数是 worker 的配置文件. 其中包括 Kafka connection 参数，序列化格式，和如何频繁的提交offsets。 所提供的示例可以在本地良好的运行，使用默认提供的配置 <code>config/server.properties</code> 。它需要调整以配合不同的配置或生产环境部署。所有的workers（独立和分布式）都需要一些配置 :</p>
<ul>
<li><code>bootstrap.servers</code> - List of Kafka servers used to bootstrap connections to Kafka</li>
<li><code>key.converter</code> - Converter class used to convert between Kafka Connect format and the serialized form that is written to Kafka. This controls the format of the keys in messages written to or read from Kafka, and since this is independent of connectors it allows any connector to work with any serialization format. Examples of common formats include JSON and Avro.</li>
<li><code>value.converter</code> - Converter class used to convert between Kafka Connect format and the serialized form that is written to Kafka. This controls the format of the values in messages written to or read from Kafka, and since this is independent of connectors it allows any connector to work with any serialization format. Examples of common formats include JSON and Avro.</li>
</ul>
<p>单机模式的重要配置如下:</p>
<ul>
<li><code>offset.storage.file.filename</code> - 存储 offset 数据的文件</li>
</ul>
<p>此处配置的参数适用于由Kafka Connect使用的 producer 和 consumer 访问配置，offset 和 status topic。对于 Kafka source和 sink 任务的配置，可以使用相同的参数，但必须以<code>consumer.</code> 和 <code>producer.</code> 作为前缀。 此外，从 worker 配置中继承的参数只有一个，就是 <code>bootstrap.servers</code>。大多数情况下，这是足够的，因为同一个集群通常用于所有的场景。但是需要注意的是一个安全集群，需要额外的参数才能允许连接。这些参数需要在 worker 配置中设置三次，一次用于管理访问，一次用于 Kafka sinks，还有一次用于 Kafka source。</p>
<p>其余参数用于 connector 的配置文件，你可以导入尽可能多的配置文件，但是所有的配置文件都将在同一个进程内(在不同的线程上)执行。</p>
<p>分布式模式下会自动进行负载均衡，允许动态的扩缩容，并提供对 active task，以及这个任务对应的配置和offset提交记录的容错。分布式执行方式和单机模式非常相似:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt; bin/connect-distributed.sh config/connect-distributed.properties</span><br></pre></td></tr></table></figure>

<h3 id="connect-distributed-properties"><a href="#connect-distributed-properties" class="headerlink" title="connect-distributed.properties"></a>connect-distributed.properties</h3><figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">##</span></span><br><span class="line"><span class="comment"># Licensed to the Apache Software Foundation (ASF) under one or more</span></span><br><span class="line"><span class="comment"># contributor license agreements.  See the NOTICE file distributed with</span></span><br><span class="line"><span class="comment"># this work for additional information regarding copyright ownership.</span></span><br><span class="line"><span class="comment"># The ASF licenses this file to You under the Apache License, Version 2.0</span></span><br><span class="line"><span class="comment"># (the &quot;License&quot;); you may not use this file except in compliance with</span></span><br><span class="line"><span class="comment"># the License.  You may obtain a copy of the License at</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment">#    http://www.apache.org/licenses/LICENSE-2.0</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># Unless required by applicable law or agreed to in writing, software</span></span><br><span class="line"><span class="comment"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span></span><br><span class="line"><span class="comment"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span></span><br><span class="line"><span class="comment"># See the License for the specific language governing permissions and</span></span><br><span class="line"><span class="comment"># limitations under the License.</span></span><br><span class="line"><span class="comment">##</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment"># This file contains some of the configurations for the Kafka Connect distributed worker. This file is intended</span></span><br><span class="line"><span class="comment"># to be used with the examples, and some settings may differ from those used in a production system, especially</span></span><br><span class="line"><span class="comment"># the `bootstrap.servers` and those specifying replication factors.</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment"># A list of host/port pairs to use for establishing the initial connection to the Kafka cluster.</span></span><br><span class="line"><span class="attr">bootstrap.servers</span>=<span class="string">localhost:9092</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment"># unique name for the cluster, used in forming the Connect cluster group. Note that this must not conflict with consumer group IDs</span></span><br><span class="line"><span class="attr">group.id</span>=<span class="string">connect-cluster</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment"># The converters specify the format of data in Kafka and how to translate it into Connect data. Every Connect user will</span></span><br><span class="line"><span class="comment"># need to configure these based on the format they want their data in when loaded from or stored into Kafka</span></span><br><span class="line"><span class="attr">key.converter</span>=<span class="string">org.apache.kafka.connect.json.JsonConverter</span></span><br><span class="line"><span class="attr">value.converter</span>=<span class="string">org.apache.kafka.connect.json.JsonConverter</span></span><br><span class="line"><span class="comment"># Converter-specific settings can be passed in by prefixing the Converter&#x27;s setting with the converter we want to apply</span></span><br><span class="line"><span class="comment"># it to</span></span><br><span class="line"><span class="attr">key.converter.schemas.enable</span>=<span class="string">true</span></span><br><span class="line"><span class="attr">value.converter.schemas.enable</span>=<span class="string">true</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment"># Topic to use for storing offsets. This topic should have many partitions and be replicated and compacted.</span></span><br><span class="line"><span class="comment"># Kafka Connect will attempt to create the topic automatically when needed, but you can always manually create</span></span><br><span class="line"><span class="comment"># the topic before starting Kafka Connect if a specific topic configuration is needed.</span></span><br><span class="line"><span class="comment"># Most users will want to use the built-in default replication factor of 3 or in some cases even specify a larger value.</span></span><br><span class="line"><span class="comment"># Since this means there must be at least as many brokers as the maximum replication factor used, we&#x27;d like to be able</span></span><br><span class="line"><span class="comment"># to run this example on a single-broker cluster and so here we instead set the replication factor to 1.</span></span><br><span class="line"><span class="attr">offset.storage.topic</span>=<span class="string">connect-offsets</span></span><br><span class="line"><span class="attr">offset.storage.replication.factor</span>=<span class="string">1</span></span><br><span class="line"><span class="comment">#offset.storage.partitions=25</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment"># Topic to use for storing connector and task configurations; note that this should be a single partition, highly replicated,</span></span><br><span class="line"><span class="comment"># and compacted topic. Kafka Connect will attempt to create the topic automatically when needed, but you can always manually create</span></span><br><span class="line"><span class="comment"># the topic before starting Kafka Connect if a specific topic configuration is needed.</span></span><br><span class="line"><span class="comment"># Most users will want to use the built-in default replication factor of 3 or in some cases even specify a larger value.</span></span><br><span class="line"><span class="comment"># Since this means there must be at least as many brokers as the maximum replication factor used, we&#x27;d like to be able</span></span><br><span class="line"><span class="comment"># to run this example on a single-broker cluster and so here we instead set the replication factor to 1.</span></span><br><span class="line"><span class="attr">config.storage.topic</span>=<span class="string">connect-configs</span></span><br><span class="line"><span class="attr">config.storage.replication.factor</span>=<span class="string">1</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment"># Topic to use for storing statuses. This topic can have multiple partitions and should be replicated and compacted.</span></span><br><span class="line"><span class="comment"># Kafka Connect will attempt to create the topic automatically when needed, but you can always manually create</span></span><br><span class="line"><span class="comment"># the topic before starting Kafka Connect if a specific topic configuration is needed.</span></span><br><span class="line"><span class="comment"># Most users will want to use the built-in default replication factor of 3 or in some cases even specify a larger value.</span></span><br><span class="line"><span class="comment"># Since this means there must be at least as many brokers as the maximum replication factor used, we&#x27;d like to be able</span></span><br><span class="line"><span class="comment"># to run this example on a single-broker cluster and so here we instead set the replication factor to 1.</span></span><br><span class="line"><span class="attr">status.storage.topic</span>=<span class="string">connect-status</span></span><br><span class="line"><span class="attr">status.storage.replication.factor</span>=<span class="string">1</span></span><br><span class="line"><span class="comment">#status.storage.partitions=5</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment"># Flush much faster than normal, which is useful for testing/debugging</span></span><br><span class="line"><span class="attr">offset.flush.interval.ms</span>=<span class="string">10000</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment"># These are provided to inform the user about the presence of the REST host and port configs </span></span><br><span class="line"><span class="comment"># Hostname &amp; Port for the REST API to listen on. If this is set, it will bind to the interface used to listen to requests.</span></span><br><span class="line"><span class="comment">#rest.host.name=</span></span><br><span class="line"><span class="comment">#rest.port=8083</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment"># The Hostname &amp; Port that will be given out to other workers to connect to i.e. URLs that are routable from other servers.</span></span><br><span class="line"><span class="comment">#rest.advertised.host.name=</span></span><br><span class="line"><span class="comment">#rest.advertised.port=</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment"># Set to a list of filesystem paths separated by commas (,) to enable class loading isolation for plugins</span></span><br><span class="line"><span class="comment"># (connectors, converters, transformations). The list should consist of top level directories that include </span></span><br><span class="line"><span class="comment"># any combination of: </span></span><br><span class="line"><span class="comment"># a) directories immediately containing jars with plugins and their dependencies</span></span><br><span class="line"><span class="comment"># b) uber-jars with plugins and their dependencies</span></span><br><span class="line"><span class="comment"># c) directories immediately containing the package directory structure of classes of plugins and their dependencies</span></span><br><span class="line"><span class="comment"># Examples: </span></span><br><span class="line"><span class="comment"># plugin.path=/usr/local/share/java,/usr/local/share/kafka/plugins,/opt/connectors,</span></span><br><span class="line"><span class="comment">#plugin.path=</span></span><br></pre></td></tr></table></figure>

<p>和单机模式不同在于启动的实现类和决定 Kafka connect 进程如何工作的配置参数，如何分配 work,offsets 存储在哪里和任务状态。在分布式模式中，Kafka Connect 存储 offsets,配置和存储在 Kafka topic中的任务状态。建议手动创建Kafka 的 offsets,配置和状态，以实现自己所期望的分区数和备份因子。如果启动Kafka Connect之前没有创建 topic，则会使用默认分区数和复制因子自动创建创建 topic，但这可能不是最适合的。</p>
<p>特别是，除了上面提到的常用设置之外，以下配置参数在启动集群之前至关重要:</p>
<ul>
<li><code>group.id</code> (default <code>connect-cluster</code>) - unique name for the cluster, used in forming the Connect cluster group; note that this <strong>must not conflict</strong> with consumer group IDs</li>
<li><code>config.storage.topic</code> (default <code>connect-configs</code>) - topic to use for storing connector and task configurations; note that this should be a single partition, highly replicated, compacted topic. You may need to manually create the topic to ensure the correct configuration as auto created topics may have multiple partitions or be automatically configured for deletion rather than compaction</li>
<li><code>offset.storage.topic</code> (default <code>connect-offsets</code>) - topic to use for storing offsets; this topic should have many partitions, be replicated, and be configured for compaction</li>
<li><code>status.storage.topic</code> (default <code>connect-status</code>) - topic to use for storing statuses; this topic can have multiple partitions, and should be replicated and configured for compaction</li>
</ul>
<p>注意在分布式模式下 connector 配置不会通过命令行传递。相反，会使用下面提到的 REST API来创建，修改和销毁 connectors。</p>
<h2 id="Configuring-Connectors"><a href="#Configuring-Connectors" class="headerlink" title="Configuring Connectors"></a>Configuring Connectors</h2><p>Connector 配置是简单的key-value 映射的格式。对于单机模式，这些配置会在 properties 文件中定义，并通过命令行传递给 Connect 进程。在分布式模式中，它们将被包含在创建（或修改）connector 的请求的JSON格式串中。</p>
<p>大多数配置都依赖于 connectors,所以在这里不能概述。但是，有几个常见选项可以看一下:</p>
<ul>
<li><code>name</code> - Unique name for the connector. Attempting to register again with the same name will fail.</li>
<li><code>connector.class</code> - The Java class for the connector</li>
<li><code>tasks.max</code> - The maximum number of tasks that should be created for this connector. The connector may create fewer tasks if it cannot achieve this level of parallelism.</li>
<li><code>key.converter</code> - (optional) Override the default key converter set by the worker.</li>
<li><code>value.converter</code> - (optional) Override the default value converter set by the worker.</li>
</ul>
<p><code>connector.class</code> 配置支持多种名称格式：这个 connector class 的全名或者别名。如果 connector 是 org.apache.kafka.connect.file.FileStreamSinkConnector，则可以指定全名，也可以使用FileStreamSink 或 FileStreamSinkConnector 来简化配置。</p>
<p>Sink connectors 还有一个额外的选项来控制他的输出:</p>
<ul>
<li><code>topics</code> - A list of topics to use as input for this connector</li>
</ul>
<p>对于任何其他选项，你应该查阅 connector的文档.</p>
<h2 id="Transformations"><a href="#Transformations" class="headerlink" title="Transformations"></a>Transformations</h2><p>connectors可以配置 transformations 操作，实现轻量级的消息单次修改，他们可以方便地用于数据修改和事件路由。</p>
<p>A transformation chain 可以在connector 配置中指定。</p>
<ul>
<li><code>transforms</code> - List of aliases for the transformation, specifying the order in which the transformations will be applied.</li>
<li><code>transforms.$alias.type</code> - Fully qualified class name for the transformation.</li>
<li><code>transforms.$alias.$transformationSpecificConfig</code> Configuration properties for the transformation</li>
</ul>
<p>例如，让我们使用内置的 file soucre connector，并使用 transformation 来添加静态字段。</p>
<p>这个例子中，我们会使用 schemaless json 数据格式。为了使用 schemaless 格式，我们将 <code>connect-standalone.properties</code> 文件中下面两行从true改成false:</p>
<figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">key.converter.schemas.enable</span></span><br><span class="line"><span class="attr">value.converter.schemas.enable</span></span><br></pre></td></tr></table></figure>

<p>这个 file source connector 读取每行数据作为一个字符串。我们会将每行数据包装进一个 Map 数据结构,然后添加一个二级字段来标识事件的来源。做这样一个操作，我们使用两个 transformations:</p>
<ul>
<li><strong>HoistField</strong> to place the input line inside a Map</li>
<li><strong>InsertField</strong> to add the static field. In this example we’ll indicate that the record came from a file connector</li>
</ul>
<p>添加完 transformations, <code>connect-file-source.properties</code> 文件像下面这样:</p>
<figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">name</span>=<span class="string">local-file-source</span></span><br><span class="line"><span class="attr">connector.class</span>=<span class="string">FileStreamSource</span></span><br><span class="line"><span class="attr">tasks.max</span>=<span class="string">1</span></span><br><span class="line"><span class="attr">file</span>=<span class="string">test.txt</span></span><br><span class="line"><span class="attr">topic</span>=<span class="string">connect-test</span></span><br><span class="line"><span class="attr">transforms</span>=<span class="string">MakeMap, InsertSource</span></span><br><span class="line"><span class="attr">transforms.MakeMap.type</span>=<span class="string">org.apache.kafka.connect.transforms.HoistField$Value</span></span><br><span class="line"><span class="attr">transforms.MakeMap.field</span>=<span class="string">line</span></span><br><span class="line"><span class="attr">transforms.InsertSource.type</span>=<span class="string">org.apache.kafka.connect.transforms.InsertField$Value</span></span><br><span class="line"><span class="attr">transforms.InsertSource.static.field</span>=<span class="string">data_source</span></span><br><span class="line"><span class="attr">transforms.InsertSource.static.value</span>=<span class="string">test-file-source</span></span><br></pre></td></tr></table></figure>

<p>所有以<code>transforms</code> 为开头的行都将被添加了静态字段用于 transformations 。 你可以看到我们创建的两个 transformations: “InsertSource” 和 “MakeMap” 是我们给的 transformations 的别称. transformation 类型基于下面给的一系列内嵌 transformations。每个 transformation 类型都有额外的配置: HoistField 需要一个配置叫做 “field”,这是 map中原始字符串的字段名称。InsertField transformation 让我们指定字段名称和我们要添加的内容。</p>
<p>当我们对一个 sample file 运行 file source connector 操作，不做transformations 操作，然后使用<code>kafka-console-consumer.sh</code> 读取数据，结果如下:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&quot;foo&quot;</span><br><span class="line">&quot;bar&quot;</span><br><span class="line">&quot;hello world&quot;</span><br></pre></td></tr></table></figure>

<p>然后我们创建一个新的file connector,然后将这个transformations 添加到配置文件中。这次结果如下:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&#123;&quot;line&quot;:&quot;foo&quot;,&quot;data_source&quot;:&quot;test-file-source&quot;&#125;</span><br><span class="line">&#123;&quot;line&quot;:&quot;bar&quot;,&quot;data_source&quot;:&quot;test-file-source&quot;&#125;</span><br><span class="line">&#123;&quot;line&quot;:&quot;hello world&quot;,&quot;data_source&quot;:&quot;test-file-source&quot;&#125;</span><br></pre></td></tr></table></figure>

<p>你可以看到我们读取的行现在JSON map的一部分，并且还有一个静态值的额外字段。这只是用 transformations 做的一个简单的例子。</p>
<p>Kafka Connect 包含几个广泛适用的数据和 routing transformations</p>
<ul>
<li>InsertField - Add a field using either static data or record metadata</li>
<li>ReplaceField - Filter or rename fields</li>
<li>MaskField - Replace field with valid null value for the type (0, empty string, etc)</li>
<li>ValueToKey</li>
<li>HoistField - Wrap the entire event as a single field inside a Struct or a Map</li>
<li>ExtractField - Extract a specific field from Struct and Map and include only this field in results</li>
<li>SetSchemaMetadata - modify the schema name or version</li>
<li>TimestampRouter - Modify the topic of a record based on original topic and timestamp. Useful when using a sink that needs to write to different tables or indexes based on timestamps</li>
<li>RegexRouter - modify the topic of a record based on original topic, replacement string and a regular expression</li>
</ul>
<h2 id="REST-API"><a href="#REST-API" class="headerlink" title="REST API"></a>REST API</h2><p>由于Kafka Connect 旨在作为服务运行，它还提供了一个用于管理 connectors 的REST API。默认情况下，此服务在端口8083上运行。以下是当前支持的功能:</p>
<ul>
<li><code>GET /connectors</code> - return a list of active connectors</li>
<li><code>POST /connectors</code> - create a new connector; the request body should be a JSON object containing a string <code>name</code> field and an object <code>config</code> field with the connector configuration parameters</li>
<li><code>GET /connectors/&#123;name&#125;</code> - get information about a specific connector</li>
<li><code>GET /connectors/&#123;name&#125;/config</code> - get the configuration parameters for a specific connector</li>
<li><code>PUT /connectors/&#123;name&#125;/config</code> - update the configuration parameters for a specific connector</li>
<li><code>GET /connectors/&#123;name&#125;/status</code> - get current status of the connector, including if it is running, failed, paused, etc., which worker it is assigned to, error information if it has failed, and the state of all its tasks</li>
<li><code>GET /connectors/&#123;name&#125;/tasks</code> - get a list of tasks currently running for a connector</li>
<li><code>GET /connectors/&#123;name&#125;/tasks/&#123;taskid&#125;/status</code> - get current status of the task, including if it is running, failed, paused, etc., which worker it is assigned to, and error information if it has failed</li>
<li><code>PUT /connectors/&#123;name&#125;/pause</code> - pause the connector and its tasks, which stops message processing until the connector is resumed</li>
<li><code>PUT /connectors/&#123;name&#125;/resume</code> - resume a paused connector (or do nothing if the connector is not paused)</li>
<li><code>POST /connectors/&#123;name&#125;/restart</code> - restart a connector (typically because it has failed)</li>
<li><code>POST /connectors/&#123;name&#125;/tasks/&#123;taskId&#125;/restart</code> - restart an individual task (typically because it has failed)</li>
<li><code>DELETE /connectors/&#123;name&#125;</code> - delete a connector, halting all tasks and deleting its configuration</li>
</ul>
<p>Kafka Connect还提供用于获取有关 connector plugin 信息的REST API:</p>
<ul>
<li><code>GET /connector-plugins</code>- return a list of connector plugins installed in the Kafka Connect cluster. Note that the API only checks for connectors on the worker that handles the request, which means you may see inconsistent results, especially during a rolling upgrade if you add new connector jars</li>
<li><code>PUT /connector-plugins/&#123;connector-type&#125;/config/validate</code> - validate the provided configuration values against the configuration definition. This API performs per config validation, returns suggested values and error messages during validation.</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2020/11/29/08-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/03-Kafka/01-Kafka%E5%85%A5%E9%97%A8/" data-id="clmcxed2b00b8u8wa6x2u6gf0" data-title="Kafka入门" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Kafka/" rel="tag">Kafka</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-08-消息队列/02-RabbitMQ/05-MQ应用-分布式事务" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2020/11/15/08-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/02-RabbitMQ/05-MQ%E5%BA%94%E7%94%A8-%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1/" class="article-date">
  <time class="dt-published" datetime="2020-11-15T14:47:17.000Z" itemprop="datePublished">2020-11-15</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/RabbitMQ/">RabbitMQ</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2020/11/15/08-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/02-RabbitMQ/05-MQ%E5%BA%94%E7%94%A8-%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1/">MQ应用-分布式事务</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>MQ应用-分布式事务</p>
        
          <p class="article-more-link">
            <a href="/2020/11/15/08-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/02-RabbitMQ/05-MQ%E5%BA%94%E7%94%A8-%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1/#more">Read More</a>
          </p>
        
      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2020/11/15/08-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/02-RabbitMQ/05-MQ%E5%BA%94%E7%94%A8-%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1/" data-id="clmcxed1q00b2u8wa7vpa6fhy" data-title="MQ应用-分布式事务" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/RabbitMQ/" rel="tag">RabbitMQ</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-08-消息队列/02-RabbitMQ/04-RabbitMQ-集群高可用" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2020/10/26/08-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/02-RabbitMQ/04-RabbitMQ-%E9%9B%86%E7%BE%A4%E9%AB%98%E5%8F%AF%E7%94%A8/" class="article-date">
  <time class="dt-published" datetime="2020-10-26T14:59:43.000Z" itemprop="datePublished">2020-10-26</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/RabbitMQ/">RabbitMQ</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2020/10/26/08-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/02-RabbitMQ/04-RabbitMQ-%E9%9B%86%E7%BE%A4%E9%AB%98%E5%8F%AF%E7%94%A8/">RabbitMQ-集群高可用</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>RabbitMQ-集群高可用</p>
        
          <p class="article-more-link">
            <a href="/2020/10/26/08-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/02-RabbitMQ/04-RabbitMQ-%E9%9B%86%E7%BE%A4%E9%AB%98%E5%8F%AF%E7%94%A8/#more">Read More</a>
          </p>
        
      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2020/10/26/08-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/02-RabbitMQ/04-RabbitMQ-%E9%9B%86%E7%BE%A4%E9%AB%98%E5%8F%AF%E7%94%A8/" data-id="clmcxed1p00azu8wafc871z6i" data-title="RabbitMQ-集群高可用" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/RabbitMQ/" rel="tag">RabbitMQ</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-08-消息队列/02-RabbitMQ/03-RabbitMQ-高级特性详解" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2020/10/25/08-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/02-RabbitMQ/03-RabbitMQ-%E9%AB%98%E7%BA%A7%E7%89%B9%E6%80%A7%E8%AF%A6%E8%A7%A3/" class="article-date">
  <time class="dt-published" datetime="2020-10-25T08:31:37.000Z" itemprop="datePublished">2020-10-25</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/RabbitMQ/">RabbitMQ</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2020/10/25/08-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/02-RabbitMQ/03-RabbitMQ-%E9%AB%98%E7%BA%A7%E7%89%B9%E6%80%A7%E8%AF%A6%E8%A7%A3/">RabbitMQ-高级特性详解</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>RabbitMQ-高级特性详解</p>
        
          <p class="article-more-link">
            <a href="/2020/10/25/08-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/02-RabbitMQ/03-RabbitMQ-%E9%AB%98%E7%BA%A7%E7%89%B9%E6%80%A7%E8%AF%A6%E8%A7%A3/#more">Read More</a>
          </p>
        
      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2020/10/25/08-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/02-RabbitMQ/03-RabbitMQ-%E9%AB%98%E7%BA%A7%E7%89%B9%E6%80%A7%E8%AF%A6%E8%A7%A3/" data-id="clmcxed1i00awu8wa84eh5g8d" data-title="RabbitMQ-高级特性详解" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/RabbitMQ/" rel="tag">RabbitMQ</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-08-消息队列/02-RabbitMQ/02-RabbitMQ-AMQP模型详解" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2020/10/08/08-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/02-RabbitMQ/02-RabbitMQ-AMQP%E6%A8%A1%E5%9E%8B%E8%AF%A6%E8%A7%A3/" class="article-date">
  <time class="dt-published" datetime="2020-10-08T01:02:07.000Z" itemprop="datePublished">2020-10-08</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/RabbitMQ/">RabbitMQ</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2020/10/08/08-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/02-RabbitMQ/02-RabbitMQ-AMQP%E6%A8%A1%E5%9E%8B%E8%AF%A6%E8%A7%A3/">RabbitMQ-AMQP模型详解</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>RabbitMQ-AMQP 0.9.1 模型详解</p>
        
          <p class="article-more-link">
            <a href="/2020/10/08/08-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/02-RabbitMQ/02-RabbitMQ-AMQP%E6%A8%A1%E5%9E%8B%E8%AF%A6%E8%A7%A3/#more">Read More</a>
          </p>
        
      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2020/10/08/08-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/02-RabbitMQ/02-RabbitMQ-AMQP%E6%A8%A1%E5%9E%8B%E8%AF%A6%E8%A7%A3/" data-id="clmcxed1g00atu8wa05srhour" data-title="RabbitMQ-AMQP模型详解" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/RabbitMQ/" rel="tag">RabbitMQ</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-08-消息队列/02-RabbitMQ/01-RabbitMQ入门" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2020/10/03/08-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/02-RabbitMQ/01-RabbitMQ%E5%85%A5%E9%97%A8/" class="article-date">
  <time class="dt-published" datetime="2020-10-03T04:37:56.000Z" itemprop="datePublished">2020-10-03</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/RabbitMQ/">RabbitMQ</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2020/10/03/08-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/02-RabbitMQ/01-RabbitMQ%E5%85%A5%E9%97%A8/">RabbitMQ入门</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>RabbitMQ 是一个开源的AMQP实现，服务器端用Erlang语言编写，支持多种客户端。用于在分布式系</p>
<p>统中存储转发消息，在易用性、扩展性、高可用性等方面表现不俗。</p>
        
          <p class="article-more-link">
            <a href="/2020/10/03/08-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/02-RabbitMQ/01-RabbitMQ%E5%85%A5%E9%97%A8/#more">Read More</a>
          </p>
        
      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2020/10/03/08-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/02-RabbitMQ/01-RabbitMQ%E5%85%A5%E9%97%A8/" data-id="clmcxed1500alu8wacr9c6ond" data-title="RabbitMQ入门" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/RabbitMQ/" rel="tag">RabbitMQ</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-08-消息队列/01-ActiveMQ/01-ActiveMQ入门" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2020/08/16/08-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/01-ActiveMQ/01-ActiveMQ%E5%85%A5%E9%97%A8/" class="article-date">
  <time class="dt-published" datetime="2020-08-16T07:15:25.000Z" itemprop="datePublished">2020-08-16</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/ActiveMQ/">ActiveMQ</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2020/08/16/08-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/01-ActiveMQ/01-ActiveMQ%E5%85%A5%E9%97%A8/">ActiveMQ入门</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>ActiveMQ是一款用Java开发的、开源的、支持多种协议的、非常流行的消息服务（消息中间件）。因为它的支持多协议，支持工业标准的协议，所以我们可以跨平台、跨语言来使用它。</p>
        
          <p class="article-more-link">
            <a href="/2020/08/16/08-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/01-ActiveMQ/01-ActiveMQ%E5%85%A5%E9%97%A8/#more">Read More</a>
          </p>
        
      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2020/08/16/08-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/01-ActiveMQ/01-ActiveMQ%E5%85%A5%E9%97%A8/" data-id="clmcxed0z00agu8wa4zky16se" data-title="ActiveMQ入门" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/ActiveMQ/" rel="tag">ActiveMQ</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-12-Tomcat/01-Tomcat源码分析1" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2020/08/15/12-Tomcat/01-Tomcat%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%901/" class="article-date">
  <time class="dt-published" datetime="2020-08-15T08:50:45.000Z" itemprop="datePublished">2020-08-15</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Tomcat/">Tomcat</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2020/08/15/12-Tomcat/01-Tomcat%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%901/">Tomcat源码分析1</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>tomcat版本：9.0.53</p>
<h1 id="Tomcat简介"><a href="#Tomcat简介" class="headerlink" title="Tomcat简介"></a>Tomcat简介</h1><p>Tomcat实现了Java Servlet，JavaServer Page，Java表达式语言和Java的WebSocket技术的一个开源实现。说白了就是一个非常流行的Servlet Web容器。</p>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20210925151815.png"></p>
<h2 id="关键目录"><a href="#关键目录" class="headerlink" title="关键目录"></a>关键目录</h2><ol>
<li><strong>lib</strong><br>Tomcat依赖的jar包 </li>
<li><strong>logs</strong><br> catalina-xxx.log<br> localhost-xxx.log</li>
<li><strong>webapps</strong><br> web应用部署目录，eclipse中的配置演示</li>
</ol>
<h2 id="配置文件"><a href="#配置文件" class="headerlink" title="配置文件"></a>配置文件</h2><p>jconsol调试参数，观察参数变化带来的影响</p>
<ol>
<li>server.xml<br>调整连接池的大小<br>设置I&#x2F;O模式(BIO与NIO)<br>去掉AJP的Connector<br>移除access-log日志</li>
<li>web.xml<br>servlet标签：DefaultServlet、JspServlet<br>servlet-mapping标签：servlet的访问路径<br>mime-mapping标签，支持的内容类型：json、xml、html、jpg等</li>
</ol>
<h2 id="部署方式"><a href="#部署方式" class="headerlink" title="部署方式"></a>部署方式</h2><h3 id="显式部署"><a href="#显式部署" class="headerlink" title="显式部署"></a>显式部署</h3><ol>
<li><p>添加context元素方式(server.xml)</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">Host</span> <span class="attr">appBase</span>=<span class="string">&quot;webapps&quot;</span> <span class="attr">autoDeploy</span>=<span class="string">&quot;true&quot;</span> <span class="attr">name</span>=<span class="string">&quot;localhost&quot;</span> <span class="attr">unpackWARs</span>=<span class="string">&quot;true&quot;</span>&gt;</span> </span><br><span class="line">    <span class="tag">&lt;<span class="name">Context</span> <span class="attr">path</span>=<span class="string">&quot;/Demo1&quot;</span> <span class="attr">docBase</span>=<span class="string">&quot;d:/Demo1&quot;</span> <span class="attr">reloadable</span>=<span class="string">&quot;true&quot;</span>&gt;</span><span class="tag">&lt;/<span class="name">Context</span>&gt;</span> </span><br><span class="line"><span class="tag">&lt;/<span class="name">Host</span>&gt;</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>创建xml的方式</p>
<p>在conf&#x2F;Catalina&#x2F;localhost目录创建的xml文件</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version=<span class="string">&quot;1.0&quot;</span> encoding=<span class="string">&quot;UTF-8&quot;</span>?&gt;</span> </span><br><span class="line"><span class="tag">&lt;<span class="name">Context</span> <span class="attr">docBase</span>=<span class="string">&quot;d:/Demo1&quot;</span> <span class="attr">reloadable</span>=<span class="string">&quot;true&quot;</span>&gt;</span><span class="tag">&lt;/<span class="name">Context</span>&gt;</span></span><br></pre></td></tr></table></figure></li>
</ol>
<h3 id="隐式部署"><a href="#隐式部署" class="headerlink" title="隐式部署"></a>隐式部署</h3><p>将一个war文件或者整个应用程序复制到Tomcat的webapps</p>
<h1 id="Tomcat源码下载与构建"><a href="#Tomcat源码下载与构建" class="headerlink" title="Tomcat源码下载与构建"></a>Tomcat源码下载与构建</h1><p>IDE装载Tomcat项目源码</p>
<p>官方构建文档：<a target="_blank" rel="noopener" href="http://tomcat.apache.org/tomcat-9.0-doc/building.html">http://tomcat.apache.org/tomcat-9.0-doc/building.html</a></p>
<p>csdn博客：<a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_45859054/article/details/104726732">https://blog.csdn.net/qq_45859054/article/details/104726732</a></p>
<h1 id="Tomcat体系结构"><a href="#Tomcat体系结构" class="headerlink" title="Tomcat体系结构"></a>Tomcat体系结构</h1><p>Tomcat整体架构：<a target="_blank" rel="noopener" href="http://tomcat.apache.org/tomcat-9.0-doc/architecture/index.html">http://tomcat.apache.org/tomcat-9.0-doc/architecture/index.html</a></p>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20210925153446.png"></p>
<h2 id="Server"><a href="#Server" class="headerlink" title="Server"></a>Server</h2><p>在Tomcat中，Server代表整个容器。</p>
<h2 id="Service"><a href="#Service" class="headerlink" title="Service"></a>Service</h2><p>服务是一个中间组件，它在服务器内部，将一个或多个连接器连接到一个引擎。</p>
<h2 id="Engine"><a href="#Engine" class="headerlink" title="Engine"></a>Engine</h2><p>Engine也就是以前版本中的Container。引擎表示特定服务的请求处理管道。 服务可能有多个连接器，引擎接收并处理来自这些连接器的所有请求，将响应返回到适当的连接器以传输到客户端。 引擎接口可以实现，以提供自定义引擎，尽管这是不常见的。引擎可以通过jvm路由参数用于Tomcat服务器集群。</p>
<p>Engine包含：Host、Context、Wrapper这几个容器，他们都是Container子类型。</p>
<h2 id="Connerctor"><a href="#Connerctor" class="headerlink" title="Connerctor"></a>Connerctor</h2><p>Connector负责把接收到的请求解析出来然后封装成request和response对象然后交给Container处理。目前Connector支持http和ajp协议。</p>
<p>连接器处理与客户端的通信。 Tomcat有多个可用的连接器。 其中包括HTTP连接器、AJP连接器。当将Tomcat作为独立服务器运行时使用HTTP协议的HTTP连接器；当将Tomcat连接到Apache HTTPD服务器等Web服务器时使用的AJP协议的AJP连接器。 还可以创建自定义连接器。</p>
<h2 id="Host"><a href="#Host" class="headerlink" title="Host"></a>Host</h2><p>主机是网络名称的关联。 例如：<a target="_blank" rel="noopener" href="http://www.yourcompany.com能访问到tomcat服务器.引擎可能包含多个主机,主机元素还支持网络别名,如yourcompany.com和abc.yourcompany.com./">www.yourcompany.com能访问到Tomcat服务器。引擎可能包含多个主机，主机元素还支持网络别名，如yourcompany.com和abc.yourcompany.com。</a> 用户很少创建自定义主机，因为标准主机实现提供了重要的附加功能。</p>
<p>Host说白了就是我们所理解的虚拟主机。</p>
<h2 id="Context"><a href="#Context" class="headerlink" title="Context"></a>Context</h2><p>上下文表示Web应用程序。 主机可能包含多个上下文，每个上下文具有唯一的路径。 上下文接口可以实现创建自定义上下文，但这种情况很少，因为标准上下文提供了重要的附加功能。</p>
<p>Context就是我们所部属的具体Web应用的上下文，每个请求都在是相应的上下文里处理的。</p>
<h2 id="Wrapper"><a href="#Wrapper" class="headerlink" title="Wrapper"></a>Wrapper</h2><p>Wrapper是针对每个Servlet的Container，每个Servlet都有相应的Wrapper来管理。</p>
<p>查看tomcat中conf目录下的server.xml文件内容</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version=<span class="string">&quot;1.0&quot;</span> encoding=<span class="string">&quot;UTF-8&quot;</span>?&gt;</span> </span><br><span class="line"><span class="comment">&lt;!-- </span></span><br><span class="line"><span class="comment">Server标签对应org.apache.catalina.Server接口。 </span></span><br><span class="line"><span class="comment">Server元素表示整个Catalina servlet容器。 </span></span><br><span class="line"><span class="comment">它的属性代表整个servlet容器的特征。 </span></span><br><span class="line"><span class="comment">Server可以包含一个或多个Services,以及顶级命名资源集。 </span></span><br><span class="line"><span class="comment">--&gt;</span> </span><br><span class="line"><span class="tag">&lt;<span class="name">Server</span> <span class="attr">port</span>=<span class="string">&quot;8005&quot;</span> <span class="attr">shutdown</span>=<span class="string">&quot;SHUTDOWN&quot;</span>&gt;</span> </span><br><span class="line">    <span class="comment">&lt;!-- </span></span><br><span class="line"><span class="comment">    Service标签对应org.apache.catalina.Service接口。</span></span><br><span class="line"><span class="comment">    Service包含一个或多个Connectors,它们共享一个Container来处理所有请求。 </span></span><br><span class="line"><span class="comment">	--&gt;</span> </span><br><span class="line">    <span class="tag">&lt;<span class="name">Service</span> <span class="attr">name</span>=<span class="string">&quot;Catalina&quot;</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!-- </span></span><br><span class="line"><span class="comment">        Connector代表一个用来接收请求并返回的端 </span></span><br><span class="line"><span class="comment">        --&gt;</span> </span><br><span class="line">        <span class="tag">&lt;<span class="name">Connector</span> <span class="attr">port</span>=<span class="string">&quot;8080&quot;</span> <span class="attr">protocol</span>=<span class="string">&quot;HTTP/1.1&quot;</span> </span></span><br><span class="line"><span class="tag">                   <span class="attr">connectionTimeout</span>=<span class="string">&quot;20000&quot;</span> </span></span><br><span class="line"><span class="tag">                   <span class="attr">redirectPort</span>=<span class="string">&quot;8443&quot;</span>/&gt;</span> </span><br><span class="line">        </span><br><span class="line">        <span class="comment">&lt;!-- </span></span><br><span class="line"><span class="comment">        Tomcat的Engine实现独立分析请求中包含的HTTP头，并传递它们到适当的主机（虚拟主机）。</span></span><br><span class="line"><span class="comment">        --&gt;</span> </span><br><span class="line">        <span class="tag">&lt;<span class="name">Engine</span> <span class="attr">name</span>=<span class="string">&quot;Catalina&quot;</span> <span class="attr">defaultHost</span>=<span class="string">&quot;localhost&quot;</span>&gt;</span> </span><br><span class="line">            <span class="tag">&lt;<span class="name">Host</span> <span class="attr">name</span>=<span class="string">&quot;localhost&quot;</span> <span class="attr">appBase</span>=<span class="string">&quot;webapps&quot;</span> <span class="attr">unpackWARs</span>=<span class="string">&quot;true&quot;</span> <span class="attr">autoDeploy</span>=<span class="string">&quot;true&quot;</span>&gt;</span> </span><br><span class="line">                <span class="tag">&lt;<span class="name">Valve</span> <span class="attr">className</span>=<span class="string">&quot;org.apache.catalina.valves.AccessLogValve&quot;</span> <span class="attr">directory</span>=<span class="string">&quot;logs&quot;</span> <span class="attr">prefix</span>=<span class="string">&quot;localhost_access_log&quot;</span> <span class="attr">suffix</span>=<span class="string">&quot;.txt&quot;</span> <span class="attr">pattern</span>=<span class="string">&quot;%h %l %u %t <span class="symbol">&amp;quot;</span>%r<span class="symbol">&amp;quot;</span> %s %b&quot;</span>/&gt;</span> </span><br><span class="line">            <span class="tag">&lt;/<span class="name">Host</span>&gt;</span> </span><br><span class="line">        <span class="tag">&lt;/<span class="name">Engine</span>&gt;</span> </span><br><span class="line">    <span class="tag">&lt;/<span class="name">Service</span>&gt;</span> </span><br><span class="line"><span class="tag">&lt;/<span class="name">Server</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>那么在Tomcat中上面的这些核心组件是怎么协调运作的呢？</p>
<h1 id="Tomcat容器与运行机制"><a href="#Tomcat容器与运行机制" class="headerlink" title="Tomcat容器与运行机制"></a>Tomcat容器与运行机制</h1><h2 id="Tomcat启动步骤"><a href="#Tomcat启动步骤" class="headerlink" title="Tomcat启动步骤"></a>Tomcat启动步骤</h2><p>官网pdf：<a target="_blank" rel="noopener" href="http://tomcat.apache.org/tomcat-9.0-doc/architecture/startup/serverStartup.pdf">http://tomcat.apache.org/tomcat-9.0-doc/architecture/startup/serverStartup.pdf</a></p>
<p>Tomcat的启动方式有下面几种方式：</p>
<ol>
<li>通过命令行启动</li>
<li>通过Java程序，作为一个内嵌服务启动</li>
<li>作为windows服务自动启动</li>
</ol>
<p>无论哪种启动方式，他们的底层的启动流程和启动步骤都是一样的，我们来看看启动的过程中Tomcat都做了些什么？</p>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20210925154105.png"></p>
<p><strong>以命令行启动为例进行启动步骤分析</strong><br><strong>步骤1：通过命令行启动</strong><br><strong>Bootstrap</strong><br>Tomcat启动类，Tomcat启动都是通过org.apache.catalina.startup.Bootstrap类的main方法来进行启动的。它所做的工作包括：</p>
<ol>
<li>启动类加载器<br>commonLoader (common)-&gt; System Loader sharedLoader (shared)-&gt; commonLoader -&gt;<br>System Loader catalinaLoader(server) -&gt; commonLoader -&gt; System Loader<br>默认，commonLoader 用于sharedLoader 和catalinaLoader</li>
<li>加载启动类<br>org.apache.catalina.startup.Catalina setParentClassloader -&gt; sharedLoader<br>Thread.contextClassloader -&gt; catalinaLoader</li>
<li>Bootstrap的守护线程初始化方法执行完成</li>
</ol>
<p><strong>步骤2：处理命令行参数</strong><br>Bootstrap处理start、stop这样的启动命令参数，这里以start参数解释启动步骤。工作流程如下：</p>
<ol>
<li><p>Catalina.setAwait(true)</p>
</li>
<li><p>Catalina.load()</p>
<ol>
<li><p>initDirs()方法设置属性值，例如</p>
<figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">catalina.home</span> = <span class="string">D:/tomcat </span></span><br><span class="line"><span class="attr">catalina.base</span> =<span class="string">= catalina.home</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>initNaming初始化命名</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">setProperty(javax.naming.Context.INITIAL_CONTEXT_FACTORY, </span><br><span class="line">            org.apache.naming.java.javaURLContextFactory - &gt;<span class="keyword">default</span>)</span><br></pre></td></tr></table></figure>
</li>
<li><p>createStartDigester()</p>
</li>
<li><p>加载server.xml并解析<br> 自动的使用digester进行server.xml的解析<br> XML对象映射工具，它将创建server.xml<br> 实际上此时容器的启动尚未开始。</p>
</li>
<li><p>日志输出处理<br> 将System.out、System.err绑定到SystemLogHandler类上</p>
</li>
<li><p>调用所有的组件初始化方法，确保每个对象都注册到了JMX代理上<br>在这个过程中，连接器也初始化适配器。适配器是执行请求预处理的组件。典型的适配器是HTTP1.1（如果没有指定协议，则默认为，org.apache.coyote.http11.Http11NioProtocol）</p>
</li>
</ol>
</li>
<li><p>Catalina.start()</p>
<ol>
<li><p>启动NamingContext绑定所有JNDI引用到NamingContext中 </p>
</li>
<li><p>启动&lt; Server&gt;标签下的service</p>
</li>
<li><p>通过Service启动StandardHost</p>
</li>
</ol>
<ul>
<li>配置ErrorReportValve以对不同的HTTP错误代码执行正确的HTML输出</li>
<li>启动管道中的阀门Valve（至少是ErrorReportValve）</li>
<li>配置StandardHostValve<br>这个阀门将Webapp类加载器绑定到线程上下文<br>它还会找到请求的会话<br>并调用上下文管道</li>
<li>启动HostConfig组件<br>此组件部署所有webapps，(webapps &amp; conf&#x2F;Catalina&#x2F;localhost&#x2F;*.xml)<br>HostConfig将为你的上下文创建一个Digester摘要器，这个Digester将调研<br>ContextConfig.start()方法<br>ContextConfig.start()将会处理默认的web.xml(conf&#x2F;web.xml)，然后再处理应用的<br>web.xml (WEB-INF&#x2F;web.xml)</li>
</ul>
<ol start="4">
<li><p>热部署检查</p>
<p>在容器（StandardEngine）的生存期内，有一个后台线程不断检查上下文是否已更改。如果上下文发生变化（war包的时间戳，context.xml文件，web.xml文件)，然后发出重新加载（stop&#x2F;remove&#x2F;deploy&#x2F;start）</p>
</li>
</ol>
</li>
<li><p>Tomcat 在HTTP端口上开始接受请求</p>
</li>
<li><p>Servlet类调用</p>
</li>
</ol>
<h2 id="Tomcat处理请求的过程"><a href="#Tomcat处理请求的过程" class="headerlink" title="Tomcat处理请求的过程"></a>Tomcat处理请求的过程</h2><p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20210925155036.png"></p>
<ol>
<li><p>用户点击网页内容，请求被发送到本机端口8080，被在那里监听的Coyote HTTP&#x2F;1.1 Connector获得。</p>
</li>
<li><p>Connector把该请求交给它所在的Service的Engine来处理，并等待Engine的回应。</p>
</li>
<li><p>Engine获得请求localhost&#x2F;test&#x2F;index.jsp，匹配所有的虚拟主机Host。 </p>
</li>
<li><p>Engine匹配到名为localhost的Host（即使匹配不到也把请求交给该Host处理，因为该Host被定义为该Engine的默认主机），名为localhost的Host获得请求&#x2F;test&#x2F;index.jsp，匹配它所拥有的所有的Context。Host匹配到路径为&#x2F;test的Context（如果匹配不到就把该请求交给路径名为“ ”的Context去处理）。 path&#x3D;“&#x2F;test”的Context获得请求&#x2F;index.jsp，在它的mapping table中寻找出对应的Servlet。Context匹配到URL PATTERN为*.jsp的Servlet,对应于JspServlet类。</p>
</li>
<li><p>构造HttpServletRequest对象和HttpServletResponse对象，作为参数调用JspServlet的doGet（）或doPost（）.执行业务逻辑、数据存储等程序。</p>
</li>
<li><p>Context把执行完之后的HttpServletResponse对象返回给Host。 </p>
</li>
<li><p>Host把HttpServletResponse对象返回给Engine。 </p>
</li>
<li><p>Engine把HttpServletResponse对象返回Connector。 </p>
</li>
<li><p>Connector把HttpServletResponse对象返回给客户Browser。</p>
</li>
</ol>
<h2 id="Tomcat启动步骤流程图总结"><a href="#Tomcat启动步骤流程图总结" class="headerlink" title="Tomcat启动步骤流程图总结"></a>Tomcat启动步骤流程图总结</h2><p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/tomcat%20request-process.png"></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2020/08/15/12-Tomcat/01-Tomcat%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%901/" data-id="clmcxec5e0000u8wa65b674xj" data-title="Tomcat源码分析1" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Tomcat/" rel="tag">Tomcat</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-12-Tomcat/02-Tomcat源码分析2" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2020/08/15/12-Tomcat/02-Tomcat%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%902/" class="article-date">
  <time class="dt-published" datetime="2020-08-15T08:50:45.000Z" itemprop="datePublished">2020-08-15</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Tomcat/">Tomcat</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2020/08/15/12-Tomcat/02-Tomcat%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%902/">Tomcat源码分析2</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="Tomcat中的组件"><a href="#Tomcat中的组件" class="headerlink" title="Tomcat中的组件"></a>Tomcat中的组件</h1><h2 id="Bootstrap"><a href="#Bootstrap" class="headerlink" title="Bootstrap"></a>Bootstrap</h2><p>Tomcat的启动类，最终交给Catalina对象初始化Tomcat，它所完成的工作。</p>
<ul>
<li>初始化类加载器</li>
<li>初始化Catalina，通过反射进行调用</li>
<li>处理启动参数命令</li>
</ul>
<h2 id="ClassLoader"><a href="#ClassLoader" class="headerlink" title="ClassLoader"></a>ClassLoader</h2><h3 id="JVM类加载器及双亲委派"><a href="#JVM类加载器及双亲委派" class="headerlink" title="JVM类加载器及双亲委派"></a>JVM类加载器及双亲委派</h3><h4 id="JVM类加载器结构"><a href="#JVM类加载器结构" class="headerlink" title="JVM类加载器结构"></a>JVM类加载器结构</h4><p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20210926091359.png"></p>
<p>示例代码</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">Main</span> &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        <span class="type">Main</span> <span class="variable">main</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Main</span>();</span><br><span class="line">        System.out.println(main.getClass().getClassLoader());</span><br><span class="line">        System.out.println(main.getClass().getClassLoader().getParent());</span><br><span class="line">        System.out.println(main.getClass().getClassLoader().getParent().getParent());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 输出</span></span><br><span class="line">sun.misc.Launcher$AppClassLoader@18b4aac2</span><br><span class="line">sun.misc.Launcher$ExtClassLoader@372f7a8d</span><br><span class="line"><span class="literal">null</span> <span class="comment">// 由于启动类加载器是 C/C++ 语言写的，所以输出为 null</span></span><br></pre></td></tr></table></figure>

<h4 id="双亲委派机制"><a href="#双亲委派机制" class="headerlink" title="双亲委派机制"></a>双亲委派机制</h4><ul>
<li>如果一个类加载器收到了类加载请求，它并不会自己先去加载，而是把这个请求委托给父类的加载器去执行。</li>
<li>如果父类加载器还存在其父类加载器，则进一步向上委托，依次递归，请求最终将到达顶层的启动类加载器。</li>
<li>如果父类加载器可以完成类加载任务，就成功返回，倘若父类加载器无法完成此加载任务，子加载器才会尝试自己去加载，这就是双亲委派模式。</li>
</ul>
<p>每个儿子都很懒，每次有活就丢给父亲去干，直到父亲说这件事我也干不了时，儿子自己才想办法去完成。</p>
<h4 id="双亲委派优势"><a href="#双亲委派优势" class="headerlink" title="双亲委派优势"></a>双亲委派优势</h4><ul>
<li><p>避免重复加载</p>
<p>Java类随着它的类加载器一起具备了一种带有优先级的层次关系，通过这种层级关可以避免类的重复加载，当父亲已经加载了该类时，子ClassLoader就没有必要再加载一次。</p>
</li>
<li><p>安全-防止核心API库被篡改</p>
<p>java核心api中定义类型不会被随意替换，假设通过网络传递一个名为java.lang.Integer的类，通过双亲委托模式传递到启动类加载器，而启动类加载器在核心Java API发现这个名字的类，发现该类已被加载，并不会重新加载网络传递的过来的java.lang.Integer，而直接返回已加载过的Integer.class，这样便可以防止核心API库被随意篡改。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> java.lang; <span class="comment">// 自定义的包,伪装成java.lang.String</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">String</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        System.out.println(<span class="string">&quot;这是自定义的java.lang.String类&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>由于 jre\lib\ext 中存在 java.lang.String 类，当加载该类的时候，根据全限定名进行查找，找到后由启动类加载器加载，发现 String 类中不包含 main() 方法，因此程序出错。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">- 错误: 在类 java.lang.String 中找不到 main 方法, 请将 main 方法定义为:</span><br><span class="line">     <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span></span><br><span class="line">  否则 JavaFX 应用程序类必须扩展javafx.application.Application</span><br></pre></td></tr></table></figure></li>
</ul>
<h3 id="Tomcat的类加载器"><a href="#Tomcat的类加载器" class="headerlink" title="Tomcat的类加载器"></a>Tomcat的类加载器</h3><p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20210926093532.png"></p>
<p>Catalina类加载器和Shared类加载器，他们并不是父子关系，而是兄弟关系。为啥这样设计？</p>
<ol>
<li>保证应用程序各自类库的独立隔离，一个Web容器可能需要部署多个应用程序，不同的应用程序可能会依赖同一个第三方类库的不同版本，不能要求同一个类库在同一个服务器只有一份，因此要保证每个应用程序的类库都是独立的，保证相互隔离。</li>
<li>共享相同版本类库，部署在同一个Web容器中相同的类库相同的版本可以共享。否则，如果服务器有10个应用程序，那么要有10份相同的类库加载进虚拟机，显然不合适。</li>
<li>安全-隔离Web容器与应用程序类库，Web容器也有自己依赖的类库，不能于应用程序的类库混淆。基于安全考虑，应该让容器的类库和程序的类库隔离开来。</li>
<li>支持JSP热修改，Web容器要支持jsp的修改，jsp文件最终编译成class文件才能在虚拟机中运行，在程序运行后修改jsp是常见的事，否则要你何用？ 所以，web容器需要支持 jsp 修改后不用重启。</li>
</ol>
<h4 id="打破双亲委派机制"><a href="#打破双亲委派机制" class="headerlink" title="打破双亲委派机制"></a>打破双亲委派机制</h4><p>WebAppClassLoader与JasperLoader查找类则不会先往上查找，而是直接在自己类加载器中进行查找，不遵循双亲委派机制。</p>
<h4 id="为什么不遵循双亲委派机制？"><a href="#为什么不遵循双亲委派机制？" class="headerlink" title="为什么不遵循双亲委派机制？"></a>为什么不遵循双亲委派机制？</h4><ol>
<li><p>如果使用默认的类加载器机制，那么是无法加载两个相同类库的不同版本的，默认的类加器是不管你是什么版本的，只在乎你的全限定类名，并且只有一份。tomcat 为了实现隔离性，没有遵守双亲委派机制. </p>
</li>
<li><p>每个webappClassLoader加载自己的目录下的class文件，不会传递给父类加载器。</p>
</li>
<li><p>每个jsp文件对应一个唯一的类加载器，当一个jsp文件修改了，就直接卸载这个jsp类加载器。重新创建类加载器，重新加载jsp文件。双亲委派机制无法做到。</p>
</li>
</ol>
<p>所以双亲委派机制不能满足Tomcat的需求。</p>
<p><strong>如果Tomcat 的 CommonClassLoader 想加载 WebAppClassLoader 中的类，该怎么办？</strong></p>
<p>可以使用线程上下文类加载器实现，使用线程上下文加载器，让父类加载器请求子类加载器去完成类加载的动作。</p>
<h2 id="Catalina"><a href="#Catalina" class="headerlink" title="Catalina"></a>Catalina</h2><p><code>Catalina</code> 是Tomcat的核心组件，是Servlet容器，Catalina包含了所有的容器组件，其他模块均为Catalina提供支撑。通过Coyote模块提供连接通信，Jasper模块提供JSP引擎，Naming提供JNDI服务，Juli提供日志服务。结构如下：</p>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20210926102219.png"></p>
<p>主要的功能包括接收请求，处理请求，返回结果，这些具体的实现是在catalina里面的子容器里面。</p>
<h3 id="Catalina类结构"><a href="#Catalina类结构" class="headerlink" title="Catalina类结构"></a>Catalina类结构</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//org.apache.catalina.startup.Catalina </span></span><br><span class="line"><span class="comment">// 用于await的flag </span></span><br><span class="line"><span class="keyword">protected</span> <span class="type">boolean</span> <span class="variable">await</span> <span class="operator">=</span> <span class="literal">false</span>; </span><br><span class="line"><span class="comment">// Server配置的文件路，Server.xml配置文件的地址 </span></span><br><span class="line"><span class="keyword">protected</span> <span class="type">String</span> <span class="variable">configFile</span> <span class="operator">=</span> <span class="string">&quot;conf/server.xml&quot;</span>; </span><br><span class="line"><span class="comment">// 此server的shared 类加载器，父类加载器 </span></span><br><span class="line"><span class="keyword">protected</span> <span class="type">ClassLoader</span> <span class="variable">parentClassLoader</span> <span class="operator">=</span> Catalina.class.getClassLoader();</span><br><span class="line"><span class="comment">// Server组件，通过digster工具解析server.xml文件构造该对象</span></span><br><span class="line"><span class="keyword">protected</span> <span class="type">Server</span> <span class="variable">server</span> <span class="operator">=</span> <span class="literal">null</span>; </span><br><span class="line"><span class="comment">// 使用shutdown钩子的flag </span></span><br><span class="line"><span class="keyword">protected</span> <span class="type">boolean</span> <span class="variable">useShutdownHook</span> <span class="operator">=</span> <span class="literal">true</span>; </span><br><span class="line"><span class="comment">// Shutdown钩子实例，用户shutdown时的钩子 </span></span><br><span class="line"><span class="keyword">protected</span> <span class="type">Thread</span> <span class="variable">shutdownHook</span> <span class="operator">=</span> <span class="literal">null</span>; </span><br><span class="line"><span class="comment">// 默认需要开启Naming，是否需要启动JNDI的标识 </span></span><br><span class="line"><span class="keyword">protected</span> <span class="type">boolean</span> <span class="variable">useNaming</span> <span class="operator">=</span> <span class="literal">true</span>; </span><br><span class="line"><span class="comment">// 预防重复加载的标记字段 </span></span><br><span class="line"><span class="keyword">protected</span> <span class="type">boolean</span> <span class="variable">loaded</span> <span class="operator">=</span> <span class="literal">false</span>;</span><br></pre></td></tr></table></figure>

<p>Catalina执行的方法</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 加载server.xml文件，并启动Server </span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">load</span><span class="params">(String args[])</span>; </span><br><span class="line"></span><br><span class="line"><span class="comment">// 调用Server的start()方法，注册shutdown的钩子到JVM </span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">start</span><span class="params">()</span>; </span><br><span class="line"></span><br><span class="line"><span class="comment">// 关闭catalina中的server </span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">stop</span><span class="params">()</span>;</span><br></pre></td></tr></table></figure>

<h3 id="Catalina-components"><a href="#Catalina-components" class="headerlink" title="Catalina components"></a>Catalina components</h3><p>Catalina中的核心组件有：Server、Service、Executor、Container（Context、Engine、Host、Wrapper能够处理请求进行响应的组件）等。</p>
<h2 id="Digester"><a href="#Digester" class="headerlink" title="Digester"></a>Digester</h2><h3 id="Digester架构"><a href="#Digester架构" class="headerlink" title="Digester架构"></a>Digester架构</h3><p>Digester采用SAX时间机制解析xml文件，Digester整体架构如下。</p>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20210926103944.png"></p>
<ul>
<li>最上面的 <code>DefaultHandler</code> 来自SAX， <code>Digester</code> 集成该类，足以说明 <code>Digester</code> 底层用的是SAX了。</li>
<li><code>Client</code> 表示调用Digester的Java代码，比如我们这里的Catalina。 </li>
<li><code>Digester</code> 是Digester组件的核心类和入口类，Client需要实例化该类，设置改类，并调用里面的参数。</li>
<li><code>Rules</code> 是一个保存XML的节点和规则的映射关系的接口，默认实现类是 <code>RulesBase</code></li>
<li><code>RuesBase</code> 是 <code>Rules</code> 的默认实现类。当有一个XML节点开始解析时，会在这里面找是否有对应的节点，并根据节点查找对应的处理规则。</li>
<li><code>Rule</code> 对节点的处理方法的接口，内置的或者自定义的规则都是集成该接口。</li>
<li><code>Rule</code> 是接口 <code>Rule</code> 的很多实现，根据具体的实现不同而不同。因为XML里面的内容是变化的，根据自己的XML内容进行实现。</li>
</ul>
<h4 id="使用Digester解析XML文档的流程："><a href="#使用Digester解析XML文档的流程：" class="headerlink" title="使用Digester解析XML文档的流程："></a>使用Digester解析XML文档的流程：</h4><ol>
<li>首先，Client需要创建一个Digester对象。</li>
<li>然后，Client必须根据自己的XML格式来添加所有的Rule。 </li>
<li>添加完Rule后，Client调用Digester的parse操作来解析XML文件。</li>
<li>Digester实现了SAX的接口，解析时遇到具体的XML对象时会调用startElement等接口函数。</li>
<li>在这些SAX接口函数中，会扫描规则链(RulesBase)，找到匹配规则，规则匹配一般都是根据具体的元素名称来进行匹配。</li>
<li>找到对应的rule后，依次执行rule，这里的startElement对应的是begin操作，endElement对应的是end操作，具体要做的事情都在每个rule的begin，body，end函数中。</li>
<li>文档结束后，会执行所有rule的finish函数。</li>
</ol>
<h3 id="解析server-xml文件"><a href="#解析server-xml文件" class="headerlink" title="解析server.xml文件"></a>解析server.xml文件</h3><p>Digester用栈来维护每个<strong>正在解析的</strong>对象，开始解析时就放入栈，解析完就弹出栈。</p>
<p>Catalina的使用步骤</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//3.用digester解析server.xml文件，把配置文件中的配置解析成java对象 </span></span><br><span class="line"><span class="comment">//3.1.准备好用来解析server.xml文件需要用的digester。 </span></span><br><span class="line"><span class="type">Digester</span> <span class="variable">digester</span> <span class="operator">=</span> createStartDigester(); </span><br><span class="line"><span class="comment">//3.2.server.xml文件作为一个输入流传入 </span></span><br><span class="line"><span class="type">File</span> <span class="variable">file</span> <span class="operator">=</span> configFile(); </span><br><span class="line"><span class="type">InputStream</span> <span class="variable">inputStream</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">FileInputStream</span>(file); </span><br><span class="line"><span class="comment">//3.3.使用inputStream构造一个sax的inputSource </span></span><br><span class="line"><span class="type">InputSource</span> <span class="variable">inputSource</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">InputSource</span>(file.toURI().toURL().toString());</span><br><span class="line">inputSource.setByteStream(inputStream); </span><br><span class="line"><span class="comment">//3.4.把当前类压入到digester的栈顶，用来作为digester解析出来的对象的一种引用 </span></span><br><span class="line">digester.push(<span class="built_in">this</span>); </span><br><span class="line"><span class="comment">//3.5.调用digester的parse()方法进行解析。 </span></span><br><span class="line">digester.parse(inputSource);</span><br></pre></td></tr></table></figure>

<p>创建Digester方法</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 创建一个digester实例 </span></span><br><span class="line"><span class="type">Digester</span> <span class="variable">digester</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Digester</span>(); </span><br><span class="line"><span class="comment">//是否对xml文档进行类似XSD等类型的校验，默认为fasle。 </span></span><br><span class="line">digester.setValidating(<span class="literal">false</span>); </span><br><span class="line"><span class="comment">//是否进行节点设置规则校验，如果xml中相应节点没有设置解析规则会在控制台显示提示信息。 </span></span><br><span class="line">digester.setRulesValidation(<span class="literal">true</span>); </span><br><span class="line"><span class="comment">//将XML节点中的className作为假属性，不必调用默认的setter方法 </span></span><br><span class="line"><span class="comment">//(一般的节点属性在解析时会以属性作为入参调用该节点相应对象的setter方法，而className属性的用</span></span><br><span class="line"><span class="comment">// 提示解析器使用该属性的值作来实例化对象) </span></span><br><span class="line">Map&lt;Class&lt;?&gt;, List&lt;String&gt;&gt; fakeAttributes = <span class="keyword">new</span> <span class="title class_">HashMap</span>&lt;&gt;(); </span><br><span class="line">List&lt;String&gt; objectAttrs = <span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;&gt;(); </span><br><span class="line">objectAttrs.add(<span class="string">&quot;className&quot;</span>);</span><br><span class="line">fakeAttributes.put(Object.class, objectAttrs); </span><br><span class="line"><span class="comment">// Ignore attribute added by Eclipse for its internal tracking </span></span><br><span class="line">List&lt;String&gt; contextAttrs = <span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;&gt;(); </span><br><span class="line">contextAttrs.add(<span class="string">&quot;source&quot;</span>); </span><br><span class="line">fakeAttributes.put(StandardContext.class, contextAttrs); </span><br><span class="line">digester.setFakeAttributes(fakeAttributes); </span><br><span class="line"><span class="comment">//使用当前线程的上下文类加载器，主要加载FactoryCreateRule和ObjectCreateRule</span></span><br><span class="line">digester.setUseContextClassLoader(<span class="literal">true</span>);</span><br></pre></td></tr></table></figure>

<p>创建Server实例</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 第一个参数指定操作的xml节点，这里值server节点，server对象最终由Catalina装配 </span></span><br><span class="line">digester.addObjectCreate(<span class="string">&quot;Server&quot;</span>,<span class="string">&quot;org.apache.catalina.core.StandardServer&quot;</span>,<span class="string">&quot;className&quot;</span>); </span><br><span class="line">digester.addSetProperties(<span class="string">&quot;Server&quot;</span>); </span><br><span class="line">digester.addSetNext(<span class="string">&quot;Server&quot;</span>,<span class="string">&quot;setServer&quot;</span>,<span class="string">&quot;org.apache.catalina.Server&quot;</span>);</span><br></pre></td></tr></table></figure>

<p>创建对象，设置属性，调用方法添加到Catalina类中。</p>
<p>其他的组件添加都类似，Service、Lifecycle、Host、Executor、Connector等等。</p>
<h2 id="Lifecycle"><a href="#Lifecycle" class="headerlink" title="Lifecycle"></a>Lifecycle</h2><p>Tomcat的卡特琳娜（Catalina）由许多组件构成。当Catalina启动的时候，这些组件也要跟着一起启动，并且当Catalina关闭的时候，这些组件也要同时关闭，并且要进行必要的清理操作，释放资源。例如，当容器关闭的时候，需要调用已加载的servlet对象的destroy方法，session对象也要持久化到secondary storage（二级存储，通常指的就是硬盘）。这就要求所有Component有一致的方法，可以统一处理。</p>
<p>整体采用了观察者设计模式。</p>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20210926111352.png"></p>
<p>Lifecycle接口实现体系：</p>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20210926111708.png"></p>
<p>子接口包含了Server、Service、Container等各组件，可见基本所有核心组件都实现了Lifecycle接口。</p>
<h2 id="Server"><a href="#Server" class="headerlink" title="Server"></a>Server</h2><p>Server组件用于描述一个启动的Tomcat实例，一个Tocmat被启动，在操作系统中占用一个进程号，提供web服务的功能，那个这个整个服务用Server来表示。</p>
<h3 id="StandardServer结构"><a href="#StandardServer结构" class="headerlink" title="StandardServer结构"></a>StandardServer结构</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">final</span> <span class="keyword">class</span> <span class="title class_">StandardServer</span>&#123; </span><br><span class="line">    <span class="comment">//1.java命令和地址服务相 </span></span><br><span class="line">    <span class="comment">// JNDI的context </span></span><br><span class="line">    <span class="keyword">private</span> javax.naming.<span class="type">Context</span> <span class="variable">globalNamingContext</span> <span class="operator">=</span> <span class="literal">null</span> </span><br><span class="line">	<span class="comment">// JNDI的resource </span></span><br><span class="line">	<span class="keyword">private</span> <span class="type">NamingResourcesImpl</span> <span class="variable">globalNamingResources</span> <span class="operator">=</span> <span class="literal">null</span> </span><br><span class="line">	<span class="comment">// 作用与web application的JDNI监听器 </span></span><br><span class="line">	<span class="keyword">private</span> <span class="keyword">final</span> NamingContextListener namingContextListener; </span><br><span class="line">	<span class="comment">//2.shutdown服务相关 </span></span><br><span class="line">	<span class="comment">// 用于等待shutdown命令的端口号 </span></span><br><span class="line">	<span class="keyword">private</span> <span class="type">int</span> <span class="variable">port</span> <span class="operator">=</span> <span class="number">8905</span> </span><br><span class="line">	<span class="comment">// 用于等待shutdown命令的地址 </span></span><br><span class="line">	<span class="keyword">private</span> <span class="type">String</span> <span class="variable">address</span> <span class="operator">=</span> <span class="string">&quot;localhost&quot;</span>; </span><br><span class="line">	<span class="comment">// shutdown服务在等待的命令 </span></span><br><span class="line">	<span class="keyword">private</span> <span class="type">String</span> <span class="variable">shutdown</span> <span class="operator">=</span> <span class="string">&quot;SHUTDOWN&quot;</span>; </span><br><span class="line">	<span class="comment">// 执行await()的线程</span></span><br><span class="line">	<span class="keyword">private</span> <span class="keyword">volatile</span> <span class="type">Thread</span> <span class="variable">awaitThread</span> <span class="operator">=</span> <span class="literal">null</span>;</span><br><span class="line">	<span class="comment">// 用于等待shutdown的socket对象 </span></span><br><span class="line">	<span class="keyword">private</span> <span class="keyword">volatile</span> <span class="type">ServerSocket</span> <span class="variable">awaitSocket</span> <span class="operator">=</span> <span class="literal">null</span>; </span><br><span class="line">	<span class="comment">// stopAwait的标记位 </span></span><br><span class="line">	<span class="keyword">private</span> <span class="keyword">volatile</span> <span class="type">boolean</span> <span class="variable">stopAwait</span> <span class="operator">=</span> <span class="literal">false</span>; </span><br><span class="line">	<span class="comment">//3.子组件Service相关 </span></span><br><span class="line">	<span class="comment">// Server下的service集合 </span></span><br><span class="line">	<span class="keyword">private</span> Service services[] = <span class="keyword">new</span> <span class="title class_">Service</span>[<span class="number">0</span>] </span><br><span class="line">	<span class="comment">// Service用到的锁 </span></span><br><span class="line">	<span class="keyword">private</span> <span class="keyword">final</span> <span class="type">Object</span> <span class="variable">servicesLock</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Object</span>(); </span><br><span class="line">	<span class="comment">//4.父类加载器 </span></span><br><span class="line">    <span class="comment">//父类加载器 </span></span><br><span class="line">    <span class="keyword">private</span> <span class="type">ClassLoader</span> <span class="variable">parentClassLoader</span> <span class="operator">=</span> <span class="literal">null</span>; </span><br><span class="line">    <span class="comment">//5.catalina相关的 </span></span><br><span class="line">    <span class="comment">//catalinaHome的地址 </span></span><br><span class="line">    <span class="keyword">private</span> <span class="type">File</span> <span class="variable">catalinaHome</span> <span class="operator">=</span> <span class="literal">null</span>; </span><br><span class="line">    <span class="comment">//catalinaBase的地址 </span></span><br><span class="line">    <span class="keyword">private</span> <span class="type">File</span> <span class="variable">catalinaBase</span> <span class="operator">=</span> <span class="literal">null</span>; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="StandardServer方法"><a href="#StandardServer方法" class="headerlink" title="StandardServer方法"></a>StandardServer方法</h3><p>启动Server需要调用两个方法，在真正的start()方法之前还需要执行init()方法，init方法是一个pre-start()方法。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 初始化动作 </span></span><br><span class="line"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">initInternal</span><span class="params">()</span> <span class="keyword">throws</span> LifecycleException; </span><br><span class="line"><span class="comment">// 启动方法 </span></span><br><span class="line"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">startInternal</span><span class="params">()</span> <span class="keyword">throws</span> LifecycleException; </span><br><span class="line"><span class="comment">// 关闭处理 </span></span><br><span class="line"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">stopInternal</span><span class="params">()</span> <span class="keyword">throws</span> LifecycleException;</span><br></pre></td></tr></table></figure>

<p>关闭流程是怎样的？</p>
<p>Tomcat的main线程在启动完所有的组件后，自己开一个socket服务端，在指定的端口（默认8005）上进行监听，一直到有shutdown命令发送过来，就退出socket的等待，开始执行关闭方法。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// org.apache.catalina.startup.Catalina#start </span></span><br><span class="line"><span class="comment">// 等待处理及停止 </span></span><br><span class="line"><span class="keyword">if</span> (await) &#123; </span><br><span class="line">    await(); </span><br><span class="line">    stop(); </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>await方法中则是Server类中的等待关闭命令的socket实现。stop方法则是Catalina中进行生命周期状态控制，以及Server类的关闭及销毁方法实现。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// org.apache.catalina.startup.Catalina#stop </span></span><br><span class="line"><span class="comment">// Shut down the server </span></span><br><span class="line"><span class="keyword">try</span> &#123; </span><br><span class="line">    <span class="type">Server</span> <span class="variable">s</span> <span class="operator">=</span> getServer(); </span><br><span class="line">    <span class="type">LifecycleState</span> <span class="variable">state</span> <span class="operator">=</span> s.getState();</span><br><span class="line">    <span class="keyword">if</span> (LifecycleState.STOPPING_PREP.compareTo(state) &lt;= <span class="number">0</span> &amp;&amp; LifecycleState.DESTROYED.compareTo(state) &gt;= <span class="number">0</span>) &#123; </span><br><span class="line">	<span class="comment">// Nothing to do. stop() was already called </span></span><br><span class="line">    &#125; <span class="keyword">else</span> &#123; </span><br><span class="line">	<span class="comment">//调用Server的stop方法 </span></span><br><span class="line">    s.stop(); </span><br><span class="line">    <span class="comment">//调用Server的destroy方法 </span></span><br><span class="line">    s.destroy(); </span><br><span class="line">    &#125; </span><br><span class="line">&#125; <span class="keyword">catch</span> (LifecycleException e) &#123; </span><br><span class="line">    log.error(<span class="string">&quot;Catalina.stop&quot;</span>, e); </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>Server的stop()方法</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">stopInternal</span><span class="params">()</span> <span class="keyword">throws</span> LifecycleException &#123; </span><br><span class="line">    <span class="comment">// 1.设置生命周期的状态， </span></span><br><span class="line">    setState(LifecycleState.STOPPING); </span><br><span class="line">    fireLifecycleEvent(CONFIGURE_STOP_EVENT, <span class="literal">null</span>); </span><br><span class="line">    <span class="comment">// 2.调用所有的Service的stop方法 </span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; services.length; i++) &#123; </span><br><span class="line">        services[i].stop(); </span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 3.停止JDNI </span></span><br><span class="line">    globalNamingResources.stop(); </span><br><span class="line">    <span class="comment">// 4.清理等待线程和socket </span></span><br><span class="line">    stopAwait(); </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>Server的destroy()方法</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">destroyInternal</span><span class="params">()</span> <span class="keyword">throws</span> LifecycleException &#123; </span><br><span class="line">    <span class="comment">// 1.调用所有的service的destroy()方法 </span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; services.length; i++) &#123; </span><br><span class="line">    	services[i].destroy(); </span><br><span class="line">    &#125;</span><br><span class="line">	<span class="comment">//2.调用JNDI的destroy() </span></span><br><span class="line">	globalNamingResources.destroy(); </span><br><span class="line">    <span class="comment">//3.解绑MBean,和String Cache，对应init方法中的注册。 </span></span><br><span class="line">    unregister(onameMBeanFactory); </span><br><span class="line">    unregister(onameStringCache); </span><br><span class="line">    <span class="comment">//4.调用父类的方法 </span></span><br><span class="line">    <span class="built_in">super</span>.destroyInternal(); </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>通过前面的分析Server相关联的组件有：多个Service组件，JDNI组件，JMX组件，类加载器组件</p>
<h2 id="Service"><a href="#Service" class="headerlink" title="Service"></a>Service</h2><p>Tomcat中只有一个Server，一个Server可以用多个Service，Server掌握着整个Tomcat的生死大权，Service是对外提供服务的。</p>
<p>一个Service可以有多个Connector和一个Container。Connector用于接收请求并将请求封装成Request和Response的具体处理。Container用于封装和管理Servlet，以及reqeust请求的具体处理。</p>
<p>一个Service包含多个Connector和一个Engine，两者的关联关系使用Mapper来做映射，还有一个可选的线程池Executor。</p>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20210926123124.png"></p>
<h3 id="Service类结构"><a href="#Service类结构" class="headerlink" title="Service类结构"></a>Service类结构</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// service的名称 </span></span><br><span class="line"><span class="keyword">private</span> <span class="type">String</span> <span class="variable">name</span> <span class="operator">=</span> <span class="literal">null</span>; </span><br><span class="line"><span class="comment">// Service所属的Server </span></span><br><span class="line"><span class="keyword">private</span> <span class="type">Server</span> <span class="variable">server</span> <span class="operator">=</span> <span class="literal">null</span>; </span><br><span class="line"><span class="comment">// 组件对属性改变的支持 </span></span><br><span class="line"><span class="keyword">protected</span> <span class="keyword">final</span> <span class="type">PropertyChangeSupport</span> <span class="variable">support</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">PropertyChangeSupport</span>(<span class="built_in">this</span>); </span><br><span class="line"><span class="comment">// 跟这个Service相关联的Connector集合 </span></span><br><span class="line"><span class="keyword">protected</span> Connector connectors[] = <span class="keyword">new</span> <span class="title class_">Connector</span>[<span class="number">0</span>]; </span><br><span class="line"><span class="comment">// Connector的锁 </span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> <span class="type">Object</span> <span class="variable">connectorsLock</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Object</span>(); </span><br><span class="line"><span class="comment">// 线程池 </span></span><br><span class="line"><span class="keyword">protected</span> <span class="keyword">final</span> ArrayList&lt;Executor&gt; executors = <span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;&gt;(); </span><br><span class="line"><span class="comment">// Servlet的引擎 </span></span><br><span class="line"><span class="keyword">private</span> <span class="type">Engine</span> <span class="variable">engine</span> <span class="operator">=</span> <span class="literal">null</span>; </span><br><span class="line"><span class="comment">// 类加载器 </span></span><br><span class="line"><span class="keyword">private</span> <span class="type">ClassLoader</span> <span class="variable">parentClassLoader</span> <span class="operator">=</span> <span class="literal">null</span>; </span><br><span class="line"><span class="comment">// Mapper. </span></span><br><span class="line"><span class="keyword">protected</span> <span class="keyword">final</span> <span class="type">Mapper</span> <span class="variable">mapper</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Mapper</span>(); </span><br><span class="line"><span class="comment">// Mapper 监听器 </span></span><br><span class="line"><span class="keyword">protected</span> <span class="keyword">final</span> <span class="type">MapperListener</span> <span class="variable">mapperListener</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">MapperListener</span>(<span class="built_in">this</span>);</span><br></pre></td></tr></table></figure>

<p>关键的几个点：</p>
<ul>
<li>Connector connectors[]：多个连接器，一个Servlet服务接受两个不同的协议连接，只不过不同的协议通过对应的Connector都被处理成了一个Request对象，这样对于Engine来说，都是一样的请求。</li>
<li>Engine engine：Servlet引擎，就是专门用来处理请求的，其他的都不管。</li>
<li>Mapper mapper：mapper保存了一个映射关系，不同请求路径对应哪一个Servlet的API。</li>
<li>PropertyChangeSupport support：JDK自带的观察者模式，主要是观察Java Bean对象的属性更改。</li>
</ul>
<p>在Service属性中，主要就是这四个东西，主要的架构关系，在上面的图中有解析，就不再多介绍。</p>
<h3 id="Service方法"><a href="#Service方法" class="headerlink" title="Service方法"></a>Service方法</h3><p>Service的方法比较简单，因为只是包装，自己没有太多的一个功能，所有主要功能有：</p>
<ol>
<li>监听Service属性变化，</li>
<li>启动</li>
<li>关闭。</li>
</ol>
<h4 id="生命周期方法"><a href="#生命周期方法" class="headerlink" title="生命周期方法"></a>生命周期方法</h4><p>init方法</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">initInternal</span><span class="params">()</span> <span class="keyword">throws</span> LifecycleException &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 1.父类执行init</span></span><br><span class="line">    <span class="built_in">super</span>.initInternal();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 2.执行servlet的引擎engine的init</span></span><br><span class="line">    <span class="keyword">if</span> (engine != <span class="literal">null</span>) &#123;</span><br><span class="line">        engine.init();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 3.执行Executors的init</span></span><br><span class="line">    <span class="comment">// Initialize any Executors</span></span><br><span class="line">    <span class="keyword">for</span> (Executor executor : findExecutors()) &#123;</span><br><span class="line">        <span class="keyword">if</span> (executor <span class="keyword">instanceof</span> JmxEnabled) &#123;</span><br><span class="line">            ((JmxEnabled) executor).setDomain(getDomain());</span><br><span class="line">        &#125;</span><br><span class="line">        executor.init();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 4.mapper监听器的init</span></span><br><span class="line">    <span class="comment">// Initialize mapper listener</span></span><br><span class="line">    mapperListener.init();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 5.connect的init</span></span><br><span class="line">    <span class="comment">// Initialize our defined Connectors</span></span><br><span class="line">    <span class="keyword">synchronized</span> (connectorsLock) &#123;</span><br><span class="line">        <span class="keyword">for</span> (Connector connector : connectors) &#123;</span><br><span class="line">            connector.init();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>start、stop、destroy方法与init方法都差不多的形式</p>
<h4 id="监听Service属性变化"><a href="#监听Service属性变化" class="headerlink" title="监听Service属性变化"></a>监听Service属性变化</h4><p>你怎么去监听一个java对象的属性被改变了？</p>
<p>使用监听器设计模式，是的！但是JDK已经提供好了这种使用方法，并在Tomcat里有了比较好的应用。</p>
<p>Tomcat中使用JDK中的 <code>PropertyChangeSupport</code> 类来实现监听的需求，使用该类需要按照如下要求：</p>
<ol>
<li><p>在Service中构造一个PropertyChangeSupport类，并将这个Java Bean传入。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//org.apache.catalina.core.StandardService </span></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 属性改变的监听管理 </span></span><br><span class="line"><span class="comment"> */</span> </span><br><span class="line"><span class="keyword">protected</span> <span class="keyword">final</span> <span class="type">PropertyChangeSupport</span> <span class="variable">support</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">PropertyChangeSupport</span>(<span class="built_in">this</span>);</span><br></pre></td></tr></table></figure>
</li>
<li><p>需要在Service中添加对应的添加监听器方法和删除监听器方法，对应如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Add a property change listener to this component.</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> listener The listener to add</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">addPropertyChangeListener</span><span class="params">(PropertyChangeListener listener)</span> &#123;</span><br><span class="line">    support.addPropertyChangeListener(listener);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Remove a property change listener from this component.</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> listener The listener to remove</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">removePropertyChangeListener</span><span class="params">(PropertyChangeListener listener)</span> &#123;</span><br><span class="line">    support.removePropertyChangeListener(listener);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>最后，如果需要实现自己的监听器，只需要实现 <code>void propertyChange(PropertyChangeEvent evt);</code> 方法即可。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">interface</span> <span class="title class_">PropertyChangeListener</span> <span class="keyword">extends</span> <span class="title class_">java</span>.util.EventListener &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * This method gets called when a bound property is changed.</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> evt A PropertyChangeEvent object describing the event source</span></span><br><span class="line"><span class="comment">     *          and the property that has changed.</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">void</span> <span class="title function_">propertyChange</span><span class="params">(PropertyChangeEvent evt)</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>这样就可以来完成对Service属性的监听了，非常好的方法。</p>
</li>
</ol>
<h2 id="Connector"><a href="#Connector" class="headerlink" title="Connector"></a>Connector</h2><p>通过CoyoteAdapter创建对应的Protocol，进行请求处理</p>
<p>关键的几个成员属性</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 关联的父容器service，通过它可以获得Engine进行请求的处理 </span></span><br><span class="line"><span class="keyword">protected</span> <span class="type">Service</span> <span class="variable">service</span> <span class="operator">=</span> <span class="literal">null</span>; </span><br><span class="line"><span class="comment">// Coyote Protocol处理器类名 </span></span><br><span class="line"><span class="keyword">protected</span> <span class="keyword">final</span> String protocolHandlerClassName; </span><br><span class="line"><span class="comment">// 被配置protocol的名字 </span></span><br><span class="line"><span class="keyword">protected</span> <span class="keyword">final</span> String configuredProtocol; </span><br><span class="line"><span class="comment">// coyote协议处理，默认Http11AprProtocol </span></span><br><span class="line"><span class="keyword">protected</span> <span class="keyword">final</span> ProtocolHandler protocolHandler; </span><br><span class="line"><span class="comment">// Coyote 适配器，不干具体的活转换接口. </span></span><br><span class="line"><span class="keyword">protected</span> <span class="type">Adapter</span> <span class="variable">adapter</span> <span class="operator">=</span> <span class="literal">null</span>;</span><br></pre></td></tr></table></figure>

<p>Adapter连接了Tomcat连接器Connector和容器Container.它的实现类是CoyoteAdapter主要负责的是对请求进行封装,构造Request和Response对象.并将请求转发给Container也就是Servlet容器.</p>
<p>关键方法在service中 </p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">service</span><span class="params">(org.apache.coyote.Request req, org.apache.coyote.Response res)</span></span><br><span class="line">    <span class="keyword">throws</span> Exception &#123;</span><br><span class="line"></span><br><span class="line">    <span class="type">Request</span> <span class="variable">request</span> <span class="operator">=</span> (Request) req.getNote(ADAPTER_NOTES);</span><br><span class="line">    <span class="type">Response</span> <span class="variable">response</span> <span class="operator">=</span> (Response) res.getNote(ADAPTER_NOTES);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (request == <span class="literal">null</span>) &#123;</span><br><span class="line">        <span class="comment">// Create objects</span></span><br><span class="line">        request = connector.createRequest();</span><br><span class="line">        request.setCoyoteRequest(req);</span><br><span class="line">        response = connector.createResponse();</span><br><span class="line">        response.setCoyoteResponse(res);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Link objects</span></span><br><span class="line">        request.setResponse(response);</span><br><span class="line">        response.setRequest(request);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Set as notes</span></span><br><span class="line">        req.setNote(ADAPTER_NOTES, request);</span><br><span class="line">        res.setNote(ADAPTER_NOTES, response);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Set query string encoding</span></span><br><span class="line">        req.getParameters().setQueryStringCharset(connector.getURICharset());</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (connector.getXpoweredBy()) &#123;</span><br><span class="line">        response.addHeader(<span class="string">&quot;X-Powered-By&quot;</span>, POWERED_BY);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="type">boolean</span> <span class="variable">async</span> <span class="operator">=</span> <span class="literal">false</span>;</span><br><span class="line">    <span class="type">boolean</span> <span class="variable">postParseSuccess</span> <span class="operator">=</span> <span class="literal">false</span>;</span><br><span class="line"></span><br><span class="line">    req.getRequestProcessor().setWorkerThreadName(THREAD_NAME.get());</span><br><span class="line"></span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="comment">// Parse and set Catalina and configuration specific</span></span><br><span class="line">        <span class="comment">// request parameters</span></span><br><span class="line">        postParseSuccess = postParseRequest(req, request, res, response);</span><br><span class="line">        <span class="keyword">if</span> (postParseSuccess) &#123;</span><br><span class="line">            <span class="comment">//check valves if we support async</span></span><br><span class="line">            request.setAsyncSupported(</span><br><span class="line">                connector.getService().getContainer().getPipeline().isAsyncSupported());</span><br><span class="line">            <span class="comment">// 调用容器pipeline进行请求处理，其实是触发到Engine，最终到Context</span></span><br><span class="line">            <span class="comment">// Calling the container</span></span><br><span class="line">            connector.getService().getContainer().getPipeline().getFirst().invoke(</span><br><span class="line">                request, response);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (request.isAsync()) &#123;</span><br><span class="line">            async = <span class="literal">true</span>;</span><br><span class="line">            <span class="type">ReadListener</span> <span class="variable">readListener</span> <span class="operator">=</span> req.getReadListener();</span><br><span class="line">            <span class="keyword">if</span> (readListener != <span class="literal">null</span> &amp;&amp; request.isFinished()) &#123;</span><br><span class="line">                <span class="comment">// Possible the all data may have been read during service()</span></span><br><span class="line">                <span class="comment">// method so this needs to be checked here</span></span><br><span class="line">                <span class="type">ClassLoader</span> <span class="variable">oldCL</span> <span class="operator">=</span> <span class="literal">null</span>;</span><br><span class="line">                <span class="keyword">try</span> &#123;</span><br><span class="line">                    oldCL = request.getContext().bind(<span class="literal">false</span>, <span class="literal">null</span>);</span><br><span class="line">                    <span class="keyword">if</span> (req.sendAllDataReadEvent()) &#123;</span><br><span class="line">                        req.getReadListener().onAllDataRead();</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">                    request.getContext().unbind(<span class="literal">false</span>, oldCL);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="type">Throwable</span> <span class="variable">throwable</span> <span class="operator">=</span></span><br><span class="line">                (Throwable) request.getAttribute(RequestDispatcher.ERROR_EXCEPTION);</span><br><span class="line"></span><br><span class="line">            <span class="comment">// If an async request was started, is not going to end once</span></span><br><span class="line">            <span class="comment">// this container thread finishes and an error occurred, trigger</span></span><br><span class="line">            <span class="comment">// the async error process</span></span><br><span class="line">            <span class="keyword">if</span> (!request.isAsyncCompleting() &amp;&amp; throwable != <span class="literal">null</span>) &#123;</span><br><span class="line">                request.getAsyncContextInternal().setErrorState(throwable, <span class="literal">true</span>);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            request.finishRequest();</span><br><span class="line">            response.finishResponse();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">        <span class="comment">// Ignore</span></span><br><span class="line">    &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">        <span class="type">AtomicBoolean</span> <span class="variable">error</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">AtomicBoolean</span>(<span class="literal">false</span>);</span><br><span class="line">        res.action(ActionCode.IS_ERROR, error);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (request.isAsyncCompleting() &amp;&amp; error.get()) &#123;</span><br><span class="line">            <span class="comment">// Connection will be forcibly closed which will prevent</span></span><br><span class="line">            <span class="comment">// completion happening at the usual point. Need to trigger</span></span><br><span class="line">            <span class="comment">// call to onComplete() here.</span></span><br><span class="line">            res.action(ActionCode.ASYNC_POST_PROCESS,  <span class="literal">null</span>);</span><br><span class="line">            async = <span class="literal">false</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Access log</span></span><br><span class="line">        <span class="keyword">if</span> (!async &amp;&amp; postParseSuccess) &#123;</span><br><span class="line">            <span class="comment">// Log only if processing was invoked.</span></span><br><span class="line">            <span class="comment">// If postParseRequest() failed, it has already logged it.</span></span><br><span class="line">            <span class="type">Context</span> <span class="variable">context</span> <span class="operator">=</span> request.getContext();</span><br><span class="line">            <span class="type">Host</span> <span class="variable">host</span> <span class="operator">=</span> request.getHost();</span><br><span class="line">            <span class="comment">// If the context is null, it is likely that the endpoint was</span></span><br><span class="line">            <span class="comment">// shutdown, this connection closed and the request recycled in</span></span><br><span class="line">            <span class="comment">// a different thread. That thread will have updated the access</span></span><br><span class="line">            <span class="comment">// log so it is OK not to update the access log here in that</span></span><br><span class="line">            <span class="comment">// case.</span></span><br><span class="line">            <span class="comment">// The other possibility is that an error occurred early in</span></span><br><span class="line">            <span class="comment">// processing and the request could not be mapped to a Context.</span></span><br><span class="line">            <span class="comment">// Log via the host or engine in that case.</span></span><br><span class="line">            <span class="type">long</span> <span class="variable">time</span> <span class="operator">=</span> System.currentTimeMillis() - req.getStartTime();</span><br><span class="line">            <span class="keyword">if</span> (context != <span class="literal">null</span>) &#123;</span><br><span class="line">                context.logAccess(request, response, time, <span class="literal">false</span>);</span><br><span class="line">            &#125; <span class="keyword">else</span> <span class="keyword">if</span> (response.isError()) &#123;</span><br><span class="line">                <span class="keyword">if</span> (host != <span class="literal">null</span>) &#123;</span><br><span class="line">                    host.logAccess(request, response, time, <span class="literal">false</span>);</span><br><span class="line">                &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                    connector.getService().getContainer().logAccess(</span><br><span class="line">                        request, response, time, <span class="literal">false</span>);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        req.getRequestProcessor().setWorkerThreadName(<span class="literal">null</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Recycle the wrapper request and response</span></span><br><span class="line">        <span class="keyword">if</span> (!async) &#123;</span><br><span class="line">            updateWrapperErrorCount(request, response);</span><br><span class="line">            request.recycle();</span><br><span class="line">            response.recycle();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h2 id="Container"><a href="#Container" class="headerlink" title="Container"></a>Container</h2><h3 id="Pipeline和Value"><a href="#Pipeline和Value" class="headerlink" title="Pipeline和Value"></a>Pipeline和Value</h3><p>Tomcat的容器中，内容的执行是通过一个管道来控制的，它定义了一个Pipeline，4个层次的容器都持有了一个Pipeline用以执行预定义好的任务。而具体的任务则装载在Value中，也就是所谓的阀。</p>
<p>在连接适配器中的服务方法通过了调用</p>
<p>connector.getService().getContainer().getPipeline().getFirst().invoke(request, response)的调用将请求转移到容器中执行。它实际就是调用了容器的Pipeline中的任务，即依次执行预定义好的任务(Value)。Pipeline的实现是一个链表结构，我们先看看它其中的节点实现。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="keyword">class</span> <span class="title class_">ValveBase</span> <span class="keyword">extends</span> <span class="title class_">LifecycleMBeanBase</span> <span class="keyword">implements</span> <span class="title class_">Contained</span>, Valve &#123;</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * The next Valve in the pipeline this Valve is a component of.</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">protected</span> <span class="type">Valve</span> <span class="variable">next</span> <span class="operator">=</span> <span class="literal">null</span>;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> Container <span class="title function_">getContainer</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> container;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">setContainer</span><span class="params">(Container container)</span> &#123;</span><br><span class="line">        <span class="built_in">this</span>.container = container;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> Valve <span class="title function_">getNext</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> next;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">setNext</span><span class="params">(Valve valve)</span> &#123;</span><br><span class="line">        <span class="built_in">this</span>.next = valve;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>上述精简过的代码是实现了Value接口的基础类ValueBase，它有一个Value类型的内部属性next，即同一个Pipeline中的后续Value的引用。如果玩过Java数据结构或作STL的基本上不难理解这是一个单向链表的节点。继承这个ValueBase针对不同的容器实现了不同版本的阀如StandardEngineValue，StandardHostValue，StandardContextValue，StandardWrapperValve等。他们之间不同的实现就是invoke和event的方法不同。而实际上也就是请求的路由选择，Filter应用和Servlet的处理。</p>
<p>Pipeline的标准实现是StandardPipeline。它的类注释是</p>
<p><code>Standard implementation of a processing Pipeline that will invoke a series of Valves that have been configured to be called in order.This implementation can be used for any type of Container.</code></p>
<p>意思是它用于依次执行已经配置好的阀。</p>
<h3 id="Engine"><a href="#Engine" class="headerlink" title="Engine"></a>Engine</h3><p>Tomcat的容器中，有着4个层次的Servlet容器，这4个类型的容器一般情况下是从属的关系.其中顶层的容器为Engine。这个Engine是Tomcat的整个Servlet引擎.它的子容器是Host，默认情况下Tomcat会有一个Engine，它所对应的阀是StandardEngineValve。</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">Engine</span> <span class="attr">name</span>=<span class="string">&quot;Catalina&quot;</span> <span class="attr">defaultHost</span>=<span class="string">&quot;localhost&quot;</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">Realm</span> <span class="attr">className</span>=<span class="string">&quot;org.apache.catalina.realm.LockOutRealm&quot;</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">Realm</span> <span class="attr">className</span>=<span class="string">&quot;org.apache.catalina.realm.UserDatabaseRealm&quot;</span></span></span><br><span class="line"><span class="tag">           <span class="attr">resourceName</span>=<span class="string">&quot;UserDatabase&quot;</span>/&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">Realm</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">Host</span> <span class="attr">name</span>=<span class="string">&quot;localhost&quot;</span>  <span class="attr">appBase</span>=<span class="string">&quot;webapps&quot;</span></span></span><br><span class="line"><span class="tag">        <span class="attr">unpackWARs</span>=<span class="string">&quot;true&quot;</span> <span class="attr">autoDeploy</span>=<span class="string">&quot;true&quot;</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">Valve</span> <span class="attr">className</span>=<span class="string">&quot;org.apache.catalina.valves.AccessLogValve&quot;</span> <span class="attr">directory</span>=<span class="string">&quot;logs&quot;</span></span></span><br><span class="line"><span class="tag">           <span class="attr">prefix</span>=<span class="string">&quot;localhost_access_log&quot;</span> <span class="attr">suffix</span>=<span class="string">&quot;.txt&quot;</span></span></span><br><span class="line"><span class="tag">           <span class="attr">pattern</span>=<span class="string">&quot;%h %l %u %t <span class="symbol">&amp;quot;</span>%r<span class="symbol">&amp;quot;</span> %s %b&quot;</span> /&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">Host</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">Engine</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>从配置中可以知道Service节点除了定义Connector，还定义了Engine，它里面还有一个Host。也就是说StandardService初始化的时候，就构造了Servlet引擎StandardEngine等一系列的容器层次。我们就看看Engine的构造方法。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="title function_">StandardEngine</span><span class="params">()</span> &#123;</span><br><span class="line">    <span class="built_in">super</span>();</span><br><span class="line">    pipeline.setBasic(<span class="keyword">new</span> <span class="title class_">StandardEngineValve</span>());</span><br><span class="line">    <span class="comment">/* Set the jmvRoute using the system property jvmRoute */</span></span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        setJvmRoute(System.getProperty(<span class="string">&quot;jvmRoute&quot;</span>));</span><br><span class="line">    &#125; <span class="keyword">catch</span>(Exception ex) &#123;</span><br><span class="line">        log.warn(sm.getString(<span class="string">&quot;standardEngine.jvmRouteFail&quot;</span>));</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// By default, the engine will hold the reloading thread</span></span><br><span class="line">    backgroundProcessorDelay = <span class="number">10</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>StandardEngine继承自ContainerBase自然也就有了pipeline对象和invoke方法。构造函数中设置的基础阀是StandardEngineValve。它是Engine特有的。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">final</span> <span class="keyword">void</span> <span class="title function_">invoke</span><span class="params">(Request request, Response response)</span></span><br><span class="line">    <span class="keyword">throws</span> IOException, ServletException &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Select the Host to be used for this Request</span></span><br><span class="line">    <span class="type">Host</span> <span class="variable">host</span> <span class="operator">=</span> request.getHost();</span><br><span class="line">    <span class="keyword">if</span> (host == <span class="literal">null</span>) &#123;</span><br><span class="line">        <span class="comment">// HTTP 0.9 or HTTP 1.0 request without a host when no default host</span></span><br><span class="line">        <span class="comment">// is defined.</span></span><br><span class="line">        <span class="comment">// Don&#x27;t overwrite an existing error</span></span><br><span class="line">        <span class="keyword">if</span> (!response.isError()) &#123;</span><br><span class="line">            response.sendError(<span class="number">404</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (request.isAsyncSupported()) &#123;</span><br><span class="line">        request.setAsyncSupported(host.getPipeline().isAsyncSupported());</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Ask this Host to process this request</span></span><br><span class="line">    host.getPipeline().getFirst().invoke(request, response);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>上述代码是StandardEngineValve的invoke方法,从中我们可以知道它的作用只是把调用转发给了它的子容器Host,也就是他只起到了一个路由的作用。真正处理请求的还是在它的下级容器。</p>
<h3 id="Host"><a href="#Host" class="headerlink" title="Host"></a>Host</h3><p>Engine是Tomcat的整个Servlet引擎，它的子容器是Host，一般称之为虚拟主机。Engine和Host是一对多的关系，Host既然同为Servlet容器那么它也承担着对请求的处理，主要体现在它的StandardHostValue上面。除此它还有一个重要的功能也就是虚拟主机下的应用的发布，这项功能主要体现在HostConfig上面。</p>
<h3 id="Context"><a href="#Context" class="headerlink" title="Context"></a>Context</h3><p>Context是Host的子容器，在servlet引擎中它代表了一个web application。它在每一个Catalina中部署的应用几乎都是存在的。它的子容器是Wrapper(一个具体servlet的定义)，Context是标准实现是StandardContext，与StandardHost的实现模式类似。它承担了创建Wrapper容器(Servlet)，Filter，ErrorPage等在web。xml中配置的内容。</p>
<h3 id="Warpper"><a href="#Warpper" class="headerlink" title="Warpper"></a>Warpper</h3><p>Wrapper是Context的子容器,它代表了在应用部署描述中的一个单独的servlet。它通过Servlet的init和destroy方法掌管了底层的Servlet的生命周期。并且其中的阀还负责调用Servlet响应用户请求的功能。Wrapper的默认实现是StandardWrapper。</p>
<p>StandardWrapper的构造函数还是跟StandardContext等类似，也是设置了阀，与它的上层容器不同的是，StandardWrapper已经没有下层容器了，所以在它的addChild方法实现上是直接抛出一个IllegalStateException异常。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2020/08/15/12-Tomcat/02-Tomcat%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%902/" data-id="clmcxec5q0005u8wae2yj0u52" data-title="Tomcat源码分析2" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Tomcat/" rel="tag">Tomcat</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-12-Tomcat/03-Tomcat优化" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2020/08/15/12-Tomcat/03-Tomcat%E4%BC%98%E5%8C%96/" class="article-date">
  <time class="dt-published" datetime="2020-08-15T08:50:45.000Z" itemprop="datePublished">2020-08-15</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Tomcat/">Tomcat</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2020/08/15/12-Tomcat/03-Tomcat%E4%BC%98%E5%8C%96/">Tomcat优化</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="性能优化"><a href="#性能优化" class="headerlink" title="性能优化"></a>性能优化</h1><h2 id="性能优化的三个指标"><a href="#性能优化的三个指标" class="headerlink" title="性能优化的三个指标"></a>性能优化的三个指标</h2><ul>
<li>降低响应时间</li>
<li>提高系统吞吐量(QPS)</li>
<li>提高服务的可用性</li>
</ul>
<h2 id="性能优化的原则"><a href="#性能优化的原则" class="headerlink" title="性能优化的原则"></a>性能优化的原则</h2><ul>
<li>具体情况具体分析</li>
<li>积少成多</li>
</ul>
<h1 id="Tomcat性能优化中使用的工具"><a href="#Tomcat性能优化中使用的工具" class="headerlink" title="Tomcat性能优化中使用的工具"></a>Tomcat性能优化中使用的工具</h1><h2 id="命令工具"><a href="#命令工具" class="headerlink" title="命令工具"></a>命令工具</h2><p>使用命令查看相关指标，在linux系统常用</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">01 查看tomcat进程pid</span> </span><br><span class="line">ps -ef | grep tomcat </span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">02 查看进程的信息</span> </span><br><span class="line">cat /pro/pid/status </span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">03 查看进程的cpu和内存</span> </span><br><span class="line">top -p pid </span><br><span class="line"></span><br><span class="line">jstack 进程| grep 8908 </span><br><span class="line">jps/jmap</span><br></pre></td></tr></table></figure>

<h2 id="界面工具"><a href="#界面工具" class="headerlink" title="界面工具"></a>界面工具</h2><p>JConsole：一个内置 Java性能分析器</p>
<p>JMeter：一个Apache组织开发的基于Java的压力测试工具</p>
<p>下载地址：<a target="_blank" rel="noopener" href="http://jmeter.apache.org/download_jmeter.cgi">http://jmeter.apache.org/download_jmeter.cgi</a></p>
<p>其他工具：JProfile、jvisualvm、arthas、psi-probe等</p>
<h2 id="性能优化测试原则"><a href="#性能优化测试原则" class="headerlink" title="性能优化测试原则"></a>性能优化测试原则</h2><p>指标：正确率、CPU占有率、QPS、JVM</p>
<p>程序分类：I&#x2F;0密集型(网络，文件读写),CPU密集型(加密&#x2F;解密算法)</p>
<h1 id="优化思路"><a href="#优化思路" class="headerlink" title="优化思路"></a>优化思路</h1><h2 id="server-xml核心组件"><a href="#server-xml核心组件" class="headerlink" title="server.xml核心组件"></a>server.xml核心组件</h2><p>通过官网的描述信息来观察</p>
<ul>
<li><strong>Server</strong><br>Server interface which is rarely customized by users.</li>
<li><strong>Service</strong><br>The Service element is rarely customized by users.</li>
<li>Connector<br><strong>Creating a customized connector is a significant effort.</strong></li>
<li><strong>Engine</strong><br>The Engine interface may be implemented to supply custom Engines, though this is uncommon.</li>
<li><strong>Host</strong><br>Users rarely create custom Hosts because the StandardHost implementation provides significant additional functionality.</li>
<li><strong>Context</strong><br>The Context interface may be implemented to create custom Contexts, but this is rarely the case because the StandardContext provides significant additional functionality.</li>
</ul>
<p>Context代表的是web应用，是和我们比较接近的，这块我们考虑对其适当的优化</p>
<p><strong>结论</strong>：Connector and Context两个层面可以进行着手</p>
<p>一般根据自己场景调整对应参数，常调整的参数如下：</p>
<p><strong>acceptCount</strong>：超过maxConnections配置之后的连接存储队列长度大小，默认100<br><strong>maxConnections</strong>：最大连接数，默认8192<br><strong>maxThreads</strong>：处理Connection最大线程数，默认200<br><strong>minSpareThreads</strong>：处理Connection最小线程数，默认10<br><strong>connectionTimeout</strong>：接受连接后等待的超时时间，默认60秒</p>
<p>把tomcat比作餐厅，maxThreads为服务人员的数量，maxConnections为餐厅内的桌子，acceptCount为排队号长度</p>
<h2 id="server-xml非核心组件"><a href="#server-xml非核心组件" class="headerlink" title="server.xml非核心组件"></a>server.xml非核心组件</h2><p>官网相关介绍：<a target="_blank" rel="noopener" href="https://tomcat.apache.org/tomcat-9.0-doc/config/index.html">https://tomcat.apache.org/tomcat-9.0-doc/config/index.html</a></p>
<ul>
<li><p>Listener</p>
<p>Listener(即监听器)定义的组件，可以在特定事件发生时执行特定的操作；被监听的事件通常是Tomcat的启动和停止。</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">Listener</span> <span class="attr">className</span>=<span class="string">&quot;org.apache.catalina.core.AprLifecycleListener&quot;</span> <span class="attr">SSLEngine</span>=<span class="string">&quot;on&quot;</span> /&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- -监听内存溢出--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">Listener</span> <span class="attr">className</span>=<span class="string">&quot;org.apache.catalina.core.JreMemoryLeakPreventionListener&quot;</span> /&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">Listener</span> <span class="attr">className</span>=<span class="string">&quot;org.apache.catalina.mbeans.GlobalResourcesLifecycleListener&quot;</span> /&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">Listener</span> <span class="attr">className</span>=<span class="string">&quot;org.apache.catalina.core.ThreadLocalLeakPreventionListener&quot;</span> /&gt;</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>Global Resources</p>
<p>GlobalNamingResources元素定义了全局资源，通过配置可以看出，该配置是通过读取$TOMCAT_HOME&#x2F; conf&#x2F;tomcat-users.xml实现的。</p>
<p><code>The GlobalNamingResources element defines the global JNDI resources for the [Server]</code></p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">GlobalNamingResources</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">Resource</span> <span class="attr">name</span>=<span class="string">&quot;UserDatabase&quot;</span> <span class="attr">auth</span>=<span class="string">&quot;Container&quot;</span></span></span><br><span class="line"><span class="tag">            <span class="attr">type</span>=<span class="string">&quot;org.apache.catalina.UserDatabase&quot;</span></span></span><br><span class="line"><span class="tag">            <span class="attr">description</span>=<span class="string">&quot;User database that can be updated and saved&quot;</span></span></span><br><span class="line"><span class="tag">            <span class="attr">factory</span>=<span class="string">&quot;org.apache.catalina.users.MemoryUserDatabaseFactory&quot;</span></span></span><br><span class="line"><span class="tag">            <span class="attr">pathname</span>=<span class="string">&quot;conf/tomcat-users.xml&quot;</span> /&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">GlobalNamingResources</span>&gt;</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>Valve</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">Valve</span> <span class="attr">className</span>=<span class="string">&quot;org.apache.catalina.valves.AccessLogValve&quot;</span> <span class="attr">directory</span>=<span class="string">&quot;logs&quot;</span></span></span><br><span class="line"><span class="tag">       <span class="attr">prefix</span>=<span class="string">&quot;localhost_access_log&quot;</span> <span class="attr">suffix</span>=<span class="string">&quot;.txt&quot;</span></span></span><br><span class="line"><span class="tag">       <span class="attr">pattern</span>=<span class="string">&quot;%h %l %u %t <span class="symbol">&amp;quot;</span>%r<span class="symbol">&amp;quot;</span> %s %b&quot;</span> /&gt;</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>Realm</p>
<p>Realm，可以把它理解成“域”；Realm提供了一种用户密码与web应用的映射关系，从而达到角色安全管理的作用。在本例中，Realm的配置使用name为UserDatabase的资源实现。而该资源在Server元素中使用GlobalNamingResources配置</p>
<p><code>A Realm element represents a &quot;database&quot; of usernames, passwords, and roles (similar to Unix groups) assigned to those users.</code></p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">Realm</span> <span class="attr">className</span>=<span class="string">&quot;org.apache.catalina.realm.LockOutRealm&quot;</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- This Realm uses the UserDatabase configured in the global JNDI</span></span><br><span class="line"><span class="comment">             resources under the key &quot;UserDatabase&quot;.  Any edits</span></span><br><span class="line"><span class="comment">             that are performed against this UserDatabase are immediately</span></span><br><span class="line"><span class="comment">             available for use by the Realm.  --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">Realm</span> <span class="attr">className</span>=<span class="string">&quot;org.apache.catalina.realm.UserDatabaseRealm&quot;</span></span></span><br><span class="line"><span class="tag">           <span class="attr">resourceName</span>=<span class="string">&quot;UserDatabase&quot;</span>/&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">Realm</span>&gt;</span></span><br></pre></td></tr></table></figure></li>
</ul>
<h2 id="web-xml"><a href="#web-xml" class="headerlink" title="web.xml"></a>web.xml</h2><p>全局的web.xml文件有些标签用不到的，可以删除掉。</p>
<h2 id="JVM层面"><a href="#JVM层面" class="headerlink" title="JVM层面"></a>JVM层面</h2><p>Tomcat运行起来本身就是一个Java进程，所以这块可以参照JVM部分的优化思路。</p>
<h1 id="Tomcat中server-xml优化"><a href="#Tomcat中server-xml优化" class="headerlink" title="Tomcat中server.xml优化"></a>Tomcat中server.xml优化</h1><h2 id="Connector-连接器的-I-O"><a href="#Connector-连接器的-I-O" class="headerlink" title="Connector 连接器的 I&#x2F;O"></a>Connector 连接器的 I&#x2F;O</h2><p>连接器模式改为NIO模式</p>
<p>NIO模式最大化压榨了CPU，把时间片更好利用起来</p>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20210927132247.png"></p>
<p>NIO适合大量长连接</p>
<h2 id="关闭自动重载"><a href="#关闭自动重载" class="headerlink" title="关闭自动重载"></a>关闭自动重载</h2><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">Context</span> <span class="attr">docBase</span>=<span class="string">&quot;&quot;</span> <span class="attr">reloadable</span>=<span class="string">&quot;false&quot;</span>/&gt;</span></span><br></pre></td></tr></table></figure>

<p>关闭自动重载，默认是true(不同版本中有差异)</p>
<p>自动加载增加运行开销并且很容易内存溢出</p>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20210927132603.png"></p>
<h2 id="配置线程池"><a href="#配置线程池" class="headerlink" title="配置线程池"></a>配置线程池</h2><p>Executor标签中属性</p>
<p>【namePrefix】线程命名前缀</p>
<p>【maxThreads】最大允许线程数</p>
<p>【minSpareThreads】最少空闲线程，相当于初始化的线程，线程池中的线程 Connector标签中的属性</p>
<p>【executor】对上面Executor标签标签的引用</p>
<h1 id="Tomcat中-web-xml优化"><a href="#Tomcat中-web-xml优化" class="headerlink" title="Tomcat中 web.xml优化"></a>Tomcat中 web.xml优化</h1><p>应用程序运行时最终会加载 conf&#x2F;web.xml 和应用的 web.xml 的合集</p>
<h2 id="servlet-优化"><a href="#servlet-优化" class="headerlink" title="servlet 优化"></a>servlet 优化</h2><p>当前应用是REST应用(微服务)</p>
<ol>
<li>去掉不必要的资源：JspServlet</li>
<li>seesion也可以移除</li>
</ol>
<h2 id="valve-优化"><a href="#valve-优化" class="headerlink" title="valve 优化"></a>valve 优化</h2><p>移除掉AccessLogValve</p>
<p>valve实现都需要消耗java应用的计算时间，一般我们可以使用nginx来记录日志</p>
<h2 id="JSP-预编译优化"><a href="#JSP-预编译优化" class="headerlink" title="JSP 预编译优化"></a>JSP 预编译优化</h2><p>JSP -&gt; JAVA -&gt; CLASS</p>
<p>可使用ant先编译jsp</p>
<p>Jspservlet开发模式(development)设置为false</p>
<h1 id="SpringBoot中Tomcat优化"><a href="#SpringBoot中Tomcat优化" class="headerlink" title="SpringBoot中Tomcat优化"></a>SpringBoot中Tomcat优化</h1><h2 id="Maven-中-Springboot-引入"><a href="#Maven-中-Springboot-引入" class="headerlink" title="Maven 中 Springboot 引入"></a>Maven 中 Springboot 引入</h2><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span> </span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.springframework.boot<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span> </span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spring-boot-starter-tomcat<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span> </span><br><span class="line">    <span class="tag">&lt;<span class="name">scope</span>&gt;</span>provided<span class="tag">&lt;/<span class="name">scope</span>&gt;</span> </span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h2 id="设置线程池"><a href="#设置线程池" class="headerlink" title="设置线程池"></a>设置线程池</h2><p>server.tomcat.max-threads&#x3D;1000</p>
<h2 id="关闭Accesslog日志"><a href="#关闭Accesslog日志" class="headerlink" title="关闭Accesslog日志"></a>关闭Accesslog日志</h2><p>server.tomcat.accesslog&#x3D;false</p>
<h1 id="常见问题排查"><a href="#常见问题排查" class="headerlink" title="常见问题排查"></a>常见问题排查</h1><h2 id="CPU使用率过高"><a href="#CPU使用率过高" class="headerlink" title="CPU使用率过高"></a>CPU使用率过高</h2><p><strong>可能的原因</strong></p>
<p>GC频繁或者创建了很多业务线程</p>
<p><strong>排查</strong></p>
<p>哪些线程比较消耗CPU，或者多线程上下文频繁切换</p>
<p><strong>解决思路</strong></p>
<p>top -H -p pid 查看某个Java进程各个线程使用CPU的情况，找到哪个线程占用CPU比较高 jstack pid 打印出线程信息，定位到上述的线程名称</p>
<h2 id="拒绝连接"><a href="#拒绝连接" class="headerlink" title="拒绝连接"></a>拒绝连接</h2><ul>
<li>java.net.BindException: Address already in use: JVM_Bind<br>端口被占用，可以使用netstat -an 查看端口占用情况，关闭对应的进程或者tomcat换端口</li>
<li>java.net.ConnectException: Connection refused: connect<br>ping一下服务端的IP，可能服务端机器有问题</li>
<li>java.net.SocketException: Too many open files<br>可能在高并发的情况下，创建的Socket过多，文件句柄不够用了，可以关闭无用的句柄，如果都有用，可以增加文件句柄数：ulimit -n 10000，另外还可以参考Netty中的服务器优化参数。</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2020/08/15/12-Tomcat/03-Tomcat%E4%BC%98%E5%8C%96/" data-id="clmcxec5s0008u8wadz1e989o" data-title="Tomcat优化" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Tomcat/" rel="tag">Tomcat</a></li></ul>

    </footer>
  </div>
  
</article>



  


  <nav id="page-nav">
    
    <a class="extend prev" rel="prev" href="/page/14/">&laquo; Prev</a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/13/">13</a><a class="page-number" href="/page/14/">14</a><span class="page-number current">15</span><a class="page-number" href="/page/16/">16</a><a class="page-number" href="/page/17/">17</a><span class="space">&hellip;</span><a class="page-number" href="/page/19/">19</a><a class="extend next" rel="next" href="/page/16/">Next &raquo;</a>
  </nav>

</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/ActiveMQ/">ActiveMQ</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Docker/">Docker</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Dubbo/">Dubbo</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Golang/">Golang</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Java%E5%9F%BA%E7%A1%80/">Java基础</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/K8s/">K8s</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Kafka/">Kafka</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/MQ/">MQ</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/MySQL/">MySQL</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Mybatis/">Mybatis</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Netty/">Netty</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/RPC/">RPC</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/RabbitMQ/">RabbitMQ</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Redis/">Redis</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Socker/">Socker</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Spring/">Spring</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/SpringBoot/">SpringBoot</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/SpringCloud/">SpringCloud</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Tomcat/">Tomcat</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/ZooKeeper/">ZooKeeper</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Zookeeper/">Zookeeper</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/">大数据</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/">数据结构</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E9%9D%A2%E8%AF%95%E9%A2%98/">面试题</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/ActiveMQ/" rel="tag">ActiveMQ</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/CDH/" rel="tag">CDH</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Docker/" rel="tag">Docker</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Dubbo/" rel="tag">Dubbo</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Golang/" rel="tag">Golang</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/HDFS/" rel="tag">HDFS</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/K8s/" rel="tag">K8s</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/KDTree/" rel="tag">KDTree</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Kafka/" rel="tag">Kafka</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/MQ/" rel="tag">MQ</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/MapReduce/" rel="tag">MapReduce</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/MySQL/" rel="tag">MySQL</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Mybatis/" rel="tag">Mybatis</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Netty/" rel="tag">Netty</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/RPC/" rel="tag">RPC</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/RabbitMQ/" rel="tag">RabbitMQ</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Redis/" rel="tag">Redis</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Socker/" rel="tag">Socker</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Spark/" rel="tag">Spark</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Spring/" rel="tag">Spring</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/SpringBoot/" rel="tag">SpringBoot</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/SpringCloud/" rel="tag">SpringCloud</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Tomcat/" rel="tag">Tomcat</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/YARN/" rel="tag">YARN</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ZooKeeper/" rel="tag">ZooKeeper</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%A4%9A%E7%BA%BF%E7%A8%8B/" rel="tag">多线程</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%B3%A8%E8%A7%A3/" rel="tag">注解</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/" rel="tag">设计模式</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E9%9B%86%E5%90%88%E6%BA%90%E7%A0%81/" rel="tag">集合源码</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E9%9D%A2%E8%AF%95%E9%A2%98/" rel="tag">面试题</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/ActiveMQ/" style="font-size: 10px;">ActiveMQ</a> <a href="/tags/CDH/" style="font-size: 10px;">CDH</a> <a href="/tags/Docker/" style="font-size: 18px;">Docker</a> <a href="/tags/Dubbo/" style="font-size: 13px;">Dubbo</a> <a href="/tags/Golang/" style="font-size: 19px;">Golang</a> <a href="/tags/HDFS/" style="font-size: 10px;">HDFS</a> <a href="/tags/K8s/" style="font-size: 10px;">K8s</a> <a href="/tags/KDTree/" style="font-size: 10px;">KDTree</a> <a href="/tags/Kafka/" style="font-size: 10px;">Kafka</a> <a href="/tags/MQ/" style="font-size: 10px;">MQ</a> <a href="/tags/MapReduce/" style="font-size: 10px;">MapReduce</a> <a href="/tags/MySQL/" style="font-size: 10px;">MySQL</a> <a href="/tags/Mybatis/" style="font-size: 14px;">Mybatis</a> <a href="/tags/Netty/" style="font-size: 14px;">Netty</a> <a href="/tags/RPC/" style="font-size: 11px;">RPC</a> <a href="/tags/RabbitMQ/" style="font-size: 15px;">RabbitMQ</a> <a href="/tags/Redis/" style="font-size: 17px;">Redis</a> <a href="/tags/Socker/" style="font-size: 10px;">Socker</a> <a href="/tags/Spark/" style="font-size: 11px;">Spark</a> <a href="/tags/Spring/" style="font-size: 20px;">Spring</a> <a href="/tags/SpringBoot/" style="font-size: 14px;">SpringBoot</a> <a href="/tags/SpringCloud/" style="font-size: 10px;">SpringCloud</a> <a href="/tags/Tomcat/" style="font-size: 13px;">Tomcat</a> <a href="/tags/YARN/" style="font-size: 11px;">YARN</a> <a href="/tags/ZooKeeper/" style="font-size: 12px;">ZooKeeper</a> <a href="/tags/%E5%A4%9A%E7%BA%BF%E7%A8%8B/" style="font-size: 16px;">多线程</a> <a href="/tags/%E6%B3%A8%E8%A7%A3/" style="font-size: 10px;">注解</a> <a href="/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/" style="font-size: 11px;">设计模式</a> <a href="/tags/%E9%9B%86%E5%90%88%E6%BA%90%E7%A0%81/" style="font-size: 13px;">集合源码</a> <a href="/tags/%E9%9D%A2%E8%AF%95%E9%A2%98/" style="font-size: 10px;">面试题</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/01/">January 2022</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/12/">December 2021</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/05/">May 2021</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/04/">April 2021</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/03/">March 2021</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/02/">February 2021</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/01/">January 2021</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/12/">December 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/11/">November 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/10/">October 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/08/">August 2020</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2023/09/10/11-MySQL/04-MySQL%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%8A%E8%AF%BB%E5%86%99%E5%88%86%E7%A6%BB/05-%E9%AB%98%E5%8F%AF%E7%94%A8%E6%95%B0%E6%8D%AE%E5%BA%93%E9%9B%86%E7%BE%A4%E5%AE%89%E8%A3%85%E6%90%AD%E5%BB%BA%E6%89%8B%E5%86%8C/">(no title)</a>
          </li>
        
          <li>
            <a href="/2023/09/10/11-MySQL/04-MySQL%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%8A%E8%AF%BB%E5%86%99%E5%88%86%E7%A6%BB/04-Sharding-JDBC%E8%AF%BB%E5%86%99%E5%88%86%E7%A6%BB/">(no title)</a>
          </li>
        
          <li>
            <a href="/2023/09/10/11-MySQL/04-MySQL%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%8A%E8%AF%BB%E5%86%99%E5%88%86%E7%A6%BB/03-MyCat%E8%AF%BB%E5%86%99%E5%88%86%E7%A6%BB/">(no title)</a>
          </li>
        
          <li>
            <a href="/2023/09/10/11-MySQL/04-MySQL%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%8A%E8%AF%BB%E5%86%99%E5%88%86%E7%A6%BB/02-MySQL%E4%B8%BB%E4%BB%8E%E5%A4%8D%E5%88%B6/">(no title)</a>
          </li>
        
          <li>
            <a href="/2023/09/10/11-MySQL/04-MySQL%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%8A%E8%AF%BB%E5%86%99%E5%88%86%E7%A6%BB/01-MySQL%E4%B8%BB%E4%BB%8E%E5%A4%8D%E5%88%B6%E5%92%8C%E9%AB%98%E5%8F%AF%E7%94%A8/">(no title)</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2023 John Doe<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.6.4.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
</body>
</html>