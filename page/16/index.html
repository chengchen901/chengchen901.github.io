<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta property="og:type" content="website">
<meta property="og:title" content="Hexo">
<meta property="og:url" content="http://example.com/page/16/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/typeface-source-code-pro@0.0.71/index.min.css">

  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
  
<meta name="generator" content="Hexo 6.3.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hexo</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"><span class="fa fa-bars"></span></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
        
          <a class="nav-icon" href="/atom.xml" title="RSS Feed"><span class="fa fa-rss"></span></a>
        
        <a class="nav-icon nav-search-btn" title="Search"><span class="fa fa-search"></span></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://example.com"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main">
  
    <article id="post-12-Tomcat/04-手写Tomcat" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2020/08/15/12-Tomcat/04-%E6%89%8B%E5%86%99Tomcat/" class="article-date">
  <time class="dt-published" datetime="2020-08-15T08:50:45.000Z" itemprop="datePublished">2020-08-15</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Tomcat/">Tomcat</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2020/08/15/12-Tomcat/04-%E6%89%8B%E5%86%99Tomcat/">手写Tomcat</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="什么是Servlet"><a href="#什么是Servlet" class="headerlink" title="什么是Servlet?"></a>什么是Servlet?</h1><p>servlet是一个基于Java™技术的Web组件，由容器管理生成动态内容。与其他基于Java技术的组件一样，servlet</p>
<p>编译成平台无关字节的独立于平台的Java类码可以动态加载到支持Java技术的Web中并由其运行的代码服务器。容器有时被称为servlet引擎，是Web服务器的扩展提供servlet功能。servlet通过由servlet容器实现的请求&#x2F;响应范例。</p>
<h1 id="什么是Servlet容器"><a href="#什么是Servlet容器" class="headerlink" title="什么是Servlet容器?"></a>什么是Servlet容器?</h1><p>servlet容器是Web服务器或应用服务器的一部分，它提供发送请求和响应的网络服务基于mime进行解码</p>
<p>请求，并格式化基于mime的响应。servlet容器还包含和在整个生命周期中管理servlet。</p>
<p>servlet容器可以构建到主机Web服务器中，也可以作为附加组件安装组件通过该服务器的本地扩展API发送到Web服务器。Servlet容器也可以内置到或可能安装到支持web的应用服务器中。</p>
<p>所有servlet容器都必须支持HTTP作为请求和响应的协议，但附加的基于请求&#x2F;响应的协议，如HTTPS (HTTP over SSL)可能支持。容器所需的HTTP规范版本必须实现HTTP&#x2F;1.1和HTTP&#x2F;2。当支持HTTP&#x2F;2时，servlet容器必须支持“h2”和“h2c”协议标识符(如HTTP&#x2F;2 RFC的3.1节)。这意味着所有servlet容器都必须支持ALPN。因为容器可能具有RFC 7234中描述的缓存机制(HTTP&#x2F;1.1缓存)，它可以修改来自客户端的请求在交付它们之前，可以修改servlet生成的响应，然后再将它们发送给客户端，或者响应请求而不将请求传递给下面的servlet符合RFC 7234。</p>
<p>servlet容器可以对环境施加安全限制servlet执行。在Java平台、标准版(J2SE, v.1.3或以上版本)或Java中平台，企业版(Java EE, v.1.3或以上)环境，这些限制应该使用Java平台定义的权限体系结构放置。为例如，一些应用程序服务器可能会限制Thread对象的创建以确保容器的其他组件不会受到负面影响。</p>
<p>Java SE 8是servlet使用的底层Java平台的最小版本必须建造集装箱。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2020/08/15/12-Tomcat/04-%E6%89%8B%E5%86%99Tomcat/" data-id="clmcxec5u000bu8wa6v1q55ds" data-title="手写Tomcat" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Tomcat/" rel="tag">Tomcat</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-13-大数据/01-基础及安装/01-CDH6.2安装" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2020/08/15/13-%E5%A4%A7%E6%95%B0%E6%8D%AE/01-%E5%9F%BA%E7%A1%80%E5%8F%8A%E5%AE%89%E8%A3%85/01-CDH6.2%E5%AE%89%E8%A3%85/" class="article-date">
  <time class="dt-published" datetime="2020-08-15T08:50:45.000Z" itemprop="datePublished">2020-08-15</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/">大数据</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2020/08/15/13-%E5%A4%A7%E6%95%B0%E6%8D%AE/01-%E5%9F%BA%E7%A1%80%E5%8F%8A%E5%AE%89%E8%A3%85/01-CDH6.2%E5%AE%89%E8%A3%85/">CDH6.2安装</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="CDH集群搭建-6-2-0版本"><a href="#CDH集群搭建-6-2-0版本" class="headerlink" title="CDH集群搭建[6.2.0版本]"></a>CDH集群搭建[6.2.0版本]</h1><h2 id="大数据集群搭建方式选择"><a href="#大数据集群搭建方式选择" class="headerlink" title="大数据集群搭建方式选择"></a>大数据集群搭建方式选择</h2><p><strong>逐个安装大数据组件：</strong></p>
<ul>
<li>优点：灵活，深入各组件安装细节</li>
<li>缺点：繁琐，无统一监控、管理工具</li>
</ul>
<p><strong>使用CDH，Ambari管理工具安装：</strong></p>
<ul>
<li>优点：一站式安装大部分组件，提供监控、管理功能</li>
<li>缺点：需要更多的资源，安装过程有一定复杂性</li>
</ul>
<p><strong>建议：</strong>机器资源足够则使用CDH方式，否则使用逐个安装方式</p>
<h2 id="什么是CDH？"><a href="#什么是CDH？" class="headerlink" title="什么是CDH？"></a>什么是CDH？</h2><p>全称：Cloudera’s Distribution Including Apache Hadoop，Cloudera 公司的Hadoop版本，包含大数据组件和监控管理工具（Cloudera Manager)两部分</p>
<h2 id="Cloudera-Manger-架构"><a href="#Cloudera-Manger-架构" class="headerlink" title="Cloudera Manger 架构"></a>Cloudera Manger 架构</h2><p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211107112003.png"></p>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211107112119.png"></p>
<h2 id="CDH-6-2-0-组件版本"><a href="#CDH-6-2-0-组件版本" class="headerlink" title="CDH 6.2.0 组件版本"></a>CDH 6.2.0 组件版本</h2><table>
<thead>
<tr>
<th>组件名</th>
<th>版本</th>
</tr>
</thead>
<tbody><tr>
<td>Apache Hadoop</td>
<td>3.0.0</td>
</tr>
<tr>
<td>Apache HBase</td>
<td>2.1.2</td>
</tr>
<tr>
<td>Apache Hive</td>
<td>2.1.1</td>
</tr>
<tr>
<td>Hue</td>
<td>4.3.0</td>
</tr>
<tr>
<td>Apache Impala</td>
<td>3.2.0</td>
</tr>
<tr>
<td>Apache Kafka</td>
<td>2.1.0</td>
</tr>
<tr>
<td>Apache Spark</td>
<td>2.4.0</td>
</tr>
<tr>
<td>Apache Sqoop</td>
<td>1.4.7</td>
</tr>
<tr>
<td>Apache ZooKeeper</td>
<td>3.4.5</td>
</tr>
</tbody></table>
<p>完整参考：<a target="_blank" rel="noopener" href="https://docs.cloudera.com/documentation/enterprise/6/release-notes/topics/rg_cdh_62_packaging.html#cdh_packaging_60x">https://docs.cloudera.com/documentation/enterprise/6/release-notes/topics/rg_cdh_62_packaging.html#cdh_packaging_60x</a></p>
<h2 id="安装环境"><a href="#安装环境" class="headerlink" title="安装环境"></a>安装环境</h2><ul>
<li>操作系统：centos 7 64位</li>
<li>CDH：6.2.0</li>
<li>JDK：1.8</li>
</ul>
<p><strong>机器资源</strong></p>
<p>内存：主机在20G以上，可以搭建三台虚拟机，其中master内存在8G以上，slave内存在4G以上</p>
<p>硬盘：每个虚机的硬盘空间100G+ </p>
<p><strong>集群结构</strong></p>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211107112751.png"></p>
<p><strong>需下载的安装包（均可从网盘下载）</strong></p>
<table>
<thead>
<tr>
<th><strong>安装包</strong></th>
<th><strong>下载地址</strong></th>
</tr>
</thead>
<tbody><tr>
<td>virtualbox：虚拟机</td>
<td><a target="_blank" rel="noopener" href="https://www.virtualbox.org/wiki/Downloads">https://www.virtualbox.org/wiki/Downloads</a></td>
</tr>
<tr>
<td>centos镜像</td>
<td><a target="_blank" rel="noopener" href="http://vault.centos.org/6.8/isos/x86_64/">http://vault.centos.org/6.8/isos/x86_64/</a></td>
</tr>
<tr>
<td>parcel：所有大数据组件，以二进制方式打包在一个文件中</td>
<td>网盘下载</td>
</tr>
<tr>
<td>Clouder Manager相关文件</td>
<td>网盘下载</td>
</tr>
</tbody></table>
<p>链接：<a target="_blank" rel="noopener" href="https://pan.baidu.com/s/1wcxyyydj632rsgEIlwT8hg">https://pan.baidu.com/s/1wcxyyydj632rsgEIlwT8hg</a><br>提取码：tzm3																																																											</p>
<h1 id="虚拟机设置"><a href="#虚拟机设置" class="headerlink" title="虚拟机设置"></a>虚拟机设置</h1><p>以下步骤如未特别说明，均需在<strong>所有节点</strong>操作</p>
<h2 id="Step1-安装虚拟机"><a href="#Step1-安装虚拟机" class="headerlink" title="Step1: 安装虚拟机"></a>Step1: 安装虚拟机</h2><p>建议：安装设置好一台后通过导入导出功能复制其他两台</p>
<h2 id="Step2-设置网络"><a href="#Step2-设置网络" class="headerlink" title="Step2: 设置网络"></a>Step2: 设置网络</h2><p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211107171429.png"></p>
<p>设置参考：《 VirtualBox配置Centos7固定IP和外网访问.pdf 》</p>
<h2 id="Step3-设置hosts-后续操作均以root用户执行"><a href="#Step3-设置hosts-后续操作均以root用户执行" class="headerlink" title="Step3:设置hosts(后续操作均以root用户执行)"></a>Step3:设置hosts(后续操作均以root用户执行)</h2><ol>
<li><p>确定各节点hostname 正确设置：</p>
<p>vi &#x2F;etc&#x2F;hostname</p>
</li>
<li><p>设置hosts：</p>
<p>vi &#x2F;etc&#x2F;hosts<br>192.168.31.110 master<br>192.168.31.111 slave01<br>192.168.31.112 slave02</p>
</li>
</ol>
<h2 id="Step4-关闭SELinux及防火墙"><a href="#Step4-关闭SELinux及防火墙" class="headerlink" title="Step4:关闭SELinux及防火墙"></a>Step4:关闭SELinux及防火墙</h2><ol>
<li>关闭SELinux:<br>vi &#x2F;etc&#x2F;selinux&#x2F;config，修改如下：<br>SELINUX&#x3D;disabled</li>
<li>关闭防火墙：<br>systemctl stop firewalld<br>systemctl disable firewalld</li>
</ol>
<h2 id="Step5-设置ssh免密登录"><a href="#Step5-设置ssh免密登录" class="headerlink" title="Step5: 设置ssh免密登录"></a>Step5: 设置ssh免密登录</h2><ol>
<li>生成密钥：<br>ssh-keygen -t rsa（默认位于 ~&#x2F;.ssh&#x2F;）</li>
<li>拷贝公钥到所有机器：<br>ssh-copy-id root@master<br>ssh-copy-id root@slave01<br>ssh-copy-id root@slave02</li>
<li>测试免密登录：<br>ssh master<br>ssh slave01<br>ssh slave02</li>
</ol>
<h2 id="Step6-设置yum源为阿里云的源"><a href="#Step6-设置yum源为阿里云的源" class="headerlink" title="Step6: 设置yum源为阿里云的源"></a>Step6: 设置yum源为阿里云的源</h2><ol>
<li>备份原有配置：<br>mkdir &#x2F;etc&#x2F;yum.repos.d&#x2F;repo-bak<br>mv &#x2F;etc&#x2F;yum.repos.d&#x2F;*.repo &#x2F;etc&#x2F;yum.repos.d&#x2F;repo-bak&#x2F;</li>
<li>将安装包下面的<strong>CentOS-Base.repo</strong>文件拷贝至 &#x2F;etc&#x2F;yum.repos.d&#x2F; 目录</li>
<li>更新缓存：<br>yum clean all<br>yum makecache</li>
</ol>
<h2 id="Step7-设置ntp时间同步服务"><a href="#Step7-设置ntp时间同步服务" class="headerlink" title="Step7: 设置ntp时间同步服务"></a>Step7: 设置ntp时间同步服务</h2><ol>
<li>安装 ntp<br>yum install ntpdate ntp -y</li>
<li>启动并设置NTP服务开机启动<br>systemctl start ntpd<br>systemctl enable ntpd</li>
</ol>
<h2 id="Step8-设置ntp时间同步服务"><a href="#Step8-设置ntp时间同步服务" class="headerlink" title="Step8: 设置ntp时间同步服务"></a>Step8: 设置ntp时间同步服务</h2><p>将master设置为主服务器（在master节点操作）：</p>
<ol>
<li><p>vi &#x2F;etc&#x2F;ntp.conf，内容如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">driftfile /var/lib/ntp/ntp.drift #草稿文件</span><br><span class="line">允许内网其他机器同步时间（192.168.137.0 修改为自己的ip掩码）</span><br><span class="line">restrict 192.168.137.0 mask 255.255.255.0 nomodify notrap</span><br><span class="line"> </span><br><span class="line"># Use public servers from the pool.ntp.org project.</span><br><span class="line"># 中国这边最活跃的时间服务器 : [http://www.pool.ntp.org/zone/cn](http://www.pool.ntp.org/zone/cn)</span><br><span class="line">server 210.72.145.44 perfer   # 中国国家受时中心</span><br><span class="line">server 202.112.10.36             # 1.cn.pool.ntp.org</span><br><span class="line">server 59.124.196.83             # 0.asia.pool.ntp.org</span><br><span class="line"> </span><br><span class="line"># allow update time by the upper server </span><br><span class="line"># 允许上层时间服务器主动修改本机时间</span><br><span class="line">restrict 210.72.145.44 nomodify notrap noquery</span><br><span class="line">restrict 202.112.10.36 nomodify notrap noquery</span><br><span class="line">restrict 59.124.196.83 nomodify notrap noquery</span><br><span class="line"> </span><br><span class="line"># 外部时间服务器不可用时，以本地时间作为时间服务</span><br><span class="line">server  127.127.1.0     # local clock</span><br><span class="line">fudge   127.127.1.0 stratum 10</span><br></pre></td></tr></table></figure>
</li>
<li><p>重启服务： service ntpd restart</p>
</li>
<li><p>查看同步状态： netstat -tlunp | grep ntp</p>
</li>
</ol>
<h2 id="Step9-设置ntp时间同步服务"><a href="#Step9-设置ntp时间同步服务" class="headerlink" title="Step9: 设置ntp时间同步服务"></a>Step9: 设置ntp时间同步服务</h2><p>设置slave到master 的同步（在slave节点操作）：</p>
<ol>
<li><p>vi &#x2F;etc&#x2F;ntp.conf，内容如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">driftfile /var/lib/ntp/ntp.drift # 草稿文件</span><br><span class="line"></span><br><span class="line">statsdir /var/log/ntpstats/</span><br><span class="line">statistics loopstats peerstats clockstats</span><br><span class="line">filegen loopstats file loopstats type day enable</span><br><span class="line">filegen peerstats file peerstats type day enable</span><br><span class="line">filegen clockstats file clockstats type day enable</span><br><span class="line"></span><br><span class="line"># 让NTP Server为内网的ntp服务器（ 192.168.31.110修改为master节点ip）</span><br><span class="line">server 192.168.31.110</span><br><span class="line">fudge 192.168.31.110 stratum 5</span><br><span class="line"></span><br><span class="line"># 不允许来自公网上ipv4和ipv6客户端的访问</span><br><span class="line">restrict -4 default kod notrap nomodify nopeer noquery </span><br><span class="line">restrict -6 default kod notrap nomodify nopeer noquery</span><br><span class="line"></span><br><span class="line"># Local users may interrogate the ntp server more closely.</span><br><span class="line">restrict 127.0.0.1</span><br><span class="line">restrict ::1</span><br><span class="line"></span><br></pre></td></tr></table></figure>
</li>
<li><p>重启服务： service ntpd restart</p>
</li>
<li><p>手动同步： ntpdate -u 192.168.31.110</p>
</li>
</ol>
<h2 id="Step10-安装CM本地yum仓库-仅master节点"><a href="#Step10-安装CM本地yum仓库-仅master节点" class="headerlink" title="Step10: 安装CM本地yum仓库(仅master节点)"></a>Step10: 安装CM本地yum仓库(仅master节点)</h2><ol>
<li><p>安装http服务：<br>yum –y install httpd</p>
</li>
<li><p>修改http服务配置(依次执行如下命令)：<br>cp &#x2F;etc&#x2F;httpd&#x2F;conf&#x2F;httpd.conf &#x2F;etc&#x2F;httpd&#x2F;conf&#x2F;httpd.conf.<code>date +%F</code><br>grep “.tgz” &#x2F;etc&#x2F;httpd&#x2F;conf&#x2F;httpd.conf | grep -v ‘    #’<br>sed -i s’#.tgz#.tgz .parcel#’ &#x2F;etc&#x2F;httpd&#x2F;conf&#x2F;httpd.conf<br>grep “.tgz” &#x2F;etc&#x2F;httpd&#x2F;conf&#x2F;httpd.conf | grep -v ‘    #’</p>
</li>
<li><p>启动http服务并验证状态是否正常：<br>systemctl start httpd<br>systemctl status httpd</p>
</li>
<li><p>安装工具包：<br>yum -y install yum-utils createrepo yum-plugin-priorities</p>
</li>
<li><p>将cm 的rpm安装包放到http服务目录下：</p>
<ol>
<li><p>执行：mkdir -p &#x2F;var&#x2F;www&#x2F;html&#x2F;cm6&#x2F;6.2.0</p>
</li>
<li><p>将网盘【安装包\jdk&amp;cm 】目录下的文件拷贝至上面目录中，确保目录中包含如下文件：</p>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211107172933.png"></p>
</li>
</ol>
</li>
<li><p>创建repo：</p>
<ol>
<li><p>执行：createrepo</p>
</li>
<li><p>查看目录下是否包含repodata目录</p>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211107172945.png"></p>
</li>
</ol>
</li>
<li><p>修改安装包中cloudera-manager.repo文件，将下面地址改为master节点地址：</p>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211107173104.png"></p>
</li>
<li><p>将修改后的文件拷贝至  &#x2F;etc&#x2F;yum.repos.d&#x2F; 目录</p>
</li>
<li><p>验证yum仓库是否配置正确：</p>
<p>yum repolist</p>
</li>
</ol>
<h2 id="Step11-上传parcel文件-仅master节点"><a href="#Step11-上传parcel文件-仅master节点" class="headerlink" title="Step11: 上传parcel文件(仅master节点)"></a>Step11: 上传parcel文件(仅master节点)</h2><ol>
<li><p>在master节点创建目录：mkdir -p &#x2F;var&#x2F;www&#x2F;html&#x2F;cdh6.2</p>
</li>
<li><p>将网盘中【安装包&#x2F;parcel】文件夹下面的内容拷贝至上面的目录中，上传后确保目录中包含如下文件：</p>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211107173310.png"></p>
</li>
</ol>
<h2 id="Step12-本地仓库检查"><a href="#Step12-本地仓库检查" class="headerlink" title="Step12: 本地仓库检查"></a>Step12: 本地仓库检查</h2><ol>
<li><p>在网页中打开 <a target="_blank" rel="noopener" href="http://master/cm6%EF%BC%8C%E7%A1%AE%E8%AE%A4%E6%98%BE%E7%A4%BA%E5%A6%82%E4%B8%8B%E5%86%85%E5%AE%B9%EF%BC%9A">http://master/cm6，确认显示如下内容：</a></p>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211107173420.png"></p>
</li>
<li><p>在网页中打开<a target="_blank" rel="noopener" href="http://master/cdh6.2%EF%BC%8C%E7%A1%AE%E8%AE%A4%E6%98%BE%E7%A4%BA%E5%A6%82%E4%B8%8B%E5%86%85%E5%AE%B9%EF%BC%9A">http://master/cdh6.2，确认显示如下内容：</a></p>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211107173608.png"></p>
</li>
</ol>
<h1 id="CDH安装"><a href="#CDH安装" class="headerlink" title="CDH安装"></a>CDH安装</h1><h2 id="Step13-安装jdk-所有节点"><a href="#Step13-安装jdk-所有节点" class="headerlink" title="Step13: 安装jdk(所有节点)"></a>Step13: 安装jdk(所有节点)</h2><ol>
<li><p>将网盘中【安装包\jdk&amp;cm】下的oracle-j2sdk1.8-1.8.0+update181-1.x86_64.rpm文件拷贝至各节点</p>
</li>
<li><p>在目录下执行：<br>yum -y install oracle-j2sdk1.8-1.8.0+update181-1.x86_64.rpm</p>
</li>
<li><p>设置java路径：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">vi /etc/profile</span><br><span class="line">在该文件末尾添加以下行</span><br><span class="line">export JAVA_HOME=/usr/java/jdk1.8.0_181-cloudera/</span><br><span class="line">export PATH=$JAVA_HOME/bin:$PATH</span><br></pre></td></tr></table></figure>
</li>
<li><p>检查安装：<br>java -version</p>
</li>
</ol>
<h2 id="Step14-安装cm-server-master节点"><a href="#Step14-安装cm-server-master节点" class="headerlink" title="Step14: 安装cm server(master节点)"></a>Step14: 安装cm server(master节点)</h2><p>在master节点上执行：<br>yum -y install cloudera-manager-daemons cloudera-manager-server</p>
<h2 id="Step15-安装mysql数据库-master节点"><a href="#Step15-安装mysql数据库-master节点" class="headerlink" title="Step15: 安装mysql数据库(master节点)"></a>Step15: 安装mysql数据库(master节点)</h2><p>需要用到数据库的组件：</p>
<ul>
<li>Cloudera Manager Server</li>
<li>Cloudera Management Service roles:<ul>
<li>Activity Monitor</li>
<li>Reports Manager</li>
</ul>
</li>
<li>Cloudera Navigator Audit Server</li>
<li>Cloudera Navigator Metadata Server</li>
<li>Hue</li>
<li>Each Hive metastore</li>
<li>Oozie</li>
</ul>
<p>参考：<a target="_blank" rel="noopener" href="https://docs.cloudera.com/documentation/enterprise/6/6.2/topics/cm_ig_mysql.html#cmig_topic_5_5">https://docs.cloudera.com/documentation/enterprise/6/6.2/topics/cm_ig_mysql.html#cmig_topic_5_5</a></p>
<p>参考： mysql安装步骤.txt</p>
<p>根据下表组件中的信息运行如下语句为组件创建用户(可以直接运行：mysql_init.sql )：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> DATABASE <span class="operator">&lt;</span>database<span class="operator">&gt;</span> <span class="keyword">DEFAULT</span> <span class="type">CHARACTER</span> <span class="keyword">SET</span> utf8 <span class="keyword">DEFAULT</span> <span class="keyword">COLLATE</span> utf8_general_ci;</span><br><span class="line"><span class="keyword">GRANT</span> <span class="keyword">ALL</span> <span class="keyword">ON</span> <span class="operator">&lt;</span>database<span class="operator">&gt;</span>.<span class="operator">*</span> <span class="keyword">TO</span> <span class="string">&#x27;&lt;user&gt;&#x27;</span>@<span class="string">&#x27;%&#x27;</span> IDENTIFIED <span class="keyword">BY</span> <span class="string">&#x27;&lt;password&gt;’;</span></span><br></pre></td></tr></table></figure>

<table>
<thead>
<tr>
<th><strong>Service</strong></th>
<th><strong>Database</strong></th>
<th><strong>User</strong></th>
</tr>
</thead>
<tbody><tr>
<td>Cloudera Manager Server</td>
<td>scm</td>
<td>scm</td>
</tr>
<tr>
<td>Activity Monitor</td>
<td>amon</td>
<td>amon</td>
</tr>
<tr>
<td>Reports Manager</td>
<td>rman</td>
<td>rman</td>
</tr>
<tr>
<td>Hue</td>
<td>hue</td>
<td>hue</td>
</tr>
<tr>
<td>Hive Metastore Server</td>
<td>metastore</td>
<td>hive</td>
</tr>
<tr>
<td>Sentry Server</td>
<td>sentry</td>
<td>sentry</td>
</tr>
<tr>
<td>Cloudera Navigator Audit Server</td>
<td>nav</td>
<td>nav</td>
</tr>
<tr>
<td>Cloudera Navigator Metadata Server</td>
<td>navms</td>
<td>navms</td>
</tr>
<tr>
<td>Oozie</td>
<td>oozie</td>
<td>oozie</td>
</tr>
</tbody></table>
<p><strong>在master节点运行如下脚本配置scm server数据库：</strong><br>sudo &#x2F;opt&#x2F;cloudera&#x2F;cm&#x2F;schema&#x2F;scm_prepare_database.sh mysql scm scm</p>
<h2 id="Step16-安装CDH-master节点"><a href="#Step16-安装CDH-master节点" class="headerlink" title="Step16: 安装CDH(master节点)"></a>Step16: 安装CDH(master节点)</h2><ol>
<li>启动scm server：<br>sudo systemctl start cloudera-scm-server</li>
<li>查看日志，检查是否启动完成：<br>sudo tail -f &#x2F;var&#x2F;log&#x2F;cloudera-scm-server&#x2F;cloudera-scm-server.log<br>–查看到【WebServerImpl:com.cloudera.server.cmf.WebServerImpl: Started Jetty server.】日志表示启动完成</li>
<li>登录主页(账号&#x2F;密码：admin&#x2F;admin)：<br><a target="_blank" rel="noopener" href="http://master:7180/">http://master:7180</a></li>
<li>初始化CDH，建立Cluster</li>
</ol>
<h2 id="Step17-创建集群-cm仓库设置"><a href="#Step17-创建集群-cm仓库设置" class="headerlink" title="Step17: 创建集群-cm仓库设置"></a>Step17: 创建集群-cm仓库设置</h2><p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211107174313.png"></p>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211107174337.png"></p>
<h2 id="Step18-HDFS设置"><a href="#Step18-HDFS设置" class="headerlink" title="Step18: HDFS设置"></a>Step18: HDFS设置</h2><ul>
<li>在CM console中将副本设为2：dfs.replication&#x3D;2</li>
<li>在master服务器命令行中执行： hadoop fs -setrep -w 2 &#x2F;</li>
</ul>
<h2 id="CDH启动与关闭"><a href="#CDH启动与关闭" class="headerlink" title="CDH启动与关闭"></a>CDH启动与关闭</h2><p><strong>CM Portal 地址：</strong><br>    <a target="_blank" rel="noopener" href="http://master:7180/cmf/home">http://master:7180/cmf/home</a></p>
<p><strong>关闭步骤：</strong></p>
<ol>
<li>在CM portal上关闭 cluster</li>
<li>在所有节点关闭CM agent： sudo systemctl stop cloudera-scm-agent</li>
<li>在master节点关闭CM server： sudo systemctl stop cloudera-scm-server</li>
</ol>
<p><strong>启动步骤：</strong></p>
<ol>
<li>在所有节点启动CM agent： sudo systemctl start cloudera-scm-agent</li>
<li>在master节点启动CM server： sudo systemctl start cloudera-scm-server</li>
<li>在CM portal上启动 cluster</li>
</ol>
<p><strong>查看启动日志：</strong></p>
<p> &#x2F;var&#x2F;log&#x2F;cloudera-scm-server&#x2F;cloudera-scm-server.log<br> &#x2F;var&#x2F;log&#x2F;cloudera-scm-agent&#x2F;cloudera-scm-agent.log</p>
<h1 id="安装总结"><a href="#安装总结" class="headerlink" title="安装总结"></a>安装总结</h1><ul>
<li>仔细，认真，严格按照步骤</li>
<li>常见问题：网络，防火墙等主机设置</li>
<li>由于资源与网络问题，CDH安装和运行过程可能会不稳定，组件安装不成功或页面有些告警，可以先通过重试看是否能解决</li>
<li>碰到问题：</li>
</ul>
<ol>
<li>查看日志:<ol>
<li>cloudera manager server</li>
<li>cloudera manager agent</li>
</ol>
</li>
<li>官网&amp;Google&amp;百度</li>
<li>官方参考： <a target="_blank" rel="noopener" href="https://docs.cloudera.com/documentation/enterprise/6/6.2/topics/installation.html">https://docs.cloudera.com/documentation/enterprise/6/6.2/topics/installation.html</a></li>
</ol>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2020/08/15/13-%E5%A4%A7%E6%95%B0%E6%8D%AE/01-%E5%9F%BA%E7%A1%80%E5%8F%8A%E5%AE%89%E8%A3%85/01-CDH6.2%E5%AE%89%E8%A3%85/" data-id="clmcxec62000eu8wafjsw9ltj" data-title="CDH6.2安装" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/CDH/" rel="tag">CDH</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-13-大数据/01-基础及安装/02-HDFS" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2020/08/15/13-%E5%A4%A7%E6%95%B0%E6%8D%AE/01-%E5%9F%BA%E7%A1%80%E5%8F%8A%E5%AE%89%E8%A3%85/02-HDFS/" class="article-date">
  <time class="dt-published" datetime="2020-08-15T08:50:45.000Z" itemprop="datePublished">2020-08-15</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/">大数据</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2020/08/15/13-%E5%A4%A7%E6%95%B0%E6%8D%AE/01-%E5%9F%BA%E7%A1%80%E5%8F%8A%E5%AE%89%E8%A3%85/02-HDFS/">HDFS</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="HDFS简介"><a href="#HDFS简介" class="headerlink" title="HDFS简介"></a>HDFS简介</h1><h2 id="什么是HDFS？"><a href="#什么是HDFS？" class="headerlink" title="什么是HDFS？"></a>什么是HDFS？</h2><p>Hadoop 分布式文件系统</p>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211121183230.png"></p>
<h2 id="起源与发展"><a href="#起源与发展" class="headerlink" title="起源与发展"></a>起源与发展</h2><p>Hadoop之父 <strong>Doug Cutting</strong></p>
<ol>
<li>Hadoop最早起源于<strong>lucene</strong>下的<strong>Nutch</strong>。Nutch的设计目标是构建一个大型的全网搜索引擎，包括网页抓取、索引、查询等功能，但随着抓取网页数量的增加，遇到了严重的可扩展性问题——如何解决数十亿网页的存储和索引问题。</li>
<li>2003年、2004年谷歌发表的三篇论文为该问题提供了可行的解决方案。<br>——分布式文件系统（GFS），可用于处理海量网页的存储<br>——分布式计算框架MAPREDUCE，可用于处理海量网页的索引计算问题。<br>——分布式的结构化数据存储系统<strong>Bigtable</strong>，用来处理海量结构化数据。</li>
<li>Doug Cutting基于这三篇论文完成了相应的开源实现HDFS和MAPREDUCE，并从Nutch中剥离成为独立项目HADOOP，到2008年1月，HADOOP成为Apache顶级项目(同年，<strong>cloudera</strong>公司成立)，迎来了它的快速发展期。</li>
</ol>
<p>0.x-1.x：hdfs，mapreduce</p>
<p>2.x：hdfs（namenode 支持ha），mapreduce，yarn</p>
<p>3.x：hdfs（namenode 支持ha），mapreduce，yarn，其它高级特性</p>
<h2 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h2><ul>
<li>基于JAVA实现的一个分布式文件系统</li>
<li>基于unix&#x2F;linux</li>
<li>是Hadoop最重要的核心组件</li>
<li>支持顺序写入，而非随机定位读写</li>
</ul>
<h2 id="HDFS前提和设计目标"><a href="#HDFS前提和设计目标" class="headerlink" title="HDFS前提和设计目标"></a>HDFS前提和设计目标</h2><ul>
<li><p>存储超大文件</p>
<p>HDFS适合存储大文件，单个文件大小通常在百MB以上 </p>
<p>HDFS适合存储海量文件，总存储量可达PB,EB级 </p>
</li>
<li><p>硬件容错<br>基于普通机器搭建，硬件错误是常态而不是异常，因此错误检测和快速、自动的恢复是HDFS最核心的架构目标</p>
</li>
<li><p>流式数据访问<br>为数据批处理而设计，关注数据访问的高吞吐量</p>
</li>
<li><p>简单的一致性模型<br>一次写入，多次读取<br>一个文件经过创建、写入和关闭之后就不需要改变</p>
</li>
<li><p>本地计算<br>将计算移动到数据附近</p>
</li>
</ul>
<h1 id="HDFS构成及工作原理解析"><a href="#HDFS构成及工作原理解析" class="headerlink" title="HDFS构成及工作原理解析"></a>HDFS构成及工作原理解析</h1><h2 id="基本构成"><a href="#基本构成" class="headerlink" title="基本构成"></a>基本构成</h2><ul>
<li>数据块<ul>
<li>文件以块为单位进行切分存储，块通常设置的比较大（最小6M，默认 128M） </li>
<li>块越大，寻址越快，读取效率越高，但同时由于MapReduce任务也是以块为最小单位来处理，所以太大的块不利于于对数据的并行处理</li>
<li>一个文件至少占用一个块（逻辑概念）</li>
</ul>
</li>
<li>Namenode与Datanode<ul>
<li>namenode 负责维护整个文件系统的信息，包括：整个文件树，文件的块分布信息，文件系统的元数据，数据复制策略等</li>
<li>datanode 存储文件内容，负责文件实际的读写操作，保持与namenode的通信，同步文件块信息</li>
</ul>
</li>
</ul>
<h2 id="数据读写过程"><a href="#数据读写过程" class="headerlink" title="数据读写过程"></a>数据读写过程</h2><h3 id="数据写入过程"><a href="#数据写入过程" class="headerlink" title="数据写入过程"></a>数据写入过程</h3><ol>
<li><p>client 发起创建文件请求，namenode 检查是否可以创建文件，检查权限、检查目录结构、集群是否可用、检查文件租约（lease）</p>
</li>
<li><p>namenode 返回 client 确认，可以创建文件</p>
</li>
<li><p>client 在本地切分 block，具体切分由 dfs.replication、block.size 俩个参数指定，默认 dfs.replication&#x3D;3、block.size&#x3D;128m，文件130m，需要创建俩个 block，client 向 namenode 发送创建 block 请求</p>
</li>
<li><p>namenode 给 block 分配写入的 datanode，分配策略考虑的因素有数据可靠性、数据的写入效率、datanode 的负载均衡性。第一个 datanode 为与 client 最接近的 rack（机架）上的 datanode，第二个为不同 rack（机架）的 datanode，第三个为与第二个相同 rack（机架）的 datanode，namenode 返回写入的 datanode 队列 datanode1、datanode4、datanode5</p>
</li>
<li><p>client  以 pipline 方式写入 block 数据，client 向 namenode 返回的队列中第一个 datanode 写入数据，下一个 datanode 数据由前一个 datanode 写入，即 client 向 datanode1 写入数据，datanode1 向 datanode2 写入数据，datanode2 向 datanode3 写入数据。</p>
<p>为了保证数据的可靠性需要将数据写入到多个机架。</p>
<p>首先写入 datanode1 是因为 client 离 datanode1 最近，在 hdfs 是以网络结构拓扑图来定义距离。如下图的 switch 所示，client 到 datanode1 需要经过 rack1、datanode1 俩跳。</p>
<p>pipline 真正在写入时候的数据单位，client 在写入的时候以 packet（64kb）单位来发送数据，packet 内是以 chunk 为单元，在每次读取数据写入到磁盘时需要校验数据是否没有丢失，校验是以 chunk 为单位，根据 chunk 来确认接收，如果以整个 block 来确认接收力度太大，失败成本很高。</p>
<p>datanode 在接收到 client 写入数据时，会执行俩个任务，第一个是数据接收与存储任务，第二个是数据确认任务。确认任务由写入 block 的 datanode 队列中最后的 datanode 依次向前进行 ack 确认</p>
<p><strong>失败情况</strong></p>
<p><strong>在建立 pipeline 时失败</strong>，发现部分或全部 datanode 异常，client 会根据最小写入副本数（默认1）来判断是否需要 namenode 重新生成 datanode 对象。</p>
<p>例如 datanode4 异常，只有2个 datanode 满足最小写入副本数，则由 datanode1 将数据写入到 datanode5 </p>
<p><strong>发送 ack 失败</strong>，如果满足最小写入副本数则写入成功。只写入成功2份数据，不满足 dfs.replication&#x3D;3，这种情况是由 namenode 不间断接受 datanode 发送心跳、blockreport，namenode 根据 datanode 的反馈，发现 client 只写成功2个数据，namenode 会维护一个队列根据配置的副本数策略将数据补齐</p>
</li>
<li><p>client 接收到 ack 进行判断是否满足最小写入的副本数，满足则成功。不满足则失败，client 再次向 namenode 申请 datanode 队列尝试写入</p>
</li>
</ol>
<h3 id="数据读取过程"><a href="#数据读取过程" class="headerlink" title="数据读取过程"></a>数据读取过程</h3><ol>
<li><p>client 读取 &#x2F;user&#x2F;zhangsan&#x2F;sample.txt 文件向 namenode 发送请求</p>
</li>
<li><p>namenode 进行权限、目录是否存在等校验，之后根据内存中维护目录结构得到数据存在哪些 datanode 上。namenode 返回块的地址信息，例如存在俩个块，b1、b2，获取所有 block 所在节点，根据 client 读取那一个 datanode 效率进行排序返回，例如下面格式<br>b1:dn1,dn4,dn5<br>b2:dn4,dn3,dn2</p>
</li>
<li><p>读取数据</p>
<p>一次返回每个 block 所有的 datanode，这么做的好处就是读取失败后可以使用下一个 datanode 读取数据，不需要重复请求 namenode</p>
</li>
</ol>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/HDFS2.png"></p>
<h2 id="集群结构"><a href="#集群结构" class="headerlink" title="集群结构"></a>集群结构</h2><p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211121224930.png"></p>
<h2 id="namenode深入"><a href="#namenode深入" class="headerlink" title="namenode深入"></a>namenode深入</h2><h3 id="作用："><a href="#作用：" class="headerlink" title="作用："></a>作用：</h3><ul>
<li>Namespace管理：负责管理文件系统中的树状目录结构以及文件与数据块的映射关系</li>
<li>块信息管理：负责管理文件系统中文件的物理块与实际存储位置的映射关系BlocksMap</li>
<li>集群信息管理：机架信息，datanode信息</li>
<li>集中式缓存管理：从Hadoop2.3 开始，支持datanode将文件缓存到内存中，这部分缓存通过NN集中管理</li>
</ul>
<h3 id="存储结构："><a href="#存储结构：" class="headerlink" title="存储结构："></a>存储结构：</h3><ul>
<li>内存： Namespace数据，BlocksMap数据，其他信息</li>
<li>文件：<ul>
<li>已持久化的namespace数据：FsImage</li>
<li>未持久化的namespace操作：Edits</li>
</ul>
</li>
</ul>
<h3 id="启动过程："><a href="#启动过程：" class="headerlink" title="启动过程："></a>启动过程：</h3><ol>
<li>开启安全模式：不能执行数据修改操作 </li>
<li>加载fsimage</li>
<li>逐个执行所有Edits文件中的每一条操作将操作合并到fsimage，完成后生成一个空的edits文件</li>
<li>接收datanode发送来的心跳消息和块信息</li>
<li>根据以上信息确定文件系统状态</li>
<li>退出安全模式</li>
</ol>
<ul>
<li><p>安全模式：文件系统只接受读数据请求，而不接受删除、修改等变更请求 </p>
</li>
<li><p>什么情况下进入：NameNode主节点启动时，HDFS进入安全模式 </p>
</li>
<li><p>什么时候时候退出：系统达到安全标准时，HDFS退出安全模式</p>
<ul>
<li>dfs.namenode.safemode.min.datanodes: 最小可用datanode数量</li>
<li>dfs.namenode.safemode.threshold-pct: 副本数达到最小要求的block占系统总block数的百分比</li>
<li>dfs.namenode.safemode.extension: 稳定时间</li>
</ul>
</li>
<li><p>相关命令：</p>
<ul>
<li>hdfs dfsadmin -safemode get：查看当前状态</li>
<li>hdfs dfsadmin -safemode enter：进入安全模式</li>
<li>hdfs dfsadmin -safemode leave：强制离开安全模式</li>
<li>hdfs dfsadmin -safemode wait：一直等待直到安全模式结束</li>
</ul>
</li>
</ul>
<h3 id="namenode-edits-和-fsimage-关系"><a href="#namenode-edits-和-fsimage-关系" class="headerlink" title="namenode edits 和 fsimage 关系"></a>namenode edits 和 fsimage 关系</h3><ol>
<li><p>namenode 在启动的时候将 edits 合并到 fsimage，加载 fsimage 文件到内存</p>
</li>
<li><p>client 进行读操作时直接从内存读取，写操作先把数据写入到 edits 文件，再写入到内存</p>
<p>这样会导致 edits 文件越来越大，每次启动时都需要合并导致启动变慢，这时需要另一个角色 secondaty namenode</p>
<p>secondaty namenode 有俩个作用</p>
<ul>
<li>冷备，在 namenode 挂掉后 secondaty namenode 可以进行切换</li>
<li>负责在 namenode 运行过程中进行 edits 合并</li>
</ul>
</li>
</ol>
<h3 id="secondaty-namenode-合并-edtis-过程"><a href="#secondaty-namenode-合并-edtis-过程" class="headerlink" title="secondaty namenode 合并 edtis 过程"></a>secondaty namenode 合并 edtis 过程</h3><ol>
<li>secondaty namenode 在启动时会拉取 namenode 的 fsimage 文件</li>
<li>在某一个时间点通过请求获取到 namenode 的 edits 文件，在请求时 namenode 生成 edits.new 文件，对新的写操作写入到 edits.new 文件，这样是让 namenode 和 secondaty namenode 俩边的 edits 文件保持一致，这样最终合并的文件才没有问题</li>
<li>secondaty namenode 将获取到的 edits 文件与启动时拉取到的 fsimage 文件进行合并生成 fsimage.ckpt 文件</li>
<li>secondaty namenode 将 fsimage.ckpt 文件发送给 namenode</li>
<li>namenode 将旧的 fsimage 进行重命名或清理，例如：fsimage.history，把 fsimage.ckpt 修改为 fsimage。将旧的 edits 文件进行重命名或清理，例如：edits.history，将 edits.new 重命名为 edits</li>
</ol>
<p>edits 合并叫过程有一个 checkpoint 概念，触发 checkpoint 的俩个触发时机，每隔一段时间，另一个是 edits 文件大小，谁先达到就执行合并动作</p>
<p>文件系统检查点事务阈值（客户端累计操作次数）：dfs.namenode.checkpoint.txns</p>
<p>文件系统将检查点周期：dfs.namenode.checkpoint.period</p>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211127142126.png"></p>
<h2 id="HDFS-HA"><a href="#HDFS-HA" class="headerlink" title="HDFS HA"></a>HDFS HA</h2><ul>
<li><p>Datanode: 通过数据冗余保证数据的可用性</p>
</li>
<li><p>Namenode: 在2.0以前存在SPOF风险，从2.0之后：</p>
<ol>
<li>把name.dir指向NFS（Network File System）</li>
<li>QJM（Quorum Journal Manager） 方案</li>
</ol>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/HA.png"></p>
</li>
</ul>
<h2 id="HDFS文件格式"><a href="#HDFS文件格式" class="headerlink" title="HDFS文件格式"></a>HDFS文件格式</h2><p>文件格式会影响写入效率、存储效率、读取性能、读取便利性</p>
<p>HDFS 支持任意文件格式</p>
<h2 id="HDFS文件类型-列式与行式存储"><a href="#HDFS文件类型-列式与行式存储" class="headerlink" title="HDFS文件类型-列式与行式存储"></a>HDFS文件类型-列式与行式存储</h2><p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211127145757.png"></p>
<h2 id="HDFS文件类型-常用文件类型"><a href="#HDFS文件类型-常用文件类型" class="headerlink" title="HDFS文件类型-常用文件类型"></a>HDFS文件类型-常用文件类型</h2><table>
<thead>
<tr>
<th>文件格式</th>
<th>类型</th>
<th>存储方式</th>
<th>是否带 schema</th>
<th>特点描述</th>
<th>出处</th>
</tr>
</thead>
<tbody><tr>
<td>txt,json,csv</td>
<td>行式</td>
<td>文本</td>
<td>否</td>
<td>默认存储方式(txt)，数据内容可以直接cat查看，存储效率较高，处理效率低。压缩比较低</td>
<td>Hadoop</td>
</tr>
<tr>
<td>Sequence file</td>
<td>行式</td>
<td>二进制</td>
<td>是</td>
<td>以key，value对的方式存储。压缩比中等</td>
<td>Hadoop</td>
</tr>
<tr>
<td>Avro</td>
<td>行式</td>
<td>二进制</td>
<td>是</td>
<td>数据序列化框架，同时支持RPC，数据自带schema，支持比较丰富的数据类型。与protobuf, thrift 类似。压缩比中等</td>
<td>Hadoop</td>
</tr>
<tr>
<td>RC(record columnar)</td>
<td>列式</td>
<td>二进制</td>
<td>是</td>
<td>列式存储，将数据按照行组分块，读取以行组为单位，但是行组中可以跳过不需要的列。压缩比中等</td>
<td>Hive</td>
</tr>
<tr>
<td>ORC(optimized record columnar)</td>
<td>列式</td>
<td>二进制</td>
<td>是</td>
<td>升级版的RC，使用了更优化的存储结构，从而获得更好的性能，另外支持对数据的修改和ACID。压缩比高</td>
<td>Hive</td>
</tr>
<tr>
<td>Parquet</td>
<td>列式</td>
<td>二进制</td>
<td>是</td>
<td>支持嵌套类型，可高效实现对列的查询或统计。压缩比高</td>
<td>Impala</td>
</tr>
</tbody></table>
<h2 id="HDFS文件类型-如何使用？"><a href="#HDFS文件类型-如何使用？" class="headerlink" title="HDFS文件类型-如何使用？"></a>HDFS文件类型-如何使用？</h2><p>ALTER TABLE table_name <strong>SET FILEFORMAT PARQUET;</strong> </p>
<p>CREATE TABLE table_name (x INT, y STRING) <strong>STORED AS PARQUET;</strong> </p>
<p>SET <strong>hive.default.fileformat&#x3D;Orc</strong></p>
<h1 id="如何提高数据存取效率"><a href="#如何提高数据存取效率" class="headerlink" title="如何提高数据存取效率"></a>如何提高数据存取效率</h1><h2 id="存储效率"><a href="#存储效率" class="headerlink" title="存储效率"></a>存储效率</h2><p><strong>压缩</strong></p>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211127152038.png"></p>
<p><strong>纠删码</strong></p>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211127152049.png"></p>
<p><strong>异构存储</strong></p>
<p>HOT：DISK</p>
<p>WARN：DISK、ARCHIVE</p>
<p>COLD：ARCHIVE</p>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211127164830.png"></p>
<h2 id="数据压缩-作用"><a href="#数据压缩-作用" class="headerlink" title="数据压缩-作用"></a>数据压缩-作用</h2><ul>
<li>节省数据占用的磁盘空间</li>
<li>加快数据在磁盘和网络中的传输速度，从而提高系统的处理速度</li>
</ul>
<h2 id="数据压缩-评价指标"><a href="#数据压缩-评价指标" class="headerlink" title="数据压缩-评价指标"></a>数据压缩-评价指标</h2><ul>
<li>压缩比：压缩比越高，压缩后文件越小，所以压缩比越高越好</li>
<li>压缩时间：越快越好</li>
<li>已经压缩的格式文件是否可以再分割：可以分割的格式允许单一文件由多个Mapper程序处理，可以更好的并行化</li>
</ul>
<h2 id="数据压缩-类型"><a href="#数据压缩-类型" class="headerlink" title="数据压缩-类型"></a>数据压缩-类型</h2><table>
<thead>
<tr>
<th>压缩格式</th>
<th>split</th>
<th>压缩率</th>
<th>压缩速度</th>
<th>是否hadoop自带</th>
<th>linux命令</th>
<th>换成压缩格式后，原来的应用程序是否要修改</th>
<th>使用建议</th>
</tr>
</thead>
<tbody><tr>
<td>gzip</td>
<td>否</td>
<td>很高</td>
<td>比较快</td>
<td>是</td>
<td>有</td>
<td>和文本处理一样，不需要修改</td>
<td>使用方便<br/>当每个文件压缩之后在130M以内的（1个块大小内），都可以考虑用gzip压缩格式</td>
</tr>
<tr>
<td>lzo</td>
<td>是</td>
<td>比较高</td>
<td>很快</td>
<td>否，需要安装</td>
<td>有</td>
<td>需要建索引，还需要指定输入格式</td>
<td>压缩率和压缩速度综合考虑<br/>支持split，是hadoop中最流行的压缩格式；<br/>一个很大的文本文件，压缩之后还大于200M以上的可以考虑，而且单个文件越大，lzo优点越明显<br/>cloudera&amp;twitter</td>
</tr>
<tr>
<td>snappy</td>
<td>否</td>
<td>比较高</td>
<td>很快</td>
<td>否，需要安装</td>
<td>没有</td>
<td>和文本处理一样，不需要修改</td>
<td>压缩率和压缩速度综合考虑<br/>当mapreduce作业的map输出的数据比较大的时候， 作为map到reduce的中间数据的压缩格式<br/>spark默认压缩格式<br/>google出品</td>
</tr>
<tr>
<td>bzip2</td>
<td>是</td>
<td>最高</td>
<td>慢</td>
<td>是</td>
<td>有</td>
<td>和文本处理一样，不需要修改</td>
<td>压缩率高<br/>适合对速度要求不高，但需要较高的压缩率，比如数据比较大，需要压缩存档减少磁盘空间并且以后数据用得比较少的情况</td>
</tr>
</tbody></table>
<h2 id="数据压缩-使用场景？"><a href="#数据压缩-使用场景？" class="headerlink" title="数据压缩-使用场景？"></a>数据压缩-使用场景？</h2><ul>
<li><p>HDFS命令行写入：将数据压缩后写入</p>
</li>
<li><p>Flume写入：写入时指定hdfs.codeC参数 </p>
</li>
<li><p>Sqoop写入：写入时指定参数<br>–compression-codec org.apache.hadoop.io.compress.SnappyCodec</p>
</li>
<li><p>HBase 数据存储: 创建表时指定<br>create ‘xx_table’, {NAME &#x3D;&gt; ‘xx_cf’, COMPRESSION &#x3D;&gt; ‘GZ’}</p>
</li>
<li><p>Mapreduce中间结果和最终结果：hadoop jar xxx “-Dmapred.compress.map.output&#x3D;true” <br>“-Dmapred.map.output.compression.codec&#x3D;xxx” <br>“-Dmapred.output.compress&#x3D;true” “-Dmapred.output.compression.codec&#x3D;xxx”</p>
</li>
<li><p>Hive中间结果和最终结果：</p>
<p>set hive.exec.compress.intermediate&#x3D;true </p>
<p>set mapred.map.output.compression.codec&#x3D;xxx </p>
<p>set mapred.map.output.compression.codec&#x3D;xxx </p>
<p>set hive.exec.compress.output&#x3D;true </p>
<p>set mapred.output.compression.codec&#x3D;xxx</p>
</li>
</ul>
<p>Spark（RDD分区、广播变量、shuffle输出）：</p>
<ul>
<li>rdd：spark.rdd.compress，是否压缩已序列化的rdd，默认关闭</li>
<li>broadcast：spark.broadcast.compress，是否压缩broadcast数据，默认打开</li>
<li>结果存储：saveAsTextFile(path,codec)</li>
<li>压缩算法：spark.io.compression.codec，默认为snappy</li>
</ul>
<h2 id="数据压缩-使用建议"><a href="#数据压缩-使用建议" class="headerlink" title="数据压缩-使用建议"></a>数据压缩-使用建议</h2><ul>
<li>选择何种压缩格式：<ul>
<li>考虑是否支持切分</li>
<li>压缩率vs压缩速度</li>
</ul>
</li>
<li>什么时候使用：<ul>
<li>存储：磁盘空间紧张</li>
<li>计算：性能调优（内存空间占用，IO传输）</li>
</ul>
</li>
</ul>
<h2 id="纠删码-朴素原理"><a href="#纠删码-朴素原理" class="headerlink" title="纠删码-朴素原理"></a>纠删码-朴素原理</h2><p>hadoop 3.0 之后有了纠删码实现</p>
<p>复制策略：hdfs 保证数据可靠性通常是使用拷贝3份数据，这种方式会使用较多的磁盘空间，1TB数据需要3TB磁盘存储</p>
<p>纠删码：只需要复制策略50%的磁盘空间，而且同样可以保证数据的可靠性，1TB只需要1.5TB磁盘空间</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">朴素原理</span><br><span class="line"></span><br><span class="line">理解思路如下：</span><br><span class="line">x1=1</span><br><span class="line">x2=2</span><br><span class="line">x3=3</span><br><span class="line">x1+x2+x3=6</span><br><span class="line">x1+2*x2+3*x3=14</span><br><span class="line">x1+3*x2+4*x3=19</span><br><span class="line"></span><br><span class="line">我们需要求出x1，x2，x3的值，那么我们最少需要的方程数是？3</span><br><span class="line">如果有4个方程，允许丢失任意一个方程</span><br><span class="line"></span><br><span class="line">x1=1</span><br><span class="line">x2=2</span><br><span class="line">x3=3</span><br><span class="line">视为我们的数据</span><br><span class="line"></span><br><span class="line">x1+x2+x3=6</span><br><span class="line">x1+2*x2+3*x3=14</span><br><span class="line">x1+3*x2+4*x3=19</span><br><span class="line">视为一个校验/冗余数据</span><br><span class="line"></span><br><span class="line">如果是复制策略，要允许任意2份数据丢失，我们需要：3*3=9份数据</span><br><span class="line">如果是纠删码，要允许任意2份数据丢失，我们需要：3+2=5份数据，时间换空间的策略</span><br><span class="line"></span><br><span class="line">一个文件有n个块，最少需要的数据块数是多少？n+2</span><br><span class="line">不是针对一个文件所有的块进行纠删码的计算，而是按照一定的size切分成block group，按照bg来计算冗余块</span><br></pre></td></tr></table></figure>

<p>D为原始数据，C为校验&#x2F;冗余数据</p>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211127162211.png"></p>
<h2 id="纠删码-相关命令"><a href="#纠删码-相关命令" class="headerlink" title="纠删码-相关命令"></a>纠删码-相关命令</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">hdfs ec -listPolicies</span><br><span class="line">Name=RS-3-2-1024K，</span><br><span class="line">RS：实现的算法</span><br><span class="line">3：block group 原始数据块的数量，3表示按照3个数据库进行 block group 的切分</span><br><span class="line">2：要计算几个冗余块</span><br><span class="line"></span><br><span class="line">Policy默认状态是<span class="built_in">disable</span>，需要先 hdfs ec -enablePolicy -policy &lt;policy&gt; 开启</span><br><span class="line">设置纠删码策略：hdfs ec -setPolicy -path /user -policy RS-3-2-1024K</span><br><span class="line"></span><br><span class="line">hdfs ec [COMMAND] </span><br><span class="line">[-listPolicies] </span><br><span class="line">[-addPolicies -policyFile &lt;file&gt;] </span><br><span class="line">[-getPolicy -path &lt;path&gt;] </span><br><span class="line">[-removePolicy -policy &lt;policy&gt;] </span><br><span class="line">[-setPolicy -path &lt;path&gt; [-policy &lt;policy&gt;] [-replicate]] </span><br><span class="line">[-unsetPolicy -path &lt;path&gt;] </span><br><span class="line">[-listCodecs] </span><br><span class="line">[-enablePolicy -policy &lt;policy&gt;] </span><br><span class="line">[-disablePolicy -policy &lt;policy&gt;] </span><br><span class="line">[-<span class="built_in">help</span> &lt;command-name&gt;]</span><br></pre></td></tr></table></figure>

<h2 id="纠删码-使用建议"><a href="#纠删码-使用建议" class="headerlink" title="纠删码-使用建议"></a>纠删码-使用建议</h2><p>将冷门数据以纠删码格式转存，减少空间占用： </p>
<ol>
<li>指定某个目录为纠删码模式：<br>hdfs ec -setPolicy -path [path] -policy [policy]</li>
<li>通过distcp命令将原有数据转存</li>
</ol>
<h2 id="异构存储"><a href="#异构存储" class="headerlink" title="异构存储"></a>异构存储</h2><p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211127155144.png"></p>
<ol>
<li><p>配置dn存储路径时指定存储格式：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[SSD]file:///path,[ARCHIVE]file:///path</span><br></pre></td></tr></table></figure>
</li>
<li><p>dn通过心跳汇报自身数据存储目录的StorageType给nn</p>
</li>
<li><p>nn汇总并更新集群内各个节点的存储类型情况</p>
</li>
<li><p>客户端写入时根据设定的存储策略向nn请求响应的dn作为候选节点</p>
</li>
</ol>
<h2 id="异构存储-相关命令"><a href="#异构存储-相关命令" class="headerlink" title="异构存储-相关命令"></a>异构存储-相关命令</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">hdfs storagepolicies [COMMAND] </span><br><span class="line">[-listPolicies] </span><br><span class="line">[-setStoragePolicy -path &lt;path&gt; -policy &lt;policy&gt;] </span><br><span class="line">[-getStoragePolicy -path &lt;path&gt;] </span><br><span class="line">[-unsetStoragePolicy -path &lt;path&gt;] </span><br><span class="line">[-<span class="built_in">help</span> &lt;command-name&gt;]</span><br><span class="line"></span><br><span class="line">hdfs mover [-p &lt;files/dirs&gt; | -f &lt;<span class="built_in">local</span> file&gt;]</span><br></pre></td></tr></table></figure>

<h2 id="异构存储-使用建议"><a href="#异构存储-使用建议" class="headerlink" title="异构存储-使用建议"></a>异构存储-使用建议</h2><ul>
<li>一般使用默认策略（HOT，磁盘）即可</li>
<li>ARCHIVE：计算能力较弱，存储密度高，存储冷数据</li>
<li>SSD：土豪专用</li>
</ul>
<h1 id="HDFS关键设置及常见问题"><a href="#HDFS关键设置及常见问题" class="headerlink" title="HDFS关键设置及常见问题"></a>HDFS关键设置及常见问题</h1><h2 id="常用配置"><a href="#常用配置" class="headerlink" title="常用配置"></a>常用配置</h2><p>配置文件路径：$HADOOP_HOME$&#x2F;etc&#x2F;hadoop </p>
<p>主要配置文件： </p>
<ul>
<li>hdfs-site.xml</li>
<li>core-site.xm</li>
</ul>
<table>
<thead>
<tr>
<th>参数</th>
<th>描述</th>
<th>默认</th>
<th>配置文件</th>
<th>示例</th>
</tr>
</thead>
<tbody><tr>
<td>fs.default.name</td>
<td>namenode RPC交互端口</td>
<td>localhost:8020</td>
<td>core-site.xml</td>
<td>hdfs:&#x2F;&#x2F;master:8020&#x2F;</td>
</tr>
<tr>
<td>dfs.http.address</td>
<td>namenode web管理端口</td>
<td>50070</td>
<td>hdfs-site.xml</td>
<td>0.0.0.0:50070</td>
</tr>
<tr>
<td>dfs.datanode.address</td>
<td>datanode 控制端口</td>
<td>50010</td>
<td>hdfs-site.xml</td>
<td>0.0.0.0:50010</td>
</tr>
<tr>
<td>dfs.datanode.data.dir</td>
<td>数据存储路径</td>
<td>NA</td>
<td>hdfs-site.xml</td>
<td>&#x2F;mnt&#x2F;data&#x2F;dfs&#x2F;nn</td>
</tr>
<tr>
<td>dfs.namenode.name.dir</td>
<td>namenode 数据存储目录</td>
<td>NA</td>
<td>hdfs-site.xml</td>
<td>&#x2F;mnt&#x2F;data&#x2F;dfs&#x2F;nn</td>
</tr>
<tr>
<td>dfs.replication</td>
<td>默认副本数</td>
<td>3</td>
<td>hdfs-site.xml</td>
<td>3</td>
</tr>
<tr>
<td>dfs.blocksize</td>
<td>默认块大小</td>
<td>128M</td>
<td>hdfs-site.xml</td>
<td>256M</td>
</tr>
<tr>
<td>namenode_java_heapsize</td>
<td>namenode堆大小</td>
<td>2048M</td>
<td>hdfs-site.xml</td>
<td>1024M</td>
</tr>
<tr>
<td>dfs.permissions</td>
<td>是否开启权限检查</td>
<td>true</td>
<td>hdfs-site.xml</td>
<td>false</td>
</tr>
</tbody></table>
<h2 id="小文件问题"><a href="#小文件问题" class="headerlink" title="小文件问题"></a>小文件问题</h2><ul>
<li><p>定义：大量大小小于块大小的文件</p>
</li>
<li><p>实际场景：网页，Hive动态分区插入数据等</p>
</li>
<li><p>背景：每个文件的元数据对象约占150byte，所以如果有1千万个小文件，每个文件占用一个block，则NameNode大约需要2G空间。如果存储1亿个文件，则NameNode需要20G空间；数据以块为单位进行处理。</p>
</li>
<li><p>影响：占用资源，降低处理效率</p>
</li>
<li><p>解决方案：</p>
<ul>
<li><p>从源头减少小文件</p>
</li>
<li><p>使用archive打包</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop archive</span><br></pre></td></tr></table></figure>
</li>
<li><p>使用其他存储方式，如Hbase，ES等</p>
</li>
</ul>
</li>
</ul>
<h2 id="Namenode管理-内存结构"><a href="#Namenode管理-内存结构" class="headerlink" title="Namenode管理-内存结构"></a>Namenode管理-内存结构</h2><p>内存主要占用为 Namespace、BlockManager，Namespace、BlockManager 是会随着数据的增加而增加的</p>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/namenode%E5%86%85%E5%AD%98.png"></p>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211127155529.png"></p>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211127155546.png"></p>
<h2 id="Namenode-管理"><a href="#Namenode-管理" class="headerlink" title="Namenode 管理"></a>Namenode 管理</h2><p>大数据量下的namenode问题： </p>
<ol>
<li>启动时间变长 </li>
<li>性能开始下降</li>
<li>NameNode JVM FGC风险较高</li>
</ol>
<p>解决方案： </p>
<ol>
<li>根据数据增长情况，预估namenode内存需求，提前做好预案</li>
<li>使用HDFS Federation，扩展NameNode分散单点负载</li>
<li>引入外部系统支持NameNode内存数据</li>
<li>合并小文件</li>
<li>调整合适的BlockSize</li>
</ol>
<h2 id="数据迁移"><a href="#数据迁移" class="headerlink" title="数据迁移"></a>数据迁移</h2><h2 id="Namenode管理-内存预估"><a href="#Namenode管理-内存预估" class="headerlink" title="Namenode管理-内存预估"></a>Namenode管理-内存预估</h2><p>文件元数据对象约占200byte，block元数据约占180byte： </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">总内存=198 * num(Directory + Files) + 176 * num(blocks) + 2% * 总内存</span><br></pre></td></tr></table></figure>

<h2 id="数据迁移-1"><a href="#数据迁移-1" class="headerlink" title="数据迁移"></a>数据迁移</h2><p>场景：</p>
<ul>
<li>冷热数据迁移</li>
<li>集群升级、维护</li>
</ul>
<p>方案：</p>
<ul>
<li>hadoop distcp 命令</li>
</ul>
<h2 id="数据平衡"><a href="#数据平衡" class="headerlink" title="数据平衡"></a>数据平衡</h2><p>原因：长期运行的集群增删节点，节点增删磁盘等 </p>
<p>影响：</p>
<ul>
<li>跨节点拷贝数据</li>
<li>task会存在任务失败的风险</li>
<li>磁盘利用不均</li>
</ul>
<p>方案：</p>
<ul>
<li>集群节点间：hdfs balancer 命令</li>
<li>单节点磁盘间：hdfs diskbalancer 命令</li>
</ul>
<h2 id="数据平衡-hdfs-balancer"><a href="#数据平衡-hdfs-balancer" class="headerlink" title="数据平衡-hdfs balancer"></a>数据平衡-hdfs balancer</h2><p>参数：</p>
<ul>
<li><p>-threshold 30 ：判断集群是否平衡的目标参数，每一个 datanode 存储使用率和集群总存储使用率的差值的绝对值都应该小于这个阀值</p>
<p>例如：整体空间占用：30%，threshold：10，balancer 之后每个dn的空间占用：20~40%</p>
<p>dn1：60%</p>
<p>dn2：10%</p>
<p>dn3：30%</p>
<p>每个机器配置都一样的话整体占用：33.3%，threshold 10 ，balancer 后结果如下，每一个 datanode 存储使用率和集群总存储使用率的差值的绝对值都小于10</p>
<p>dn1：40%</p>
<p>dn2：30%</p>
<p>dn3：30%</p>
<p>threshold  是不是越低越好，因为在 balancer 时 client 还会写入数据，执行完 balancer 后检查发现还没有到达阈值，继续执行，导致执行时间变长。可以多次执行调整</p>
</li>
<li><p>-include ：执行balance的DN列表</p>
</li>
<li><p>dfs.balance.bandwidthPerSec 300MB ：balance工具在运行中所能占用的带宽，设置的过大会影响其他任务</p>
</li>
</ul>
<p>建议：</p>
<ul>
<li>对于一些大型的HDFS集群(随时可能扩容或下架服务器)，balance脚本建议作为后台常驻进程</li>
<li>根据官方建议，脚本需要部署在相对空闲的服务器上</li>
<li>停止脚本通过kill进程实现</li>
</ul>
<h2 id="其他管理命令"><a href="#其他管理命令" class="headerlink" title="其他管理命令"></a>其他管理命令</h2><ul>
<li>hdfs dfsadmin</li>
<li>hdfs fsck</li>
</ul>
<h1 id="Java-API"><a href="#Java-API" class="headerlink" title="Java API"></a>Java API</h1><p><a target="_blank" rel="noopener" href="https://github.com/chengchen901/demo_hdfs.git">https://github.com/chengchen901/demo_hdfs.git</a></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hadoop jar demo_hdfs-1.0-SNAPSHOT-jar-with-dependencies.jar com.study.sample.hdfs.HDFSFileWrite /root/test.txt /root/word.txt</span><br><span class="line"></span><br><span class="line">hadoop jar demo_hdfs-1.0-SNAPSHOT-jar-with-dependencies.jar com.study.sample.hdfs.HDFSFileRead /root/word.txt</span><br></pre></td></tr></table></figure>

<h1 id="思考"><a href="#思考" class="headerlink" title="思考"></a>思考</h1><ul>
<li>数据块的复制策略？ </li>
<li>调整块的大小会造成哪些影响？ </li>
<li>namenode 启动过程？ </li>
<li>namenode HA方案？ </li>
<li>secondary namenode 的作用？ </li>
<li>hdfs常用文件格式有哪些？各有什么优缺点？ </li>
<li>如何扩展HDFS的存储容量？</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2020/08/15/13-%E5%A4%A7%E6%95%B0%E6%8D%AE/01-%E5%9F%BA%E7%A1%80%E5%8F%8A%E5%AE%89%E8%A3%85/02-HDFS/" data-id="clmcxec6c000ju8wa833z0blq" data-title="HDFS" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/HDFS/" rel="tag">HDFS</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-13-大数据/01-基础及安装/03-MapReduce" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2020/08/15/13-%E5%A4%A7%E6%95%B0%E6%8D%AE/01-%E5%9F%BA%E7%A1%80%E5%8F%8A%E5%AE%89%E8%A3%85/03-MapReduce/" class="article-date">
  <time class="dt-published" datetime="2020-08-15T08:50:45.000Z" itemprop="datePublished">2020-08-15</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/">大数据</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2020/08/15/13-%E5%A4%A7%E6%95%B0%E6%8D%AE/01-%E5%9F%BA%E7%A1%80%E5%8F%8A%E5%AE%89%E8%A3%85/03-MapReduce/">MapReduce</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="MapReduce简介"><a href="#MapReduce简介" class="headerlink" title="MapReduce简介"></a>MapReduce简介</h1><h2 id="什么是MapReduce？"><a href="#什么是MapReduce？" class="headerlink" title="什么是MapReduce？"></a>什么是MapReduce？</h2><p>一种大规模数据处理的编程模型</p>
<p>源自于2004年Google发布的论文</p>
<p><strong>MapReduce in Hadoop</strong></p>
<p>开源社区实现版本，核心代码使用Java实现</p>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20220220141243.png"></p>
<h2 id="MapReduce计算场景"><a href="#MapReduce计算场景" class="headerlink" title="MapReduce计算场景"></a>MapReduce计算场景</h2><p><strong>数据查找</strong></p>
<ul>
<li>分布式Grep</li>
</ul>
<p><strong>Web访问日志分析</strong></p>
<ul>
<li>词频统计</li>
<li>网站PV UV统计</li>
<li>Top K问题</li>
</ul>
<p><strong>倒排索引</strong></p>
<ul>
<li>建立搜索引擎索引</li>
</ul>
<p><strong>分布式排序</strong></p>
<h2 id="MapReduce优点和缺点"><a href="#MapReduce优点和缺点" class="headerlink" title="MapReduce优点和缺点"></a>MapReduce优点和缺点</h2><p><strong>优点</strong></p>
<ol>
<li>模型简单，Map + Reduce</li>
<li>高伸缩性，支持横向扩展</li>
<li>灵活，结构化和非结构化数据</li>
<li>速度快，高吞吐离线处理数据</li>
<li>并行处理，编程模型天然支持并行处理</li>
<li>容错能力强</li>
</ol>
<p><strong>缺点</strong></p>
<ol>
<li>流式数据-MapReduce处理模型就决定了需要静态数据</li>
<li>实时计算-不适合低延迟数据处理，需要毫秒级别响应</li>
<li>复杂算法-例如SVM支持向量机</li>
<li>迭代计算-例如斐波那契数列</li>
</ol>
<h1 id="MapReduce编程模型"><a href="#MapReduce编程模型" class="headerlink" title="MapReduce编程模型"></a>MapReduce编程模型</h1><h2 id="如何统计一个文本中单词的出现次数？"><a href="#如何统计一个文本中单词的出现次数？" class="headerlink" title="如何统计一个文本中单词的出现次数？"></a>如何统计一个文本中单词的出现次数？</h2><h3 id="Bash命令实现"><a href="#Bash命令实现" class="headerlink" title="Bash命令实现"></a>Bash命令实现</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">tr</span> -s <span class="string">&quot; &quot;</span> <span class="string">&quot;\n&quot;</span></span><br><span class="line"><span class="built_in">sort</span> file</span><br><span class="line"><span class="built_in">uniq</span> –c</span><br><span class="line"><span class="built_in">cat</span> wordcount.txt | <span class="built_in">tr</span> -s <span class="string">&quot; &quot;</span> <span class="string">&quot;\n&quot;</span> | <span class="built_in">sort</span> | <span class="built_in">uniq</span> -c</span><br></pre></td></tr></table></figure>

<h3 id="单机版实现"><a href="#单机版实现" class="headerlink" title="单机版实现"></a>单机版实现</h3><p>使用HashMap统计</p>
<h2 id="如果数据量极大如何在分布式的机器上计算？"><a href="#如果数据量极大如何在分布式的机器上计算？" class="headerlink" title="如果数据量极大如何在分布式的机器上计算？"></a>如果数据量极大如何在分布式的机器上计算？</h2><p>MapReduce使用了分治思想简化了计算处理模型为两步：</p>
<ol>
<li>Map阶段，获得输入数据，对输入数据进行转换并输出</li>
<li>Reduce阶段，对输出结果进行聚合计算</li>
</ol>
<h2 id="MapReduce-Map"><a href="#MapReduce-Map" class="headerlink" title="MapReduce-Map"></a>MapReduce-Map</h2><p>目标：∑sin(x)</p>
<p>对输入数据集的每个值都执行函数以创建新的结果集合</p>
<p>例如：</p>
<ul>
<li>输入数据[1,2,3,4,5,6,7,8,9,10]</li>
<li>定义Map方法需要执行的变换为f(x)&#x3D;sin(x)</li>
<li>则输出结果为[0.84,0.91,0.14,-0.76,-0.96,-0.28,0.66,0.99,0.41,-0.54]</li>
</ul>
<p>形式化的表达：</p>
<ul>
<li>each &lt;key, value&gt; in input</li>
<li>map &lt;key, value&gt; to &lt;intermediate key, intermediate value&gt;</li>
</ul>
<p>map(𝑥) &#x3D; sin 𝑥</p>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211213164314.png"></p>
<h2 id="MapReduce-Reduce"><a href="#MapReduce-Reduce" class="headerlink" title="MapReduce-Reduce"></a>MapReduce-Reduce</h2><p>对Map输出的结果进行聚合，输出一个或者多个聚合结果</p>
<p>例如：</p>
<ul>
<li>Map的输出结果为[0.84,0.91,0.14,-0.76,-0.96,-0.28,0.66,0.99,0.41,-0.54]</li>
<li>使用求和+作为聚合方法</li>
<li>则输出结果为[1.41]</li>
</ul>
<p>形式化的表达：</p>
<ul>
<li>each &lt;intermediate key, List&lt; intermediate value&gt;&gt; in input</li>
<li>reduce&lt;reduce key, reduce value&gt;</li>
</ul>
<p>reduce 𝑥 &#x3D; ∑𝑥𝑖</p>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211128105346.png"></p>
<p>相同的key会被划分到同一个Reduce上</p>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211128110320.png"></p>
<h2 id="Word-Count-Example"><a href="#Word-Count-Example" class="headerlink" title="Word Count Example"></a>Word Count Example</h2><p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211128110623.png"></p>
<h2 id="Map-数据输入"><a href="#Map-数据输入" class="headerlink" title="Map 数据输入"></a>Map 数据输入</h2><p>Map阶段由一定的数量的Map Task组成</p>
<p>文件分片</p>
<ul>
<li>输入数据会被split切分为多份</li>
<li>HDFS默认Block大小<ul>
<li>Hadoop 1.0 &#x3D; 64MB</li>
<li>Hadoop 2.0 &#x3D; 128MB</li>
</ul>
</li>
<li>默认将文件解析为&lt;key, value&gt;对的实现是TextInputFormat<ul>
<li>key为偏移量</li>
<li>value为每一行内容</li>
</ul>
</li>
<li>因此有多少个Map Task任务?<ul>
<li>一个split一个Map Task，默认情况下一个block对应一个split</li>
<li>例如一个文件大小为10TB，block大小设置为128MB，则一共会有81920个Map Task任务（10 * 1024 * 1024 &#x2F; 128 &#x3D; 81920）</li>
</ul>
</li>
</ul>
<h2 id="Reduce-数据输入"><a href="#Reduce-数据输入" class="headerlink" title="Reduce 数据输入"></a>Reduce 数据输入</h2><ul>
<li>Partitioner决定了哪个Reduce会接收到Map输出的&lt;key, value&gt;对</li>
<li>在Hadoop中默认的Partitioner实现为HashPartitioner</li>
<li>计算公式<ul>
<li>Abs(Hash(key)) mod NR 其中 NR等于Reduce Task数目</li>
</ul>
</li>
<li>Partitioner可以自定义，例如，有3个Reduce Task，那么Partitioner会返回0 ~ 2</li>
</ul>
<h2 id="MapReduce-Shuffle"><a href="#MapReduce-Shuffle" class="headerlink" title="MapReduce-Shuffle"></a>MapReduce-Shuffle</h2><p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211128112253.png"></p>
<h2 id="Word-Count中的shuffle"><a href="#Word-Count中的shuffle" class="headerlink" title="Word Count中的shuffle"></a>Word Count中的shuffle</h2><p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211128112413.png"></p>
<h2 id="Shuffle"><a href="#Shuffle" class="headerlink" title="Shuffle"></a>Shuffle</h2><ul>
<li><p>为何需要shuffle</p>
<p>Reduce阶段的数据来源于不同的Map</p>
</li>
<li><p>Shuffle由Map端和Reduce端组成</p>
</li>
<li><p>Shuffle的核心机制</p>
<p>数据分区+排序</p>
</li>
<li><p>Map端</p>
<p>对Map输出结果进行spill（溢写）</p>
</li>
<li><p>Reduce端</p>
<p>拷贝Map端输出结果到本地</p>
<p>对拷贝的数据进行归并排序</p>
</li>
</ul>
<h2 id="Shuffle-Map端"><a href="#Shuffle-Map端" class="headerlink" title="Shuffle Map端"></a>Shuffle Map端</h2><p>Map端会源源不断的把数据输入到一个环形内存缓冲区</p>
<ul>
<li>MapTask.MapOutputBuffer</li>
</ul>
<p>达到一定阈值时</p>
<ul>
<li>默认0.8</li>
<li>唤醒后台溢出线程</li>
<li>内存缓冲区中的数据会溢出到磁盘</li>
</ul>
<p>在溢出的过程中</p>
<ul>
<li>调用Partitioner进行分组</li>
<li>对于每个组，按照Key进行排序</li>
</ul>
<p>Map处理完毕后</p>
<ul>
<li>对溢出到磁盘上的多个文件进行Merge操作</li>
<li>合并为一个大的文件和一个索引文件</li>
</ul>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211128113807.png"></p>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211128114319.png"></p>
<h2 id="Shuffle-Reduce端"><a href="#Shuffle-Reduce端" class="headerlink" title="Shuffle Reduce端"></a>Shuffle Reduce端</h2><p>Map端完成之后会暴露一个Http Server共Reduce端获取数据</p>
<p>Reduce启动拷贝线程从各个Map端拷贝结果</p>
<ul>
<li>有大量的网络I&#x2F;O开销</li>
</ul>
<p>一边拷贝一边进行Merge操作（归并排序）</p>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211128114656.png"></p>
<h2 id="Combiner"><a href="#Combiner" class="headerlink" title="Combiner"></a>Combiner</h2><p>Map端本地Reducer</p>
<p>合并了Map端输出数据 &#x3D;&gt; 减少Http Traffic</p>
<p>Combiner可以自定义</p>
<p>例如Word Count中，对同一个Map输出的相同的key，直接对其value进行reduce</p>
<p>可以使用Combiner的前提</p>
<ul>
<li>满足结合律：求最大值、求和</li>
<li>不适用场景：计算平均数</li>
</ul>
<p>Combiner在Map什么阶段发生？</p>
<h2 id="Combiner-Example"><a href="#Combiner-Example" class="headerlink" title="Combiner Example"></a>Combiner Example</h2><p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211128114926.png"></p>
<h1 id="MapReduce-应用-API介绍"><a href="#MapReduce-应用-API介绍" class="headerlink" title="MapReduce 应用 API介绍"></a>MapReduce 应用 API介绍</h1><h2 id="MapReduce-Java-API"><a href="#MapReduce-Java-API" class="headerlink" title="MapReduce Java API"></a>MapReduce Java API</h2><p>基于Hadoop 3.0.0版本</p>
<p>新版本API均在包org.apache.hadoop.mapreduce下面</p>
<p>编写MapReduce程序的核心</p>
<ul>
<li><p>继承Hadoop提供的Mapper类并实现其中的map方法</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">Mapper</span>&lt;KEYIN, VALUEIN, KEYOUT, VALUEOUT&gt;</span><br></pre></td></tr></table></figure>
</li>
<li><p>继承Hadoop提供的Reducer类并实现其中的reduce方法</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">Reducer</span>&lt;KEYIN,VALUEIN, KEYOUT,VALUEOUT&gt;</span><br></pre></td></tr></table></figure></li>
</ul>
<h2 id="WordCount-Mapper实现"><a href="#WordCount-Mapper实现" class="headerlink" title="WordCount Mapper实现"></a>WordCount Mapper实现</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">class</span> <span class="title class_">TokenizerMapper</span></span><br><span class="line">    <span class="keyword">extends</span> <span class="title class_">Mapper</span>&lt;Object, Text, Text, IntWritable&gt; &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">static</span> <span class="type">IntWritable</span> <span class="variable">one</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">IntWritable</span>(<span class="number">1</span>);</span><br><span class="line">    <span class="keyword">private</span> <span class="type">Text</span> <span class="variable">word</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Text</span>();</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">map</span><span class="params">(Object key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line">        <span class="comment">// StringTokenizer对读入的每行文本内容按空格进行切分</span></span><br><span class="line">        <span class="type">StringTokenizer</span> <span class="variable">itr</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">StringTokenizer</span>(value.toString());</span><br><span class="line">        <span class="comment">// 判断是否处理完一行文本</span></span><br><span class="line">        <span class="keyword">while</span> (itr.hasMoreTokens()) &#123;</span><br><span class="line">            <span class="comment">// 处理每一个Token</span></span><br><span class="line">            word.set(itr.nextToken());</span><br><span class="line">            <span class="comment">// 每遇到一个单词就输出kv对，key=单词本身，value=出现次数1次</span></span><br><span class="line">            context.write(word, one);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211128120545.png"></p>
<h2 id="WordCount-Reducer实现"><a href="#WordCount-Reducer实现" class="headerlink" title="WordCount Reducer实现"></a>WordCount Reducer实现</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">class</span> <span class="title class_">IntSumReducer</span></span><br><span class="line">    <span class="keyword">extends</span> <span class="title class_">Reducer</span>&lt;Text, IntWritable, Text, IntWritable&gt; &#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="type">IntWritable</span> <span class="variable">result</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">IntWritable</span>();</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">reduce</span><span class="params">(Text key, Iterable&lt;IntWritable&gt; values,</span></span><br><span class="line"><span class="params">                       Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line">        <span class="comment">// Reduce进行累加即可获得统计结果</span></span><br><span class="line">        <span class="type">int</span> <span class="variable">sum</span> <span class="operator">=</span> <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span> (IntWritable val : values) &#123;</span><br><span class="line">            sum += val.get();</span><br><span class="line">        &#125;</span><br><span class="line">        result.set(sum);</span><br><span class="line">        context.write(key, result);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211128120722.png"></p>
<h2 id="WordCount-Main方法实现"><a href="#WordCount-Main方法实现" class="headerlink" title="WordCount Main方法实现"></a>WordCount Main方法实现</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">    <span class="type">Configuration</span> <span class="variable">conf</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Configuration</span>();</span><br><span class="line">    <span class="comment">// 配置Job</span></span><br><span class="line">    <span class="type">Job</span> <span class="variable">job</span> <span class="operator">=</span> Job.getInstance(conf, <span class="string">&quot;word count&quot;</span>);</span><br><span class="line">    job.setJarByClass(WordCount.class);</span><br><span class="line">    job.setMapperClass(TokenizerMapper.class);</span><br><span class="line">    <span class="comment">// 这里Combiner设置的类和Reducer一致</span></span><br><span class="line">    job.setCombinerClass(IntSumReducer.class);</span><br><span class="line">    job.setReducerClass(IntSumReducer.class);</span><br><span class="line">    job.setOutputKeyClass(Text.class);</span><br><span class="line">    job.setOutputValueClass(IntWritable.class);</span><br><span class="line">    <span class="comment">// 添加输入路径和输出路径</span></span><br><span class="line">    FileInputFormat.addInputPath(job, <span class="keyword">new</span> <span class="title class_">Path</span>(args[<span class="number">0</span>]));</span><br><span class="line">    FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> <span class="title class_">Path</span>(args[<span class="number">1</span>]));</span><br><span class="line">    <span class="comment">// 向集群提交Job，并等待完成</span></span><br><span class="line">    System.exit(job.waitForCompletion(<span class="literal">true</span>) ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="WordCount-编译和执行"><a href="#WordCount-编译和执行" class="headerlink" title="WordCount 编译和执行"></a>WordCount 编译和执行</h2><p>通过mvn compile package将code打包成jar</p>
<p>将example数据上传至HDFS</p>
<ul>
<li>hadoop fs -mkdir -p &#x2F;user&#x2F;cdh&#x2F;wordcount&#x2F;input</li>
<li>hadoop fs -put example.txt &#x2F;user&#x2F;cdh&#x2F;wordcount&#x2F;input</li>
</ul>
<p>使用hadoop jar命令执行</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hadoop jar命令格式：hadoop jar &lt;job jar file path&gt; &lt;main class&gt; &lt;arg1&gt; &lt;arg2&gt; …</span><br><span class="line"></span><br><span class="line">hadoop jar demo_mapreducer-1.0-SNAPSHOT-jar-with-dependencies.jar com.study.sample.mapreducer.WordCount /user/cdh/wordcount/input /user/cdh/wordcount/output</span><br></pre></td></tr></table></figure>

<h1 id="MapReduce-源代码解析"><a href="#MapReduce-源代码解析" class="headerlink" title="MapReduce 源代码解析"></a>MapReduce 源代码解析</h1><h2 id="Hadoop-Mapper-定义"><a href="#Hadoop-Mapper-定义" class="headerlink" title="Hadoop Mapper 定义"></a>Hadoop Mapper 定义</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">Mapper</span>&lt;KEYIN, VALUEIN, KEYOUT, VALUEOUT&gt; &#123;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * The &lt;code&gt;Context&lt;/code&gt; passed on to the &#123;<span class="doctag">@link</span> Mapper&#125; implementations.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="keyword">public</span> <span class="keyword">abstract</span> <span class="keyword">class</span> <span class="title class_">Context</span></span><br><span class="line">    <span class="keyword">implements</span> <span class="title class_">MapContext</span>&lt;KEYIN,VALUEIN,KEYOUT,VALUEOUT&gt; &#123;</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Called once at the beginning of the task.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">setup</span><span class="params">(Context context</span></span><br><span class="line"><span class="params">                       )</span> <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line">    <span class="comment">// NOTHING</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Called once for each key/value pair in the input split. Most applications</span></span><br><span class="line"><span class="comment">   * should override this, but the default is the identity function.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="meta">@SuppressWarnings(&quot;unchecked&quot;)</span></span><br><span class="line">  <span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">map</span><span class="params">(KEYIN key, VALUEIN value, </span></span><br><span class="line"><span class="params">                     Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line">    context.write((KEYOUT) key, (VALUEOUT) value);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Called once at the end of the task.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">cleanup</span><span class="params">(Context context</span></span><br><span class="line"><span class="params">                         )</span> <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line">    <span class="comment">// NOTHING</span></span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Expert users can override this method for more complete control over the</span></span><br><span class="line"><span class="comment">   * execution of the Mapper.</span></span><br><span class="line"><span class="comment">   * <span class="doctag">@param</span> context</span></span><br><span class="line"><span class="comment">   * <span class="doctag">@throws</span> IOException</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">run</span><span class="params">(Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line">    setup(context);</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      <span class="keyword">while</span> (context.nextKeyValue()) &#123;</span><br><span class="line">        map(context.getCurrentKey(), context.getCurrentValue(), context);</span><br><span class="line">      &#125;</span><br><span class="line">    &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">      cleanup(context);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="Hadoop-Reducer-定义"><a href="#Hadoop-Reducer-定义" class="headerlink" title="Hadoop Reducer 定义"></a>Hadoop Reducer 定义</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">Reducer</span>&lt;KEYIN,VALUEIN,KEYOUT,VALUEOUT&gt; &#123;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * The &lt;code&gt;Context&lt;/code&gt; passed on to the &#123;<span class="doctag">@link</span> Reducer&#125; implementations.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="keyword">public</span> <span class="keyword">abstract</span> <span class="keyword">class</span> <span class="title class_">Context</span> </span><br><span class="line">    <span class="keyword">implements</span> <span class="title class_">ReduceContext</span>&lt;KEYIN,VALUEIN,KEYOUT,VALUEOUT&gt; &#123;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Called once at the start of the task.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">setup</span><span class="params">(Context context</span></span><br><span class="line"><span class="params">                       )</span> <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line">    <span class="comment">// NOTHING</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * This method is called once for each key. Most applications will define</span></span><br><span class="line"><span class="comment">   * their reduce class by overriding this method. The default implementation</span></span><br><span class="line"><span class="comment">   * is an identity function.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="meta">@SuppressWarnings(&quot;unchecked&quot;)</span></span><br><span class="line">  <span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">reduce</span><span class="params">(KEYIN key, Iterable&lt;VALUEIN&gt; values, Context context</span></span><br><span class="line"><span class="params">                        )</span> <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line">    <span class="keyword">for</span>(VALUEIN value: values) &#123;</span><br><span class="line">      context.write((KEYOUT) key, (VALUEOUT) value);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Called once at the end of the task.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">cleanup</span><span class="params">(Context context</span></span><br><span class="line"><span class="params">                         )</span> <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line">    <span class="comment">// NOTHING</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Advanced application writers can use the </span></span><br><span class="line"><span class="comment">   * &#123;<span class="doctag">@link</span> #run(org.apache.hadoop.mapreduce.Reducer.Context)&#125; method to</span></span><br><span class="line"><span class="comment">   * control how the reduce task works.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">run</span><span class="params">(Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line">    setup(context);</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      <span class="keyword">while</span> (context.nextKey()) &#123;</span><br><span class="line">        reduce(context.getCurrentKey(), context.getValues(), context);</span><br><span class="line">        <span class="comment">// If a back up store is used, reset it</span></span><br><span class="line">        Iterator&lt;VALUEIN&gt; iter = context.getValues().iterator();</span><br><span class="line">        <span class="keyword">if</span>(iter <span class="keyword">instanceof</span> ReduceContext.ValueIterator) &#123;</span><br><span class="line">          ((ReduceContext.ValueIterator&lt;VALUEIN&gt;)iter).resetBackupStore();        </span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">      cleanup(context);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="Hadoop-Partitioner-定义和默认实现"><a href="#Hadoop-Partitioner-定义和默认实现" class="headerlink" title="Hadoop Partitioner 定义和默认实现"></a>Hadoop Partitioner 定义和默认实现</h2><p>Partitioner抽象类定义</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="keyword">class</span> <span class="title class_">Partitioner</span>&lt;KEY, VALUE&gt; &#123;</span><br><span class="line">  </span><br><span class="line">  <span class="comment">/** </span></span><br><span class="line"><span class="comment">   * Get the partition number for a given key (hence record) given the total </span></span><br><span class="line"><span class="comment">   * number of partitions i.e. number of reduce-tasks for the job.</span></span><br><span class="line"><span class="comment">   *   </span></span><br><span class="line"><span class="comment">   * &lt;p&gt;Typically a hash function on a all or a subset of the key.&lt;/p&gt;</span></span><br><span class="line"><span class="comment">   *</span></span><br><span class="line"><span class="comment">   * <span class="doctag">@param</span> key the key to be partioned.</span></span><br><span class="line"><span class="comment">   * <span class="doctag">@param</span> value the entry value.</span></span><br><span class="line"><span class="comment">   * <span class="doctag">@param</span> numPartitions the total number of partitions.</span></span><br><span class="line"><span class="comment">   * <span class="doctag">@return</span> the partition number for the &lt;code&gt;key&lt;/code&gt;.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="keyword">public</span> <span class="keyword">abstract</span> <span class="type">int</span> <span class="title function_">getPartition</span><span class="params">(KEY key, VALUE value, <span class="type">int</span> numPartitions)</span>;</span><br><span class="line">  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>默认HashPartitioner实现</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">HashPartitioner</span>&lt;K, V&gt; <span class="keyword">extends</span> <span class="title class_">Partitioner</span>&lt;K, V&gt; &#123;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/** Use &#123;<span class="doctag">@link</span> Object#hashCode()&#125; to partition. */</span></span><br><span class="line">  <span class="keyword">public</span> <span class="type">int</span> <span class="title function_">getPartition</span><span class="params">(K key, V value,</span></span><br><span class="line"><span class="params">                          <span class="type">int</span> numReduceTasks)</span> &#123;</span><br><span class="line">    <span class="keyword">return</span> (key.hashCode() &amp; Integer.MAX_VALUE) % numReduceTasks;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="Hadoop-InputFormat-定义和默认实现"><a href="#Hadoop-InputFormat-定义和默认实现" class="headerlink" title="Hadoop InputFormat 定义和默认实现"></a>Hadoop InputFormat 定义和默认实现</h2><table>
<thead>
<tr>
<th>输入格式</th>
<th>功能描述</th>
<th>键类型</th>
<th>值类型</th>
</tr>
</thead>
<tbody><tr>
<td>TextInputFormat</td>
<td>默认格式，读取文件行</td>
<td>行字节偏移量LongWritable</td>
<td>行的内容Text</td>
</tr>
<tr>
<td>KeyValueTextInputFormat</td>
<td>读取文件行把行解析成了键值对</td>
<td>第一个定义key&#x2F;value分隔符之前的所有字符Text</td>
<td>第一个定义key&#x2F;value分隔符之后的所有字符Text</td>
</tr>
<tr>
<td>SequenceFileInputFormat</td>
<td>Hadoop定义的高性能二进制格式</td>
<td>用户自定义</td>
<td>用户自定义</td>
</tr>
</tbody></table>
<h2 id="与-InputFormat-有关的类"><a href="#与-InputFormat-有关的类" class="headerlink" title="与 InputFormat 有关的类"></a>与 InputFormat 有关的类</h2><p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/1638075688(1).jpg"></p>
<h2 id="TextInputFormat处理流程"><a href="#TextInputFormat处理流程" class="headerlink" title="TextInputFormat处理流程"></a>TextInputFormat处理流程</h2><ol>
<li><p>划分split</p>
<p>调用父类FileInputFormat的getSplit方法把文件分割成splits</p>
</li>
<li><p>处理split</p>
<p>调用createRecordReader方法给每一个split分配一个LineRecordReader</p>
</li>
<li><p>生成record</p>
<p>LineRecordReader类实现RecordReader方法，并且依赖LineReader来读取每一行</p>
</li>
<li><p>处理record</p>
<p>调用map task的map方法</p>
</li>
</ol>
<h1 id="MapReduce执行机制"><a href="#MapReduce执行机制" class="headerlink" title="MapReduce执行机制"></a>MapReduce执行机制</h1><h2 id="MapReduce-on-Yarn"><a href="#MapReduce-on-Yarn" class="headerlink" title="MapReduce on Yarn"></a>MapReduce on Yarn</h2><p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211128181135.png"></p>
<h2 id="MapReduce-容错性"><a href="#MapReduce-容错性" class="headerlink" title="MapReduce 容错性"></a>MapReduce 容错性</h2><p>Case 1. 如果Task运行失败</p>
<ul>
<li>Map Task失败<ul>
<li>MRAppMaster重启Map Task，Map Task没有依赖性</li>
</ul>
</li>
<li>Reduce Task失败<ul>
<li>MRAppMaster重启Reduce Task，Map Task的输出保存在磁盘上</li>
</ul>
</li>
<li>同一个Task运行多次失败（默认4次）则本次作业失败</li>
</ul>
<p>Case 2. 如果Task所在的Node节点挂了</p>
<ul>
<li>在另外一个节点上重启所有在挂掉节点上曾经运行过的任务</li>
</ul>
<p>Case 3. 如果Task运行缓慢</p>
<ul>
<li>通常由于硬件损坏、软件Bug或者配置错误导致</li>
<li>单个task运行缓慢会显著影响整体作业运行时间</li>
<li>解决方案：推测执行<ul>
<li>在另外一个节点上启动相同的任务，谁先完成就kill掉另外一个节点上的任务</li>
</ul>
</li>
<li>无法启动推测执行的情况：写入数据库</li>
</ul>
<h2 id="MapReduce-数据本地性问题"><a href="#MapReduce-数据本地性问题" class="headerlink" title="MapReduce 数据本地性问题"></a>MapReduce 数据本地性问题</h2><ul>
<li>在集群中网络资源是一种稀缺资源</li>
<li>文件在HDFS上存储在不同的DataNode节点上</li>
<li>如果Map Task任务从远程机器上拷贝数据会消耗大量的网络带宽</li>
</ul>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211128191930.png"></p>
<ul>
<li><p>HDFS上同一份文件会有多份拷贝（默认是3份）</p>
</li>
<li><p>MapReduce调度原则</p>
<ul>
<li>在包含副本的节点上启动Map Task任务</li>
<li>或者在就近的节点上启动Map Task任务</li>
</ul>
</li>
<li><p>因此数据本地性有三个级别</p>
<ul>
<li><p>Node Local</p>
<ul>
<li><p>Map Task和数据在同一个节点上</p>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211128192223.png"></p>
</li>
</ul>
</li>
<li><p>Rack Local</p>
<ul>
<li><p>Map Task和数据在同一个机架上</p>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211128192307.png"></p>
</li>
</ul>
</li>
<li><p>Different Rack</p>
<ul>
<li><p>Map Task和数据即不再同一个节点又不在同一个机架上</p>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211128192405.png"></p>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h1 id="案例介绍"><a href="#案例介绍" class="headerlink" title="案例介绍"></a>案例介绍</h1><h2 id="用户行为分析-PV-UV计算"><a href="#用户行为分析-PV-UV计算" class="headerlink" title="用户行为分析-PV&#x2F;UV计算"></a>用户行为分析-PV&#x2F;UV计算</h2><p>计算思路？</p>
<p>IP、URL、REF_URL、Cookie、Timestamp</p>
<h2 id="用户行为分析-PV-计算"><a href="#用户行为分析-PV-计算" class="headerlink" title="用户行为分析-PV 计算"></a>用户行为分析-PV 计算</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">class</span> <span class="title class_">PVMapper</span> <span class="keyword">extends</span> <span class="title class_">Mapper</span>&lt;Object, Text, Text, IntWritable&gt; &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">static</span> <span class="type">IntWritable</span> <span class="variable">one</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">IntWritable</span>(<span class="number">1</span>);</span><br><span class="line">    <span class="keyword">private</span> <span class="type">Text</span> <span class="variable">word</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Text</span>();</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">map</span><span class="params">(Object key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line">        <span class="comment">// 使用tab分隔符对nginx的日志进行切分</span></span><br><span class="line">        String[] rawLogFields = value.toString().split(<span class="string">&quot;\t&quot;</span>);</span><br><span class="line">        <span class="comment">// 拿到访问的url： 例如 http://opencart.gp-bd.com/index.php?route=product/product/review&amp;product_id=41</span></span><br><span class="line">        <span class="type">String</span> <span class="variable">accessURL</span> <span class="operator">=</span> rawLogFields[<span class="number">1</span>];</span><br><span class="line">        <span class="keyword">if</span> (StringUtils.isNotEmpty(accessURL) &amp;&amp; accessURL.contains(<span class="string">&quot;opencart.com&quot;</span>)) &#123;</span><br><span class="line">            <span class="comment">// 使用UrlEncoded进行解析product_id</span></span><br><span class="line">            MultiMap&lt;String&gt; values = <span class="keyword">new</span> <span class="title class_">MultiMap</span>&lt;String&gt;();</span><br><span class="line">            UrlEncoded.decodeTo(accessURL, values, <span class="string">&quot;UTF-8&quot;</span>);</span><br><span class="line">            <span class="type">String</span> <span class="variable">productId</span> <span class="operator">=</span> values.getValue(<span class="string">&quot;product_id&quot;</span>, <span class="number">0</span>);</span><br><span class="line">            <span class="comment">// 如果有product_id这个参数，说明访问了这个页面</span></span><br><span class="line">            <span class="keyword">if</span> (StringUtils.isNumeric(productId)) &#123;</span><br><span class="line">                <span class="comment">// key为product_id，value为访问次数1次</span></span><br><span class="line">                word.set(productId);</span><br><span class="line">                context.write(word, one);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">class</span> <span class="title class_">PVReducer</span> <span class="keyword">extends</span> <span class="title class_">Reducer</span>&lt;Text, IntWritable, Text, IntWritable&gt; &#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="type">IntWritable</span> <span class="variable">result</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">IntWritable</span>();</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">reduce</span><span class="params">(Text key, Iterable&lt;IntWritable&gt; values,</span></span><br><span class="line"><span class="params">                       Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line">        <span class="comment">// 按照WordCount思路对相同key的value进行累加即可获得页面点击次数</span></span><br><span class="line">        <span class="type">int</span> <span class="variable">sum</span> <span class="operator">=</span> <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span> (IntWritable val : values) &#123;</span><br><span class="line">            sum += val.get();</span><br><span class="line">        &#125;</span><br><span class="line">        result.set(sum);</span><br><span class="line">        context.write(key, result);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">    <span class="type">Configuration</span> <span class="variable">conf</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Configuration</span>();</span><br><span class="line">    <span class="type">Job</span> <span class="variable">job</span> <span class="operator">=</span> Job.getInstance(conf, <span class="string">&quot;opencart user action pv count&quot;</span>);</span><br><span class="line">    job.setJarByClass(PVCount.class);</span><br><span class="line">    job.setMapperClass(PVMapper.class);</span><br><span class="line">    job.setCombinerClass(PVReducer.class);</span><br><span class="line">    job.setReducerClass(PVReducer.class);</span><br><span class="line">    job.setOutputKeyClass(Text.class);</span><br><span class="line">    job.setOutputValueClass(IntWritable.class);</span><br><span class="line">    FileInputFormat.addInputPath(job, <span class="keyword">new</span> <span class="title class_">Path</span>(args[<span class="number">0</span>]));</span><br><span class="line">    FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> <span class="title class_">Path</span>(args[<span class="number">1</span>]));</span><br><span class="line">    System.exit(job.waitForCompletion(<span class="literal">true</span>) ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h2 id="用户行为分析-UV-计算"><a href="#用户行为分析-UV-计算" class="headerlink" title="用户行为分析-UV 计算"></a>用户行为分析-UV 计算</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * 第一个Job的Mapper</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">class</span> <span class="title class_">UVDistinctMapper</span> <span class="keyword">extends</span> <span class="title class_">Mapper</span>&lt;Object, Text, Text, NullWritable&gt; &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="type">Text</span> <span class="variable">word</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Text</span>();</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">map</span><span class="params">(Object key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line">        <span class="comment">// 按照tab切分每一行日志</span></span><br><span class="line">        String[] rawLogFields = value.toString().split(<span class="string">&quot;\t&quot;</span>);</span><br><span class="line">        <span class="comment">// 获得productId</span></span><br><span class="line">        <span class="type">String</span> <span class="variable">accessURL</span> <span class="operator">=</span> rawLogFields[<span class="number">1</span>];</span><br><span class="line">        <span class="keyword">if</span> (StringUtils.isNotEmpty(accessURL) &amp;&amp; accessURL.contains(<span class="string">&quot;opencart.com&quot;</span>)) &#123;</span><br><span class="line">            MultiMap&lt;String&gt; values = <span class="keyword">new</span> <span class="title class_">MultiMap</span>&lt;String&gt;();</span><br><span class="line">            UrlEncoded.decodeTo(accessURL, values, <span class="string">&quot;UTF-8&quot;</span>);</span><br><span class="line">            <span class="type">String</span> <span class="variable">productId</span> <span class="operator">=</span> values.getValue(<span class="string">&quot;product_id&quot;</span>, <span class="number">0</span>);</span><br><span class="line">            <span class="comment">// 获得userId</span></span><br><span class="line">            <span class="type">String</span> <span class="variable">cookie</span> <span class="operator">=</span> rawLogFields[<span class="number">3</span>];</span><br><span class="line">            <span class="type">String</span> <span class="variable">userId</span> <span class="operator">=</span> cookie.contains(<span class="string">&quot;uid=&quot;</span>) ? cookie.split(<span class="string">&quot;=&quot;</span>)[<span class="number">1</span>] : <span class="literal">null</span>;</span><br><span class="line">            <span class="keyword">if</span> (StringUtils.isNumeric(productId) &amp;&amp; StringUtils.isNotEmpty(userId)) &#123;</span><br><span class="line">                <span class="comment">// 使用productId和userId作为主键，value设置为null</span></span><br><span class="line">                word.set(productId + <span class="string">&quot;\t&quot;</span> + userId);</span><br><span class="line">                context.write(word, NullWritable.get());</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * 第一个Job的Reducer</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">class</span> <span class="title class_">UVDistinctReducer</span> <span class="keyword">extends</span> <span class="title class_">Reducer</span>&lt;Text, NullWritable, Text, NullWritable&gt; &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">reduce</span><span class="params">(Text key, Iterable&lt;NullWritable&gt; values,</span></span><br><span class="line"><span class="params">                       Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line">        <span class="comment">// 直接输出结果，MapReduce框架已经帮助我们对key去重了</span></span><br><span class="line">        context.write(key, NullWritable.get());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * 第二个Job的Mapper</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">class</span> <span class="title class_">UVCountMapper</span> <span class="keyword">extends</span> <span class="title class_">Mapper</span>&lt;Object, Text, Text, IntWritable&gt; &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">static</span> <span class="type">IntWritable</span> <span class="variable">one</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">IntWritable</span>(<span class="number">1</span>);</span><br><span class="line">    <span class="keyword">private</span> <span class="type">Text</span> <span class="variable">word</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Text</span>();</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">map</span><span class="params">(Object key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line">        <span class="comment">// 使用tab作为分隔符，切分第一个Job去重后的key：product_Id \t userId</span></span><br><span class="line">        String[] dataFields = value.toString().split(<span class="string">&quot;\t&quot;</span>);</span><br><span class="line">        <span class="comment">// 只需要取出productId即可，因为我们统计的是产品的pv</span></span><br><span class="line">        word.set(dataFields[<span class="number">0</span>]);</span><br><span class="line">        <span class="comment">// 输出kv对，key为productId，value为1</span></span><br><span class="line">        context.write(word, one);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * 第二个Job的Reducer</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">class</span> <span class="title class_">UVCountReducer</span> <span class="keyword">extends</span> <span class="title class_">Reducer</span>&lt;Text, IntWritable, Text, IntWritable&gt; &#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="type">IntWritable</span> <span class="variable">result</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">IntWritable</span>();</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">reduce</span><span class="params">(Text key, Iterable&lt;IntWritable&gt; values,</span></span><br><span class="line"><span class="params">                       Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line">        <span class="comment">// 按照WordCount思路进行累加处理</span></span><br><span class="line">        <span class="type">int</span> <span class="variable">sum</span> <span class="operator">=</span> <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span> (IntWritable val : values) &#123;</span><br><span class="line">            sum += val.get();</span><br><span class="line">        &#125;</span><br><span class="line">        result.set(sum);</span><br><span class="line">        context.write(key, result);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * main方法，需要传入三个参数，第一个job的输入路径，第一个job的输出路径，第二个job的输出路径</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">    <span class="type">Configuration</span> <span class="variable">conf</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Configuration</span>();</span><br><span class="line">    <span class="comment">// 配置Job1</span></span><br><span class="line">    <span class="type">Job</span> <span class="variable">job1</span> <span class="operator">=</span> Job.getInstance(conf, <span class="string">&quot;opencart uv phase1 distinct&quot;</span>);</span><br><span class="line">    job1.setJarByClass(UVCount.class);</span><br><span class="line">    job1.setMapperClass(UVDistinctMapper.class);</span><br><span class="line">    job1.setReducerClass(UVDistinctReducer.class);</span><br><span class="line">    job1.setOutputKeyClass(Text.class);</span><br><span class="line">    job1.setOutputValueClass(NullWritable.class);</span><br><span class="line">    FileInputFormat.addInputPath(job1, <span class="keyword">new</span> <span class="title class_">Path</span>(args[<span class="number">0</span>]));</span><br><span class="line">    FileOutputFormat.setOutputPath(job1, <span class="keyword">new</span> <span class="title class_">Path</span>(args[<span class="number">1</span>]));</span><br><span class="line">    job1.waitForCompletion(<span class="literal">true</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 配置Job2，Job2的输入数据是Job1的输出</span></span><br><span class="line">    <span class="type">Job</span> <span class="variable">job2</span> <span class="operator">=</span> Job.getInstance(conf, <span class="string">&quot;opencart uv phase2 count&quot;</span>);</span><br><span class="line">    job2.setJarByClass(UVCount.class);</span><br><span class="line">    job2.setMapperClass(UVCountMapper.class);</span><br><span class="line">    job2.setReducerClass(UVCountReducer.class);</span><br><span class="line">    job2.setOutputKeyClass(Text.class);</span><br><span class="line">    job2.setOutputValueClass(IntWritable.class);</span><br><span class="line">    FileInputFormat.addInputPath(job2, <span class="keyword">new</span> <span class="title class_">Path</span>(args[<span class="number">1</span>]));</span><br><span class="line">    FileOutputFormat.setOutputPath(job2, <span class="keyword">new</span> <span class="title class_">Path</span>(args[<span class="number">2</span>]));</span><br><span class="line">    job2.waitForCompletion(<span class="literal">true</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="PV-UV-执行"><a href="#PV-UV-执行" class="headerlink" title="PV&#x2F;UV 执行"></a>PV&#x2F;UV 执行</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -mkdir -p /user/cdh/pv/input</span><br><span class="line">hadoop fs -put nginx_sample.txt /user/cdh/wordcount/input</span><br><span class="line">hadoop jar demo_mapreducer-1.0-SNAPSHOT-jar-with-dependencies.jar com.study.sample.mapreducer.PVCount /user/cdh/pv/input /user/cdh/pv/output</span><br><span class="line"></span><br><span class="line">hadoop jar demo_mapreducer-1.0-SNAPSHOT-jar-with-dependencies.jar com.study.sample.mapreducer.UVCount /user/cdh/pv/input /user/cdh/uv/output-1 /user/cdh/uv/output-2</span><br></pre></td></tr></table></figure>

<h1 id="调优参数"><a href="#调优参数" class="headerlink" title="调优参数"></a>调优参数</h1><h2 id="Map-Task和Reduce-Task数目调整"><a href="#Map-Task和Reduce-Task数目调整" class="headerlink" title="Map Task和Reduce Task数目调整"></a>Map Task和Reduce Task数目调整</h2><p><strong>Map Task数目</strong></p>
<ul>
<li><p>Map读取文件时，通过InputFormat计算分割文件</p>
</li>
<li><p>split大小由以下三个参数决定</p>
<ul>
<li><p>dfs.blocksize HDFS Block大小</p>
</li>
<li><p>mapreduce.input.fileinputformat.split.minsize 划分最小字节数</p>
</li>
<li><p>mapreduce.input.fileinputformat.split.maxsize 划分最大字节数</p>
</li>
<li><p>计算公式</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">protected</span> <span class="type">long</span> <span class="title function_">computeSplitSize</span><span class="params">(<span class="type">long</span> blockSize, <span class="type">long</span> minSize, <span class="type">long</span> maxSize)</span> &#123;</span><br><span class="line">	<span class="keyword">return</span> Math.max(minSize, Math.min(maxSize, blockSize));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ul>
<p><strong>Reduce Task数目</strong></p>
<ul>
<li>默认每个作业Reduce Task数目可以通过mapreduce.job.reduces控制</li>
<li>在每个作业中也可以通过Job.setNumReduceTasks(Int number)进行控制</li>
</ul>
<h2 id="容错参数调整"><a href="#容错参数调整" class="headerlink" title="容错参数调整"></a>容错参数调整</h2><table>
<thead>
<tr>
<th>配置值</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>mapreduce.map.maxattempts</td>
<td>Map最大尝试次数，默认是4</td>
</tr>
<tr>
<td>mapreduce.reduce.maxattempts</td>
<td>Reduce最大尝试次数，默认是4</td>
</tr>
<tr>
<td>mapreduce.am.max-attempts</td>
<td>MRApplication最大尝试次数，默认是2</td>
</tr>
</tbody></table>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211128232306.png"></p>
<h2 id="推测执行参数调整"><a href="#推测执行参数调整" class="headerlink" title="推测执行参数调整"></a>推测执行参数调整</h2><table>
<thead>
<tr>
<th>配置值</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>mapreduce.map.speculative</td>
<td>是否对Map开启推测执行机制，默认为false</td>
</tr>
<tr>
<td>mapreduce.reduce.speculative</td>
<td>是否对Reduce开启推测执行机制，默认为false</td>
</tr>
<tr>
<td>mapreduce.job.speculative.speculative-cap-running-tasks</td>
<td>能够推测重跑正在运行任务的百分之几，默认是0.1</td>
</tr>
<tr>
<td>mapreduce.job.speculative.speculative-cap-total-tasks</td>
<td>能够推测重跑全部任务的百分之几，默认是0.01</td>
</tr>
</tbody></table>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211128232618.png"></p>
<h2 id="内存参数调整"><a href="#内存参数调整" class="headerlink" title="内存参数调整"></a>内存参数调整</h2><table>
<thead>
<tr>
<th>配置值</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>mapreduce.map.memory.mb</td>
<td>控制分配给Map内存上限，如果超过会被kill</td>
</tr>
<tr>
<td>mapreduce.map.java.opts</td>
<td>控制Map堆内存大小</td>
</tr>
<tr>
<td>mapreduce.reduce.memory.mb</td>
<td>控制分配给Reduce内存上限，如果超过会被kill</td>
</tr>
<tr>
<td>mapreduce.reduce.java.opts</td>
<td>控制Reduce堆内存大小</td>
</tr>
<tr>
<td>yarn.app.mapreduce.am.resource.mb</td>
<td>控制分配给MRApplicationMaster内存上限</td>
</tr>
<tr>
<td>yarn.app.mapreduce.am.command-opts</td>
<td>控制MRApplicationMaster堆内存大小</td>
</tr>
<tr>
<td>mapreduce.job.heap.memory-mb.ratio</td>
<td>Heap大小占Container大小的比例，默认0.8</td>
</tr>
<tr>
<td>yarn.nodemanager.vmem-pmem-ratio</td>
<td>物理内存和虚拟内存比率，默认是2.1</td>
</tr>
</tbody></table>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211128232940.png"></p>
<h1 id="手动操作与思考"><a href="#手动操作与思考" class="headerlink" title="手动操作与思考"></a>手动操作与思考</h1><ol>
<li>Word Count Example运行</li>
<li>读MapReduce TextInputFormat源代码</li>
<li>实现PV和UV计算<ul>
<li>使用maven构建项目</li>
<li>实战了解Mapper、Reducer</li>
<li>打包提交CDH集群运行并输出结果</li>
</ul>
</li>
<li>实现Top K（例如：最热门的N个商品）<ul>
<li><p>思路</p>
<p>分2个mapreduce任务，第1个对数据中的商品进行频次统计，第2个任务对第1个任务的结果排序取前K个，关键在于第2个任务 reducer 只能有1个</p>
</li>
<li><p>完整的工程</p>
<p>com.study.sample.mapreducer.TopK</p>
</li>
<li><p>执行</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -mkdir -p /user/cdh/topK/input</span><br><span class="line">hadoop fs -put topK_sample.txt /user/cdh/topK/input</span><br><span class="line">hadoop jar demo_mapreducer-1.0-SNAPSHOT-jar-with-dependencies.jar com.study.sample.mapreducer.TopK /user/cdh/topK/input /user/cdh/topK/output-1 /user/cdh/topK/output-2</span><br><span class="line"></span><br><span class="line">hadoop fs -ls /user/cdh/topK/output-1/</span><br><span class="line">hadoop fs -text /user/cdh/topK/output-1/part-r-*</span><br><span class="line"></span><br><span class="line">hadoop fs -rm -R /user/cdh/topK/output-1</span><br><span class="line">hadoop fs -rm -R /user/cdh/topK/output-2</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>思考如何解决数据倾斜问题？</li>
<li>如何使用MR实现矩阵相乘？</li>
</ol>
<h2 id="实现Top-K（例如：最热门的N个商品）"><a href="#实现Top-K（例如：最热门的N个商品）" class="headerlink" title="实现Top K（例如：最热门的N个商品）"></a>实现Top K（例如：最热门的N个商品）</h2><ol>
<li>先数据进行清洗，得到每个商品频次</li>
<li>然后对商品频次读取，需要注意2个地方，第1个自定义对象实现 WritableComparable 接口实现 compareTo 排序接口，第2个只存在一个 reducer 进行输出，每输出一个计数器加1，直到不小于 k 完成 Top K。</li>
</ol>
<h2 id="MR实现矩阵相乘"><a href="#MR实现矩阵相乘" class="headerlink" title="MR实现矩阵相乘"></a>MR实现矩阵相乘</h2><h3 id="矩阵乘法的计算"><a href="#矩阵乘法的计算" class="headerlink" title="矩阵乘法的计算"></a>矩阵乘法的计算</h3><p>矩阵，是线性代数中的基本概念之一。一个m×n的矩阵就是m×n个数排成m行n列的一个数阵。</p>
<p>矩阵乘法是一种高效的算法可以把一些一维递推优化到log(n)，还可以求路径方案等，所以更是一种应用性极强的算法。必须注意的是，只有当矩阵A的列数与矩阵B的行数相等时A×B才有意义。</p>
<p>一般单说矩阵乘积时，指的便是一般矩阵乘积。若A为m×n矩阵，B为n×p矩阵，则他们的乘积AB(有时记做A·B）会是一个m×p矩阵。其乘积矩阵的元素如下面式子得出：</p>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211205201605.png"></p>
<p>上面是一个通过代数公式的方式说明这类乘法的抽象性质，有些抽象，下面从一个具体图的角度看看这种乘法：</p>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211205203034.png"></p>
<p>在上图中，A是个4×2矩阵，B是个2×3矩阵。分别计算AB的（1,2）和（3,3）元素的值，结果可以根据上图中箭头方向两两配对，把每一对中的两个元素相乘，再把这些乘积加起来，最后得到的值即为箭头相交位置的值。</p>
<p>这样在从直观的图中转换为稍微抽象点的公式中，则对应的计算方式为：</p>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211205204416.png"></p>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211205204500.png"></p>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211205204639.png"></p>
<p>(AB)1,1 &#x3D; A1,1 * B1,1 + A1,2 * B2,1 &#x3D; 1 * 0 + 1 * 1 &#x3D; 1</p>
<p>(AB)1,3 &#x3D; A1,1 * B3,1 + A1,2 * B3,2 &#x3D; 1 * 3 + 1 * 2 &#x3D; 5</p>
<p>(AB)2,3 &#x3D; A2,1 * B3,1 + A2,2 * B3,2 &#x3D; 2 * 3 + 0 * 2 &#x3D; 6</p>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211205204659.png"></p>
<h3 id="矩阵乘法的来源"><a href="#矩阵乘法的来源" class="headerlink" title="矩阵乘法的来源"></a>矩阵乘法的来源</h3><p>在矩阵的运算中，矩阵的加法、数与矩阵的积，都与实数或向量的对应运算一致，易于接受掌握。唯独矩阵乘法，与之相差悬殊。初学时感觉莫名其妙，难以接受。扬天哀呼，为啥这么算呢？</p>
<p>来举个简单的例子：</p>
<p>设A1,A2,…,Am是m个工厂，它们都生产着n种产品B1,B2,…,Bm，而Ai厂生产Bj的年产量为aij,i&#x3D;1,2…,m;j&#x3D;1,2,…,n。于是，对照每个工厂各种产品年产量的统计表和产量矩阵就出来了：</p>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211205211022.png"></p>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211205211048.png"></p>
<p>如果第二年各厂各种产品的产量都是前一年的λ倍,就是数乘矩阵λA的意义。如果计算各厂各种产品两年的总产量，就用到了矩阵的加法。</p>
<p>接上例，设产品B1,B2,…,Bn皆需p种原料C1,C2,…,Cp，而生存一件Bk所需原料Cj的数量为bkj，于是，统计各种产品每件所需的原料数表和单间原料矩阵为：</p>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211205211440.png"></p>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211205211511.png"></p>
<p>现在需要计算各厂每年所需各种原料的总是，设Ai厂一年所需原料Cj的总数为cij，则各厂一年所需各种原料总数统计表和原料总数矩阵为：</p>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211205211642.png"></p>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211205211701.png"></p>
<p>到这一步，基本上可以想到，cij（i厂需材料j的原料数量）等于i厂各个产品的年产量乘上该产品所需j原料的数量的和，简单点说就是一年所需总料数&#x3D;年产量×单间所需原料数，用公式表达就是：</p>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211205211742.png"></p>
<p>从上面的例子可以看出，关于矩阵的乘法，并非空穴来风、无源之水，而是有它必然产生的缘由。充分说明了数学是来源于生活，之所以与生活相差较大，只是因为在语言、符号演化过程中，数学进化的方向是趋向于抽象和一般。</p>
<h3 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h3><p>矩阵乘法要求左矩阵的列数与右矩阵的行数相等。m×n的矩阵A，与n×p的矩阵B相乘，结果为m×p的矩阵C</p>
<ul>
<li>矩阵A的行数为m，列数为n，aij为矩阵A第i行j列的元素。</li>
<li>矩阵B的行数为n。列数为p。bij为矩阵B第i行j列的元素。</li>
</ul>
<p>由于分布式计算的特点，须要找到相互独立的计算过程，以便能够在不同的节点上进行计算而不会彼此影响。依据矩阵乘法的公式，C中各个元素的计算都是相互独立的，即各个cij在计算过程中彼此不影响。这种话，在Map阶段能够把计算所须要的元素都集中到同一个key中，然后，在Reduce阶段就能够从中解析出各个元素来计算cij。</p>
<p>另外，以a11为例，它将会在c11、c12……c1p的计算中使用。也就是说。在Map阶段，当我们从HDFS取出一行记录时，假设该记录是A的元素。则须要存储成p个&lt;key, value&gt;对。而且这p个key互不同样。假设该记录是B的元素，则须要存储成m个&lt;key, value&gt;对，同样的，m个key也应互不同样；但同一时候。用于存放计算cij的ai1、ai2……ain和b1j、b2j……bnj的&lt;key, value&gt;对的key应该都是同样的，这样才能被传递到同一个Reduce中。</p>
<h3 id="设计"><a href="#设计" class="headerlink" title="设计"></a>设计</h3><p>普遍有一个共识是：数据结构+算法&#x3D;程序，所以在编写代码之前须要先理清数据存储结构和处理数据的算法。</p>
<h4 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h4><h5 id="map阶段"><a href="#map阶段" class="headerlink" title="map阶段"></a>map阶段</h5><p>在map阶段。须要做的是进行数据准备。把来自矩阵A的元素aij，标识成p条&lt;key, value&gt;的形式，key&#x3D;”i,k”,（当中k&#x3D;1,2,…,p）。value&#x3D;”a:j,aij”；把来自矩阵B的元素bij，标识成m条&lt;key, value&gt;形式，key&#x3D;”k,j”（当中k&#x3D;1,2,…,m），value&#x3D;”b:i,bij”。</p>
<p>经过处理，用于计算cij须要的a、b就转变为有同样key（”i,j”）的数据对，通过value中”a:”、”b:”能区分元素是来自矩阵A还是矩阵B。以及详细的位置（在矩阵A的第几列。在矩阵B的第几行）。</p>
<h5 id="shuffle阶段"><a href="#shuffle阶段" class="headerlink" title="shuffle阶段"></a>shuffle阶段</h5><p>这个阶段是Hadoop自己主动完毕的阶段，具有同样key的value被分到同一个Iterable中，形成&lt;key,Iterable(value)&gt;对，再传递给reduce。</p>
<h5 id="reduce阶段"><a href="#reduce阶段" class="headerlink" title="reduce阶段"></a>reduce阶段</h5><p>通过map数据预处理和shuffle数据分组两个阶段，reduce阶段仅仅须要知道两件事即可：</p>
<ul>
<li>&lt;key,Iterable(value)&gt;对经过计算得到的是矩阵C的哪个元素？由于map阶段对数据的处理。key（i,j）中的数据对。就是其在矩阵C中的位置，第i行j列。</li>
<li>Iterable中的每一个value来自于矩阵A和矩阵B的哪个位置？这个也在map阶段进行了标记。对于value（x:y,z），仅仅须要找到y同样的来自不同矩阵（即x分别为a和b）的两个元素，取z相乘，然后加和就可以。</li>
</ul>
<h4 id="数据结构"><a href="#数据结构" class="headerlink" title="数据结构"></a>数据结构</h4><p>计算过程已经设计清楚了，就须要对数据结构进行设计。大体有两种设计方案：</p>
<p>第一种：使用最原始的表示方式，同样行内不同列数据通过”,”切割。不同行通过换行切割。</p>
<p>第二种：通过行列表示法，即文件里的每行数据有三个元素通过分隔符切割，第一个元素表示行，第二个元素表示列，第三个元素表示数据。这样的方式对于能够不列出为0的元素，即能够降低稀疏矩阵的数据量。</p>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211207132715.png"></p>
<p>在上图中，第一种方式存储的数据量小于另外一种，但这仅仅是由于样例中的数据设计成这样。在现实中，使用分布式计算矩阵乘法的环境中，大部分矩阵是稀疏矩阵。且数据量极大，在这样的情况下，另外一种数据结构的优势就显现了出来。并且，由于使用分布式计算，假设数据大于64m，在map阶段将不可以逐行处理，将不能确定数据来自于哪一行。只是，由于现实中对于大矩阵的乘法，考虑到存储空间和内存的情况，须要特殊的处理方式，有一种是将矩阵进行行列转换然后计算。这个时候第一种还是挺有用的。</p>
<p>第一种优点：存储空间相对第二种占用小</p>
<p>第一种缺点：数据大于64m在map阶段不能逐行处理，不能确定数据来自哪一行</p>
<p>第二种优点：计算大的稀疏矩阵可以并行处理</p>
<p>第二种缺点：存储空间相对第一种占用大</p>
<h3 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.study.sample.mapreducer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FileSystem;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileSplit;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.partition.HashPartitioner;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> java.util.HashMap;</span><br><span class="line"><span class="keyword">import</span> java.util.Iterator;</span><br><span class="line"><span class="keyword">import</span> java.util.Map;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 矩阵相乘，A * B = C</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">MatrixMultiply</span> &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">String</span> <span class="variable">MATRIX_A_FILENAME</span> <span class="operator">=</span> <span class="string">&quot;MatrixA&quot;</span>;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">String</span> <span class="variable">MATRIX_B_FILENAME</span> <span class="operator">=</span> <span class="string">&quot;MatrixB&quot;</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">class</span> <span class="title class_">MatrixMapper</span> <span class="keyword">extends</span> <span class="title class_">Mapper</span>&lt;Object, Text, Text, Text&gt; &#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">/** 文件名称，用于区分矩阵 A 或 B*/</span></span><br><span class="line">        <span class="keyword">private</span> <span class="type">String</span> <span class="variable">filename</span> <span class="operator">=</span> <span class="literal">null</span>;</span><br><span class="line">        <span class="comment">/** 矩阵 A 的行数*/</span></span><br><span class="line">        <span class="keyword">private</span> <span class="type">int</span> <span class="variable">rowNum</span> <span class="operator">=</span> <span class="number">4</span>;</span><br><span class="line">        <span class="comment">/** 矩阵 B 的列数*/</span></span><br><span class="line">        <span class="keyword">private</span> <span class="type">int</span> <span class="variable">colNum</span> <span class="operator">=</span> <span class="number">2</span>;</span><br><span class="line">        <span class="comment">/** 矩阵 A 当前在第几行*/</span></span><br><span class="line">        <span class="keyword">private</span> <span class="type">int</span> <span class="variable">rowIndexA</span> <span class="operator">=</span> <span class="number">1</span>;</span><br><span class="line">        <span class="comment">/** 矩阵 B 当前在第几行*/</span></span><br><span class="line">        <span class="keyword">private</span> <span class="type">int</span> <span class="variable">rowIndexB</span> <span class="operator">=</span> <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">setup</span><span class="params">(Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line">            filename = ((FileSplit) context.getInputSplit()).getPath().getName();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">map</span><span class="params">(Object key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line">            <span class="keyword">final</span> String[] rowFields = value.toString().split(<span class="string">&quot;,&quot;</span>);</span><br><span class="line">            <span class="keyword">if</span> (MATRIX_A_FILENAME.equals(filename)) &#123;</span><br><span class="line">                <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">1</span>; i &lt;= colNum; i++) &#123;</span><br><span class="line">                    <span class="type">Text</span> <span class="variable">k</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Text</span>(rowIndexA + <span class="string">&quot;,&quot;</span> + i);</span><br><span class="line">                    <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">j</span> <span class="operator">=</span> <span class="number">0</span>; j &lt; rowFields.length; j++) &#123;</span><br><span class="line">                        <span class="type">Text</span> <span class="variable">v</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Text</span>(MATRIX_A_FILENAME + <span class="string">&quot;,&quot;</span> + (j + <span class="number">1</span>) + <span class="string">&quot;,&quot;</span> + rowFields[j]);</span><br><span class="line">                        context.write(k, v);</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">                rowIndexA++;</span><br><span class="line">            &#125; <span class="keyword">else</span> <span class="keyword">if</span> (MATRIX_B_FILENAME.equals(filename)) &#123;</span><br><span class="line">                <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">1</span>; i &lt;= rowNum; i++) &#123;</span><br><span class="line">                    <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">j</span> <span class="operator">=</span> <span class="number">0</span>; j &lt; rowFields.length; j++) &#123;</span><br><span class="line">                        <span class="type">Text</span> <span class="variable">k</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Text</span>(i + <span class="string">&quot;,&quot;</span> + (j + <span class="number">1</span>));</span><br><span class="line">                        <span class="type">Text</span> <span class="variable">v</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Text</span>(MATRIX_B_FILENAME + <span class="string">&quot;,&quot;</span> + rowIndexB + <span class="string">&quot;,&quot;</span> + rowFields[j]);</span><br><span class="line">                        context.write(k, v);</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">                rowIndexB++;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">class</span> <span class="title class_">MatrixReducer</span> <span class="keyword">extends</span> <span class="title class_">Reducer</span>&lt;Text, Text, Text, IntWritable&gt; &#123;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">reduce</span><span class="params">(Text key, Iterable&lt;Text&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line">            Map&lt;String, String&gt; mapA = <span class="keyword">new</span> <span class="title class_">HashMap</span>&lt;&gt;();</span><br><span class="line">            Map&lt;String, String&gt; mapB = <span class="keyword">new</span> <span class="title class_">HashMap</span>&lt;&gt;();</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> (Text value : values) &#123;</span><br><span class="line">                String[] val = value.toString().split(<span class="string">&quot;,&quot;</span>);</span><br><span class="line">                <span class="keyword">if</span> (MATRIX_A_FILENAME.equals(val[<span class="number">0</span>])) &#123;</span><br><span class="line">                    mapA.put(val[<span class="number">1</span>], val[<span class="number">2</span>]);</span><br><span class="line">                &#125; <span class="keyword">else</span> <span class="keyword">if</span> (MATRIX_B_FILENAME.equals(val[<span class="number">0</span>])) &#123;</span><br><span class="line">                    mapB.put(val[<span class="number">1</span>], val[<span class="number">2</span>]);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="type">int</span> <span class="variable">result</span> <span class="operator">=</span> <span class="number">0</span>;</span><br><span class="line">            <span class="keyword">final</span> Iterator&lt;String&gt; iterator = mapA.keySet().iterator();</span><br><span class="line">            <span class="keyword">while</span> (iterator.hasNext()) &#123;</span><br><span class="line">                <span class="keyword">final</span> <span class="type">String</span> <span class="variable">mapAKey</span> <span class="operator">=</span> iterator.next();</span><br><span class="line">                <span class="keyword">if</span> (mapB.get(mapAKey) == <span class="literal">null</span>) &#123;</span><br><span class="line">                    <span class="comment">// 由于 mapAKey 取的是 mapA 的 key 集合。所以仅仅须要推断 mapB 是否存在就可以。</span></span><br><span class="line">                    <span class="keyword">continue</span>;</span><br><span class="line">                &#125;</span><br><span class="line">                result += Integer.parseInt(mapA.get(mapAKey)) * Integer.parseInt(mapB.get(mapAKey));</span><br><span class="line">            &#125;</span><br><span class="line">            context.write(key, <span class="keyword">new</span> <span class="title class_">IntWritable</span>(result));</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">        <span class="type">Configuration</span> <span class="variable">conf</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Configuration</span>();</span><br><span class="line">        <span class="comment">// 配置Job</span></span><br><span class="line">        <span class="type">Job</span> <span class="variable">job</span> <span class="operator">=</span> Job.getInstance(conf, <span class="string">&quot;matrix multiply&quot;</span>);</span><br><span class="line">        job.setJarByClass(MatrixMultiply.class);</span><br><span class="line">        job.setMapperClass(MatrixMapper.class);</span><br><span class="line">        job.setMapOutputKeyClass(Text.class);</span><br><span class="line">        job.setMapOutputValueClass(Text.class);</span><br><span class="line">        job.setReducerClass(MatrixReducer.class);</span><br><span class="line">        job.setOutputKeyClass(Text.class);</span><br><span class="line">        job.setOutputValueClass(IntWritable.class);</span><br><span class="line">        <span class="comment">// 添加输入路径和输出路径</span></span><br><span class="line">        FileInputFormat.addInputPath(job, <span class="keyword">new</span> <span class="title class_">Path</span>(args[<span class="number">0</span>]));</span><br><span class="line">        FileInputFormat.addInputPath(job, <span class="keyword">new</span> <span class="title class_">Path</span>(args[<span class="number">1</span>]));</span><br><span class="line">        <span class="keyword">final</span> <span class="type">Path</span> <span class="variable">outputDir</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Path</span>(args[<span class="number">2</span>]);</span><br><span class="line">        <span class="type">FileSystem</span> <span class="variable">fs</span> <span class="operator">=</span> FileSystem.get(conf);</span><br><span class="line">        <span class="keyword">if</span> (fs.exists(outputDir)) &#123;</span><br><span class="line">            fs.delete(outputDir, <span class="literal">true</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        FileOutputFormat.setOutputPath(job, outputDir);</span><br><span class="line">        job.setPartitionerClass(HashPartitioner.class);</span><br><span class="line">        <span class="comment">// 向集群提交Job，并等待完成</span></span><br><span class="line">        System.exit(job.waitForCompletion(<span class="literal">true</span>) ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">package</span> com.study.sample.mapreducer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FileSystem;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileSplit;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.partition.HashPartitioner;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> java.util.HashMap;</span><br><span class="line"><span class="keyword">import</span> java.util.Iterator;</span><br><span class="line"><span class="keyword">import</span> java.util.Map;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 稀疏矩阵相乘</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">SparseMatrixMultiply</span> &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">String</span> <span class="variable">MATRIX_A_FILENAME</span> <span class="operator">=</span> <span class="string">&quot;SparseMatrixA&quot;</span>;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">String</span> <span class="variable">MATRIX_B_FILENAME</span> <span class="operator">=</span> <span class="string">&quot;SparseMatrixB&quot;</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">class</span> <span class="title class_">SparseMatrixMapper</span> <span class="keyword">extends</span> <span class="title class_">Mapper</span>&lt;Object, Text, Text, Text&gt; &#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">/** 文件名称，用于区分矩阵 A 或 B*/</span></span><br><span class="line">        <span class="keyword">private</span> <span class="type">String</span> <span class="variable">filename</span> <span class="operator">=</span> <span class="literal">null</span>;</span><br><span class="line">        <span class="comment">/** 矩阵 A 的行数*/</span></span><br><span class="line">        <span class="keyword">private</span> <span class="type">int</span> <span class="variable">rowNum</span> <span class="operator">=</span> <span class="number">4</span>;</span><br><span class="line">        <span class="comment">/** 矩阵 B 的列数*/</span></span><br><span class="line">        <span class="keyword">private</span> <span class="type">int</span> <span class="variable">colNum</span> <span class="operator">=</span> <span class="number">2</span>;</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">setup</span><span class="params">(Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line">            filename = ((FileSplit) context.getInputSplit()).getPath().getName();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">map</span><span class="params">(Object key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line">            <span class="keyword">final</span> String[] rowFields = value.toString().split(<span class="string">&quot;,&quot;</span>);</span><br><span class="line">            <span class="keyword">if</span> (MATRIX_A_FILENAME.equals(filename)) &#123;</span><br><span class="line">                <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">1</span>; i &lt;= colNum; i++) &#123;</span><br><span class="line">                    <span class="type">Text</span> <span class="variable">k</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Text</span>(rowFields[<span class="number">0</span>] + <span class="string">&quot;,&quot;</span> + i);</span><br><span class="line">                    <span class="type">Text</span> <span class="variable">v</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Text</span>(MATRIX_A_FILENAME + <span class="string">&quot;,&quot;</span> + rowFields[<span class="number">1</span>] + <span class="string">&quot;,&quot;</span> + rowFields[<span class="number">2</span>]);</span><br><span class="line">                    context.write(k, v);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125; <span class="keyword">else</span> <span class="keyword">if</span> (MATRIX_B_FILENAME.equals(filename)) &#123;</span><br><span class="line">                <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">1</span>; i &lt;= rowNum; i++) &#123;</span><br><span class="line">                    <span class="type">Text</span> <span class="variable">k</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Text</span>(i + <span class="string">&quot;,&quot;</span> + rowFields[<span class="number">1</span>]);</span><br><span class="line">                    <span class="type">Text</span> <span class="variable">v</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Text</span>(MATRIX_B_FILENAME + <span class="string">&quot;,&quot;</span> + rowFields[<span class="number">0</span>] + <span class="string">&quot;,&quot;</span> + rowFields[<span class="number">2</span>]);</span><br><span class="line">                    context.write(k, v);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">class</span> <span class="title class_">SparseMatrixReducer</span> <span class="keyword">extends</span> <span class="title class_">Reducer</span>&lt;Text, Text, Text, IntWritable&gt; &#123;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">reduce</span><span class="params">(Text key, Iterable&lt;Text&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line">            Map&lt;String, String&gt; mapA = <span class="keyword">new</span> <span class="title class_">HashMap</span>&lt;&gt;();</span><br><span class="line">            Map&lt;String, String&gt; mapB = <span class="keyword">new</span> <span class="title class_">HashMap</span>&lt;&gt;();</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> (Text value : values) &#123;</span><br><span class="line">                String[] val = value.toString().split(<span class="string">&quot;,&quot;</span>);</span><br><span class="line">                <span class="keyword">if</span> (MATRIX_A_FILENAME.equals(val[<span class="number">0</span>])) &#123;</span><br><span class="line">                    mapA.put(val[<span class="number">1</span>], val[<span class="number">2</span>]);</span><br><span class="line">                &#125; <span class="keyword">else</span> <span class="keyword">if</span> (MATRIX_B_FILENAME.equals(val[<span class="number">0</span>])) &#123;</span><br><span class="line">                    mapB.put(val[<span class="number">1</span>], val[<span class="number">2</span>]);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="type">int</span> <span class="variable">result</span> <span class="operator">=</span> <span class="number">0</span>;</span><br><span class="line">            <span class="keyword">final</span> Iterator&lt;String&gt; iterator = mapA.keySet().iterator();</span><br><span class="line">            <span class="keyword">while</span> (iterator.hasNext()) &#123;</span><br><span class="line">                <span class="keyword">final</span> <span class="type">String</span> <span class="variable">mapAKey</span> <span class="operator">=</span> iterator.next();</span><br><span class="line">                <span class="keyword">if</span> (mapB.get(mapAKey) == <span class="literal">null</span>) &#123;</span><br><span class="line">                    <span class="comment">// 由于 mapAKey 取的是 mapA 的 key 集合。所以仅仅须要推断 mapB 是否存在就可以。</span></span><br><span class="line">                    <span class="keyword">continue</span>;</span><br><span class="line">                &#125;</span><br><span class="line">                result += Integer.parseInt(mapA.get(mapAKey)) * Integer.parseInt(mapB.get(mapAKey));</span><br><span class="line">            &#125;</span><br><span class="line">            context.write(key, <span class="keyword">new</span> <span class="title class_">IntWritable</span>(result));</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">        <span class="type">Configuration</span> <span class="variable">conf</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Configuration</span>();</span><br><span class="line">        <span class="comment">// 配置Job</span></span><br><span class="line">        <span class="type">Job</span> <span class="variable">job</span> <span class="operator">=</span> Job.getInstance(conf, <span class="string">&quot;sparse matrix multiply&quot;</span>);</span><br><span class="line">        job.setJarByClass(SparseMatrixMultiply.class);</span><br><span class="line">        job.setMapperClass(SparseMatrixMapper.class);</span><br><span class="line">        job.setMapOutputKeyClass(Text.class);</span><br><span class="line">        job.setMapOutputValueClass(Text.class);</span><br><span class="line">        job.setReducerClass(SparseMatrixReducer.class);</span><br><span class="line">        job.setOutputKeyClass(Text.class);</span><br><span class="line">        job.setOutputValueClass(IntWritable.class);</span><br><span class="line">        <span class="comment">// 添加输入路径和输出路径</span></span><br><span class="line">        FileInputFormat.addInputPath(job, <span class="keyword">new</span> <span class="title class_">Path</span>(args[<span class="number">0</span>]));</span><br><span class="line">        FileInputFormat.addInputPath(job, <span class="keyword">new</span> <span class="title class_">Path</span>(args[<span class="number">1</span>]));</span><br><span class="line">        <span class="keyword">final</span> <span class="type">Path</span> <span class="variable">outputDir</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Path</span>(args[<span class="number">2</span>]);</span><br><span class="line">        <span class="type">FileSystem</span> <span class="variable">fs</span> <span class="operator">=</span> FileSystem.get(conf);</span><br><span class="line">        <span class="keyword">if</span> (fs.exists(outputDir)) &#123;</span><br><span class="line">            fs.delete(outputDir, <span class="literal">true</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        FileOutputFormat.setOutputPath(job, outputDir);</span><br><span class="line">        job.setPartitionerClass(HashPartitioner.class);</span><br><span class="line">        <span class="comment">// 向集群提交Job，并等待完成</span></span><br><span class="line">        System.exit(job.waitForCompletion(<span class="literal">true</span>) ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="第一种数据结构"><a href="#第一种数据结构" class="headerlink" title="第一种数据结构"></a>第一种数据结构</h4><p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211207201017.png"></p>
<h4 id="第二种数据结构"><a href="#第二种数据结构" class="headerlink" title="第二种数据结构"></a>第二种数据结构</h4><p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211207204931.png"></p>
<h3 id="执行"><a href="#执行" class="headerlink" title="执行"></a>执行</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line">// 矩阵乘法执行</span><br><span class="line">hadoop fs -mkdir -p /user/cdh/matrix_multiply/input-1/</span><br><span class="line">hadoop fs -put MatrixA /user/cdh/matrix_multiply/input-1/</span><br><span class="line"></span><br><span class="line">hadoop fs -mkdir -p /user/cdh/matrix_multiply/input-2/</span><br><span class="line">hadoop fs -put MatrixB /user/cdh/matrix_multiply/input-2/</span><br><span class="line"></span><br><span class="line">hadoop fs -ls /user/cdh/matrix_multiply/input-1/</span><br><span class="line">hadoop fs -ls /user/cdh/matrix_multiply/input-2/</span><br><span class="line"></span><br><span class="line">hadoop jar demo_mapreducer-1.0-SNAPSHOT-jar-with-dependencies.jar com.study.sample.mapreducer.MatrixMultiply /user/cdh/matrix_multiply/input-1/ /user/cdh/matrix_multiply/input-2/ /user/cdh/matrix_multiply/output/</span><br><span class="line"></span><br><span class="line">hadoop fs -ls /user/cdh/matrix_multiply/output/</span><br><span class="line">hadoop fs -text /user/cdh/matrix_multiply/output/part-r-*</span><br><span class="line"></span><br><span class="line">1,2	46</span><br><span class="line">2,1	40</span><br><span class="line">3,2	202</span><br><span class="line">4,1	232</span><br><span class="line">1,1	43</span><br><span class="line">2,2	70</span><br><span class="line">3,1	169</span><br><span class="line">4,2	280</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">// 稀疏矩阵乘法执行</span><br><span class="line">hadoop fs -mkdir -p /user/cdh/sparse_matrix_multiply/input-1/</span><br><span class="line">hadoop fs -put SparseMatrixA /user/cdh/sparse_matrix_multiply/input-1/</span><br><span class="line"></span><br><span class="line">hadoop fs -mkdir -p /user/cdh/sparse_matrix_multiply/input-2/</span><br><span class="line">hadoop fs -put SparseMatrixB /user/cdh/sparse_matrix_multiply/input-2/</span><br><span class="line"></span><br><span class="line">hadoop fs -ls /user/cdh/sparse_matrix_multiply/input-1/</span><br><span class="line">hadoop fs -ls /user/cdh/sparse_matrix_multiply/input-2/</span><br><span class="line"></span><br><span class="line">hadoop jar demo_mapreducer-1.0-SNAPSHOT-jar-with-dependencies.jar com.study.sample.mapreducer.SparseMatrixMultiply /user/cdh/sparse_matrix_multiply/input-1/ /user/cdh/sparse_matrix_multiply/input-2/ /user/cdh/sparse_matrix_multiply/output/</span><br><span class="line"></span><br><span class="line">hadoop fs -ls /user/cdh/sparse_matrix_multiply/output/</span><br><span class="line">hadoop fs -text /user/cdh/sparse_matrix_multiply/output/part-r-*</span><br><span class="line"></span><br><span class="line">1,2	46</span><br><span class="line">2,1	40</span><br><span class="line">3,2	202</span><br><span class="line">4,1	232</span><br><span class="line">1,1	43</span><br><span class="line">2,2	70</span><br><span class="line">3,1	169</span><br><span class="line">4,2	280</span><br></pre></td></tr></table></figure>


      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2020/08/15/13-%E5%A4%A7%E6%95%B0%E6%8D%AE/01-%E5%9F%BA%E7%A1%80%E5%8F%8A%E5%AE%89%E8%A3%85/03-MapReduce/" data-id="clmcxec6j000nu8wa7fyxcj0v" data-title="MapReduce" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/MapReduce/" rel="tag">MapReduce</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-13-大数据/01-基础及安装/04-YARN" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2020/08/15/13-%E5%A4%A7%E6%95%B0%E6%8D%AE/01-%E5%9F%BA%E7%A1%80%E5%8F%8A%E5%AE%89%E8%A3%85/04-YARN/" class="article-date">
  <time class="dt-published" datetime="2020-08-15T08:50:45.000Z" itemprop="datePublished">2020-08-15</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/">大数据</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2020/08/15/13-%E5%A4%A7%E6%95%B0%E6%8D%AE/01-%E5%9F%BA%E7%A1%80%E5%8F%8A%E5%AE%89%E8%A3%85/04-YARN/">YARN</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211212214240.png"></p>
<h1 id="Yarn产生背景和基本架构"><a href="#Yarn产生背景和基本架构" class="headerlink" title="Yarn产生背景和基本架构"></a>Yarn产生背景和基本架构</h1><h2 id="Yarn产生背景"><a href="#Yarn产生背景" class="headerlink" title="Yarn产生背景"></a>Yarn产生背景</h2><p><strong>Question：</strong></p>
<ol>
<li>如何管理集群资源？</li>
<li>如何给任务合理分配资源？</li>
</ol>
<p>服务器资源：CPU、内存、GPU</p>
<h2 id="在Yarn产生之前"><a href="#在Yarn产生之前" class="headerlink" title="在Yarn产生之前"></a>在Yarn产生之前</h2><ul>
<li>MapReduce是Master&#x2F;Slave结构</li>
<li>JobTracker<ul>
<li>一个集群一个，安装在Master节点上</li>
<li>管理MapReduce作业(Job)</li>
<li>向TaskTracker分发任务(Task)</li>
</ul>
</li>
<li>TaskTracker<ul>
<li>每个slave节点一个</li>
<li>运行监控Map&#x2F;Reduce任务</li>
</ul>
</li>
</ul>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211212215722.png"></p>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211212220042.png"></p>
<h2 id="MapReduce-V1-执行job流程"><a href="#MapReduce-V1-执行job流程" class="headerlink" title="MapReduce V1 执行job流程"></a>MapReduce V1 执行job流程</h2><p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211212215321.png"></p>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211212215524.png"></p>
<h2 id="MapReduce-V1-存在的问题"><a href="#MapReduce-V1-存在的问题" class="headerlink" title="MapReduce V1 存在的问题"></a>MapReduce V1 存在的问题</h2><ul>
<li>MapReduce 模型简单，But</li>
<li>Job类型<ul>
<li>只能是MapReduce任务</li>
</ul>
</li>
<li>集群资源管理<ul>
<li>只有一个JobTracker，单点故障</li>
<li>每个Slave机器可运行的最大Map tasks数量和Reduce tasks数量固定</li>
</ul>
</li>
<li>难以共享集群资源<ul>
<li>Spark Strom Impala</li>
</ul>
</li>
</ul>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211212220239.png"></p>
<h2 id="Yarn-基本概念"><a href="#Yarn-基本概念" class="headerlink" title="Yarn 基本概念"></a>Yarn 基本概念</h2><ul>
<li><p>从Hadoop 2.0开始出现了Yarn</p>
</li>
<li><p>Yarn</p>
<ul>
<li>Yet Another Resource Negotiator（<strong>另一种资源协调者</strong>）</li>
</ul>
</li>
<li><p>在Yarn上天然支持各种分布式计算框架</p>
</li>
<li><p><strong>ResourceManager</strong></p>
<ul>
<li>一个集群只有一个，全局的资源管理器</li>
<li>负责启动客户端提交的Application</li>
<li>监控Node Manger，汇总上报的资源</li>
<li>根据请求分配资源</li>
</ul>
</li>
<li><p><strong>NodeManager</strong></p>
<ul>
<li>每个从属节点一个</li>
<li>管理自身所属节点的资源</li>
<li>监控资源使用情况（cpu, memory, disk, network)并向Resource Manager汇报</li>
</ul>
</li>
</ul>
<p>通常NodeManager会和DataNode混合部署，why？</p>
<p>NodeManager 执行任务从 DataNode 拉取数据，相当于从本地拉取数据，性能会有提升</p>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211212220905.png"></p>
<h2 id="Yarn-基本思想"><a href="#Yarn-基本思想" class="headerlink" title="Yarn 基本思想"></a>Yarn 基本思想</h2><ul>
<li>在MapReduce V1中<ul>
<li>JobTracker &#x3D; 资源管理器 + 任务调度器</li>
</ul>
</li>
<li>在Yarn中做了切分<ul>
<li>资源管理<ul>
<li>让<strong>ResourceManager</strong>负责</li>
</ul>
</li>
<li>任务调度<ul>
<li>让<strong>ApplicationMaster</strong>负责<ul>
<li>每个作业启动一个</li>
<li>根据作业切分任务tasks</li>
<li>向Resource Manager申请资源</li>
<li>与NodeManager协作，将分配申请到的资源给内部任务tasks</li>
<li>监控tasks运行情况，重启失败的任务</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>JobHistoryServer<ul>
<li>每个集群每种计算框架一个</li>
<li>负责搜集归档所有的日志</li>
</ul>
</li>
</ul>
<h2 id="Yarn-计算资源抽象"><a href="#Yarn-计算资源抽象" class="headerlink" title="Yarn 计算资源抽象"></a>Yarn 计算资源抽象</h2><ul>
<li>在Yarn中，计算资源被抽象为Container</li>
<li>每个Container描述：<ul>
<li>可使用的CPU资源和内存资源</li>
<li>执行命令</li>
<li>环境变量</li>
<li>外部资源</li>
</ul>
</li>
<li>如何获得运行各个任务的Container?<ul>
<li>由ApplicationMaster向ResourceManager申请</li>
<li>ApplicationMaster本身也运行在一个Container，这个Container由ResourceManager向自身申请启动</li>
</ul>
</li>
<li>如何启动运行？<ul>
<li>向Container所属的Node Manager发起运行</li>
</ul>
</li>
</ul>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211212221631.png"></p>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211212221855.png"></p>
<ul>
<li><p>CPU资源</p>
<ul>
<li><p>CPU资源不够，任务会运行慢</p>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211212221937.png"></p>
</li>
</ul>
</li>
<li><p>Memory资源</p>
<ul>
<li><p>内存资源不够，任务就有可能会运行失败</p>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211212222004.png"></p>
</li>
</ul>
</li>
</ul>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211212222158.png"></p>
<h1 id="Yarn资源调度过程"><a href="#Yarn资源调度过程" class="headerlink" title="Yarn资源调度过程"></a>Yarn资源调度过程</h1><h2 id="Yarn资源调度过程-1"><a href="#Yarn资源调度过程-1" class="headerlink" title="Yarn资源调度过程"></a>Yarn资源调度过程</h2><p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/yarn%E8%B5%84%E6%BA%90%E8%B0%83%E5%BA%A6%E8%BF%87%E7%A8%8B-Yarn%E8%B5%84%E6%BA%90%E8%B0%83%E5%BA%A6%E8%BF%87%E7%A8%8B-1.png"></p>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/yarn%E8%B5%84%E6%BA%90%E8%B0%83%E5%BA%A6%E8%BF%87%E7%A8%8B-Yarn%E8%B5%84%E6%BA%90%E8%B0%83%E5%BA%A6%E8%BF%87%E7%A8%8B-2.png"></p>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/yarn%E8%B5%84%E6%BA%90%E8%B0%83%E5%BA%A6%E8%BF%87%E7%A8%8B-Yarn%E8%B5%84%E6%BA%90%E8%B0%83%E5%BA%A6%E8%BF%87%E7%A8%8B-3.png"></p>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/yarn%E8%B5%84%E6%BA%90%E8%B0%83%E5%BA%A6%E8%BF%87%E7%A8%8B-Yarn%E8%B5%84%E6%BA%90%E8%B0%83%E5%BA%A6%E8%BF%87%E7%A8%8B-4.png"></p>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/yarn%E8%B5%84%E6%BA%90%E8%B0%83%E5%BA%A6%E8%BF%87%E7%A8%8B-Yarn%E8%B5%84%E6%BA%90%E8%B0%83%E5%BA%A6%E8%BF%87%E7%A8%8B-5.png"></p>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/yarn%E8%B5%84%E6%BA%90%E8%B0%83%E5%BA%A6%E8%BF%87%E7%A8%8B-Yarn%E8%B5%84%E6%BA%90%E8%B0%83%E5%BA%A6%E8%BF%87%E7%A8%8B-6.png"></p>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/yarn%E8%B5%84%E6%BA%90%E8%B0%83%E5%BA%A6%E8%BF%87%E7%A8%8B-Yarn%E8%B5%84%E6%BA%90%E8%B0%83%E5%BA%A6%E8%BF%87%E7%A8%8B-7.png"></p>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/yarn%E8%B5%84%E6%BA%90%E8%B0%83%E5%BA%A6%E8%BF%87%E7%A8%8B-Yarn%E8%B5%84%E6%BA%90%E8%B0%83%E5%BA%A6%E8%BF%87%E7%A8%8B-8.png"></p>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/yarn%E8%B5%84%E6%BA%90%E8%B0%83%E5%BA%A6%E8%BF%87%E7%A8%8B-Yarn%E8%B5%84%E6%BA%90%E8%B0%83%E5%BA%A6%E8%BF%87%E7%A8%8B-9.png"></p>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/yarn%E8%B5%84%E6%BA%90%E8%B0%83%E5%BA%A6%E8%BF%87%E7%A8%8B-Yarn%E8%B5%84%E6%BA%90%E8%B0%83%E5%BA%A6%E8%BF%87%E7%A8%8B-10.png"></p>
<h2 id="Yarn各个组件之间的心跳信号"><a href="#Yarn各个组件之间的心跳信号" class="headerlink" title="Yarn各个组件之间的心跳信号"></a>Yarn各个组件之间的心跳信号</h2><ul>
<li>Application Master与Resource Manager心跳<ul>
<li>AM-&gt;RM<ul>
<li>对Container的资源需求（CPU和Memory）和优先级</li>
<li>已用完等待回收的Container列表</li>
</ul>
</li>
<li>RM-&gt;AM<ul>
<li>新申请到的Container</li>
<li>已完成Container的状态</li>
</ul>
</li>
</ul>
</li>
<li>Application Master与Node Manager心跳<ul>
<li>AM -&gt; NM<ul>
<li>发起启动Container请求</li>
</ul>
</li>
<li>NM -&gt; AM<ul>
<li>汇报Container状态</li>
</ul>
</li>
</ul>
</li>
<li>Node Manager与Resource Manager心跳<ul>
<li>NM-&gt;RM<ul>
<li>Node Manager上所有的Container状态</li>
</ul>
</li>
<li>RM-&gt;NM<ul>
<li>已删除和等待清理的Container列表</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="Yarn资源隔离策略"><a href="#Yarn资源隔离策略" class="headerlink" title="Yarn资源隔离策略"></a>Yarn资源隔离策略</h2><p>主要两种，第一种线程监控方案，第二种基于 cgroups</p>
<p>为什么采用线程监控方案？<br>在 linux 下进程创建子进程使用 fork 命令，刚创建时资源会有一个突增，可能会超过限制大小，yarn 基于线程监控方案允许在短时间内资源上限，资源上限时间过长则会 kill 掉进程，所以使用的是监控的策略。cgroups 只要超过资源限制就 kill 掉进程。 </p>
<ul>
<li><p>内存资源隔离</p>
<ul>
<li><p>基于线程监控方案</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Container[pid=container_1406552545451_0009_01_000002,containerID=container_234132_0001_01_000001] is running beyond physical memory limits. Current usage: 569.1 MB of 512 MB physical memory used; 970.1 MB of 1.0 GB virtual memory used. Killing container.</span><br></pre></td></tr></table></figure>
</li>
<li><p>基于cgroups</p>
</li>
</ul>
</li>
<li><p>CPU资源隔离</p>
<ul>
<li>默认不对CPU资源进行隔离</li>
<li>基于cgroups</li>
</ul>
</li>
<li><p>IO资源隔离</p>
<ul>
<li>默认不对IO资源进行隔离</li>
<li>基于cgroups</li>
</ul>
</li>
</ul>
<p>yarn 默认不对 cpu、io 资源进行隔离，如果需要对 cpu、io 隔离则使用 cgroups，勾选下面两个配置即可。</p>
<p>注意：cgroups 只能对读进行隔离，写不能对写隔离。</p>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211215225244.png"></p>
<h2 id="Yarn容错处理"><a href="#Yarn容错处理" class="headerlink" title="Yarn容错处理"></a>Yarn容错处理</h2><ul>
<li>失败类型<ul>
<li>程序失败 进程奔溃 硬件问题</li>
</ul>
</li>
<li>如果作业失败了<ul>
<li>作业异常均会汇报给Application Master</li>
<li>通过心跳信号检查挂住的任务</li>
<li>一个作业的任务失败比例超过配置，就会认为该任务失败</li>
</ul>
</li>
<li>如果Application Master失败了<ul>
<li>Resource Manager接收不到心跳信号时会重启Application Master</li>
</ul>
</li>
<li>如果Node Manager失败了<ul>
<li>Resource Manager接收不到心跳信号时会将其移出</li>
<li>Resource Manager通知Application Master，让Application Master决定任务如何处理</li>
<li>如果某个Node Manager失败任务次数过多，Resource Manager调度任务时不再其上面运行任务</li>
</ul>
</li>
<li>如果Resource Manager运行失败<ul>
<li>通过checkpoint机制，定时将其状态保存到磁盘，失败的时候，重新运行</li>
<li>通过Zookeeper同步状态和实现透明的HA</li>
</ul>
</li>
</ul>
<h2 id="Example-Yarn-HelloWorld"><a href="#Example-Yarn-HelloWorld" class="headerlink" title="Example Yarn HelloWorld"></a>Example Yarn HelloWorld</h2><ul>
<li><p>Client &lt;–&gt; ResourceManager 通信</p>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211215230040.png"></p>
</li>
<li><p>ApplicationMaster &lt;–&gt; ResourceManager 通信</p>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211215230101.png"></p>
</li>
<li><p>ApplicationMaster &lt;–&gt; NodeManager 通信</p>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211215230134.png"></p>
</li>
<li><p>Yarn Example Code演示讲解</p>
<p>注意项目 pom.xml 文件中的 cdh 版本</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">// 我使用的是 cdh6.2.0版本，根据自己版本修改</span><br><span class="line"><span class="tag">&lt;<span class="name">hadoop.version</span>&gt;</span>3.0.0-cdh6.2.0<span class="tag">&lt;/<span class="name">hadoop.version</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>提交 client 到 yarn 执行</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop jar yarn-example-master-1.0-SNAPSHOT.jar com.study.bigdata.yarn.examples.MyClient</span><br></pre></td></tr></table></figure>

<p>查看 application 日志</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">yarn logs -applicationId &lt;应用<span class="built_in">id</span>&gt;</span><br><span class="line">yarn logs -applicationId application_1636115927565_0037</span><br></pre></td></tr></table></figure>

<p>查看 containerId 日志</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">yarn logs -containerId &lt;容器<span class="built_in">id</span>&gt;</span><br><span class="line">yarn logs -containerId container_1636115927565_0037_01_000003</span><br></pre></td></tr></table></figure></li>
</ul>
<h1 id="Yarn调度器和调度算法"><a href="#Yarn调度器和调度算法" class="headerlink" title="Yarn调度器和调度算法"></a>Yarn调度器和调度算法</h1><h2 id="Yarn资源调度算法"><a href="#Yarn资源调度算法" class="headerlink" title="Yarn资源调度算法"></a>Yarn资源调度算法</h2><ul>
<li>集群资源调度器需要解决：<ul>
<li>多租户(Multi-tenancy)：<ul>
<li>多用户同时提交多种应用程序</li>
<li>资源调度器需要解决如何合理分配资源。</li>
</ul>
</li>
<li>可扩展性(Scalability)：增加集群机器的数量可以提高整体集群性能。</li>
</ul>
</li>
<li>Yarn使用队列解决多租户中共享资源的问题</li>
</ul>
<p>root</p>
<p>|—-prd</p>
<p>|—-dev</p>
<p>​	|—-eng</p>
<p>​	|—-science</p>
<ul>
<li>支持三种资源调度器<ul>
<li>FIFO</li>
<li>Capacity Scheduler</li>
<li>Fair Scheduler</li>
</ul>
</li>
</ul>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211215230305.png"></p>
<h2 id="FIFO调度器"><a href="#FIFO调度器" class="headerlink" title="FIFO调度器"></a>FIFO调度器</h2><ul>
<li>所有向集群提交的作业使用一个队列</li>
<li>根据提交作业的顺序来运行（先来先服务）</li>
<li>优点：<ul>
<li>简单易懂</li>
<li>可以按照作业优先级调度</li>
</ul>
</li>
<li>缺点：<ul>
<li>资源利用率不高</li>
<li>不允许抢占</li>
</ul>
</li>
</ul>
<h2 id="Capacity-Scheduler资源调度器"><a href="#Capacity-Scheduler资源调度器" class="headerlink" title="Capacity Scheduler资源调度器"></a>Capacity Scheduler资源调度器</h2><ul>
<li>设计思想：<ul>
<li>资源按照比例分配给各个队列</li>
</ul>
</li>
<li>特点<ul>
<li>计算能力保证<ul>
<li>以队列为单位划分资源，每个队列最低资源保证。</li>
</ul>
</li>
<li>灵活性<ul>
<li>当某个队列空闲时，其资源可以分配给其他的队列使用。</li>
</ul>
</li>
<li>支持优先级<ul>
<li>单个队列内部使用FIFO，支持作业优先级调度</li>
</ul>
</li>
<li>多租户<ul>
<li>综合考虑多种因素防止单个作业、用户或者队列独占资源。</li>
<li>每个队列可以配置一定比例的最低资源配置和使用上限。</li>
<li>每个队列有严格的访问控制，只能向自己的队列提交任务。</li>
</ul>
</li>
<li>基于资源的调度<ul>
<li>支持内存资源调度和CPU资源的调度。</li>
</ul>
</li>
<li>支持抢占（从2.8.0版本开始）</li>
</ul>
</li>
</ul>
<p>root</p>
<p>|—-prd <span style="color:red">70%</span></p>
<p>|—-dev <span style="color:red">30%</span></p>
<p>​	|—-eng <span style="color:red">50%</span></p>
<p>​	|—-science <span style="color:red">50%</span></p>
<h2 id="Capacity-Scheduler配置"><a href="#Capacity-Scheduler配置" class="headerlink" title="Capacity Scheduler配置"></a>Capacity Scheduler配置</h2><p>配置capacity-scheduler.xml</p>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211215230740.png"></p>
<p>配置yarn-site.xml</p>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211215230800.png"></p>
<p>root</p>
<p>|—-prd <span style="color:red">70%</span></p>
<p>|—-dev <span style="color:red">30%</span></p>
<p>​	|—-eng <span style="color:red">50%</span></p>
<p>​	|—-science <span style="color:red">50%</span></p>
<h2 id="Capacity-Scheduler资源分配算法"><a href="#Capacity-Scheduler资源分配算法" class="headerlink" title="Capacity Scheduler资源分配算法"></a>Capacity Scheduler资源分配算法</h2><ol>
<li><p>选择队列</p>
<p>从根队列开始，使用深度优先算法找出资源占用率最低的叶子队列</p>
</li>
<li><p>选择作业</p>
<p>默认按照作业优先级和提交时间顺序选择</p>
</li>
<li><p>选择Container</p>
<p>取该作业中最高优先级的Container，如果优先级相同会选择满足本地性的Container：Node Local &gt; Rack ocal &gt; Different Rack</p>
</li>
</ol>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211215231840.png"></p>
<h2 id="Fair-Scheduler资源调度器"><a href="#Fair-Scheduler资源调度器" class="headerlink" title="Fair Scheduler资源调度器"></a>Fair Scheduler资源调度器</h2><ul>
<li>设计思想<ul>
<li>资源公平分配</li>
</ul>
</li>
<li>具有与Capacity Scheduler相似的特点<ul>
<li>树状队列</li>
<li>每个队列有独立的最小资源保证</li>
<li>空闲时可以分配资源给其他队列使用</li>
<li>支持内存资源调度和CPU资源调度</li>
<li>支持抢占</li>
</ul>
</li>
<li>不同点<ul>
<li>核心调度策略不同<ul>
<li>Capacity Scheduler优先选择资源利用率低的队列</li>
<li>公平调度器考虑的是公平，公平体现在作业对资源的缺额</li>
</ul>
</li>
<li>单独设置队列间资源分配方式<ul>
<li>FAIR（默认used memory&#x2F;min share） </li>
<li>DRF（主资源公平调度）</li>
</ul>
</li>
<li>单独设置队列内部资源分配方式<ul>
<li>FAIR DRF FIFO</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="Fair-Scheduler-FAIR资源分配算法"><a href="#Fair-Scheduler-FAIR资源分配算法" class="headerlink" title="Fair Scheduler - FAIR资源分配算法"></a>Fair Scheduler - FAIR资源分配算法</h2><ul>
<li><p>总体流程与Capacity Scheduler一致</p>
<ol>
<li>选择队列</li>
<li>选择作业</li>
<li>选择Container</li>
</ol>
</li>
<li><p>选择选择队列和作业使用公平排序算法</p>
<ul>
<li><p>实际最小份额</p>
<p>minShare &#x3D; Min(资源需求量，配置minShare） </p>
</li>
<li><p>是否饥饿</p>
<p>isNeedy &#x3D; 资源使用量&lt; minShare</p>
</li>
<li><p>资源分配比</p>
<p>minShareRatio &#x3D; 资源使用量&#x2F;Max(minShare, 1)</p>
</li>
<li><p>资源使用权重比</p>
<p>useToWeightRatio &#x3D; 资源使用量&#x2F;权重权重在配置文件中配置</p>
</li>
</ul>
</li>
<li><p>参考 FairShareComparator 实现类</p>
</li>
</ul>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211218103223.png"></p>
<h2 id="Capacity-Scheduler和Fair-Scheduler对比"><a href="#Capacity-Scheduler和Fair-Scheduler对比" class="headerlink" title="Capacity Scheduler和Fair Scheduler对比"></a>Capacity Scheduler和Fair Scheduler对比</h2><table>
<thead>
<tr>
<th></th>
<th>Capacity Scheduler</th>
<th>Fair Scheduler</th>
</tr>
</thead>
<tbody><tr>
<td>设计思想</td>
<td>资源按照比例分配，并添加各种严格限制，防止个别用户或者队列单独占资源。</td>
<td>基于公平算法分配资源</td>
</tr>
<tr>
<td>队列是树状结构</td>
<td>是</td>
<td>是</td>
</tr>
<tr>
<td>多用户</td>
<td>支持</td>
<td>支持</td>
</tr>
<tr>
<td>最小资源保证</td>
<td>支持</td>
<td>支持</td>
</tr>
<tr>
<td>最大资源限制</td>
<td>支持</td>
<td>支持</td>
</tr>
<tr>
<td>资源共享</td>
<td>支持。当一个队列有资源剩余时，可暂时将剩余资源共享给其他队列。</td>
<td>支持。当一个队列有资源剩余时，可暂时将剩余资源共享给其他队列。</td>
</tr>
<tr>
<td>限制集群或队列并发作业数</td>
<td>支持</td>
<td>支持</td>
</tr>
<tr>
<td>支持抢占</td>
<td>支持</td>
<td>支持</td>
</tr>
<tr>
<td>支持批量调度</td>
<td>支持。当一个节点有大量资源可用时，可一次性分配完成，不用多次分配每次分配一份资源。</td>
<td>支持。当一个节点有大量资源可用时，可一次性分配完成，不用多次分配每次分配一份资源。</td>
</tr>
<tr>
<td>为每个队列单独设置调度策略</td>
<td>不支持</td>
<td>支持</td>
</tr>
<tr>
<td>资源分配策略</td>
<td>FIFO或DRF</td>
<td>Fair、FIFO或DRF</td>
</tr>
<tr>
<td>动态加载配置文件</td>
<td>支持</td>
<td>支持</td>
</tr>
</tbody></table>
<h1 id="Yarn常用命令介绍"><a href="#Yarn常用命令介绍" class="headerlink" title="Yarn常用命令介绍"></a>Yarn常用命令介绍</h1><h2 id="常用Yarn命令"><a href="#常用Yarn命令" class="headerlink" title="常用Yarn命令"></a>常用Yarn命令</h2><ul>
<li><p>yarn application</p>
<ul>
<li><p>列出所有Application</p>
<p>yarn application -list</p>
</li>
<li><p>根据Application状态过滤</p>
<p>yarn application -list -appStatus ACCEPTED</p>
</li>
<li><p>Kill掉Application</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yarn application -<span class="built_in">kill</span> &lt;ApplicationId&gt; </span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p>yarn logs</p>
<ul>
<li><p>查询Application日志</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yarn logs -applicationId &lt;ApplicationId&gt; </span><br></pre></td></tr></table></figure>
</li>
<li><p>查询Container日志</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">yarn logs -applicationId &lt;ApplicationId&gt; \ </span><br><span class="line">-containerId &lt;ContainerId&gt; \ </span><br><span class="line">-nodeAddress &lt;NodeAddress&gt;</span><br></pre></td></tr></table></figure>

<p>端口是配置文件中yarn.nodemanager.webapp.address参数指定</p>
</li>
</ul>
</li>
<li><p>yarn applicationattempt</p>
<ul>
<li><p>列出所有Application尝试的列表</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yarn applicationattempt -list &lt;ApplicationId&gt;</span><br></pre></td></tr></table></figure>
</li>
<li><p>打印ApplicationAttemp状态</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yarn applicationattempt -status &lt;ApplicationAttemptId&gt;</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p>yarn container</p>
<ul>
<li><p>列出所有Container</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yarn container -list &lt;ApplicationAttemptId&gt;</span><br></pre></td></tr></table></figure>
</li>
<li><p>打印Container状态</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yarn container -status &lt;ContainerId&gt;</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p>yarn node</p>
<ul>
<li><p>列出所有节点</p>
<p>yarn node -list -all</p>
</li>
</ul>
</li>
<li><p>yarn rmadmin</p>
<ul>
<li><p>加载队列配置</p>
<p>yarn rmadmin –refreshQueues</p>
</li>
</ul>
</li>
<li><p>yarn queue</p>
<ul>
<li><p>打印队列信息</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yarn queue -status &lt;QueueName&gt; </span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p>yarn classpath</p>
<ul>
<li>打印Hadoop Jar包路径</li>
</ul>
</li>
</ul>
<h1 id="常见基于Yarn的计算框架"><a href="#常见基于Yarn的计算框架" class="headerlink" title="常见基于Yarn的计算框架"></a>常见基于Yarn的计算框架</h1><h2 id="MapReduce-On-Yarn"><a href="#MapReduce-On-Yarn" class="headerlink" title="MapReduce On Yarn"></a>MapReduce On Yarn</h2><p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/yarn%E8%B5%84%E6%BA%90%E8%B0%83%E5%BA%A6%E8%BF%87%E7%A8%8B-MapReduce%20On%20Yarn.png"></p>
<h2 id="Spark"><a href="#Spark" class="headerlink" title="Spark"></a>Spark</h2><ul>
<li>基于内存的大数据计算引擎</li>
<li>MapReduce不适合的计算场景：<ul>
<li>迭代式作业<ul>
<li>机器学习，图算法需要迭代运行Mapper和Reducer多次</li>
</ul>
</li>
<li>交互式作业<ul>
<li>交互式数据分析，尤其是涉及到机器学习算法</li>
</ul>
</li>
<li>流式作业<ul>
<li>无穷无尽的流式数据，需要不断的对数据进行聚合计算</li>
</ul>
</li>
</ul>
</li>
<li>Spark提供了一种新的数据抽象RDD弹性分布式数据集，可以将数据存储在内存中，而不是存储到HDFS上，使得快速迭代计算成为可能。</li>
</ul>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/yarn%E8%B5%84%E6%BA%90%E8%B0%83%E5%BA%A6%E8%BF%87%E7%A8%8B-Spark%20%E8%BF%AD%E4%BB%A3%E8%AE%A1%E7%AE%97.png"></p>
<p>在MapReduce和Spark上使用逻辑回归耗时</p>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211218122242.png"></p>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/yarn%E8%B5%84%E6%BA%90%E8%B0%83%E5%BA%A6%E8%BF%87%E7%A8%8B-%E5%9F%BA%E4%BA%8EMapReduce%E3%80%81Spark%E7%9A%84%E8%BF%AD%E4%BB%A3%E8%AE%A1%E7%AE%97.png"></p>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/yarn%E8%B5%84%E6%BA%90%E8%B0%83%E5%BA%A6%E8%BF%87%E7%A8%8B-Spark%20%E8%AE%A1%E7%AE%97%E6%B5%81%E7%A8%8B.png"></p>
<h2 id="Spark-On-Yarn"><a href="#Spark-On-Yarn" class="headerlink" title="Spark On Yarn"></a>Spark On Yarn</h2><p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/yarn%E8%B5%84%E6%BA%90%E8%B0%83%E5%BA%A6%E8%BF%87%E7%A8%8B-Spark%20On%20Yarn.png"></p>
<h1 id="思考和动手"><a href="#思考和动手" class="headerlink" title="思考和动手"></a>思考和动手</h1><ol>
<li><p>请简述一下Yarn的结构</p>
</li>
<li><p>描述RM和NM是如何交互的</p>
</li>
<li><p>完成在CDH集群中配置队列</p>
</li>
<li><p>将WordCount作业提交到队列上运行并观察运行过程</p>
</li>
<li><p>熟悉Yarn-Example代码</p>
</li>
</ol>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2020/08/15/13-%E5%A4%A7%E6%95%B0%E6%8D%AE/01-%E5%9F%BA%E7%A1%80%E5%8F%8A%E5%AE%89%E8%A3%85/04-YARN/" data-id="clmcxec6r000ru8wa8wb9br81" data-title="YARN" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/YARN/" rel="tag">YARN</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-14-Docker/01-Docker基本概念" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2020/08/15/14-Docker/01-Docker%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/" class="article-date">
  <time class="dt-published" datetime="2020-08-15T08:50:45.000Z" itemprop="datePublished">2020-08-15</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Docker/">Docker</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2020/08/15/14-Docker/01-Docker%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/">Docker基本概念</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="Docker是什么"><a href="#Docker是什么" class="headerlink" title="Docker是什么"></a>Docker是什么</h1><p>Docker 最初是 dotCloud 公司创始人 Solomon Hykes 在法国期间发起的一个公司内部项目，它是基于 dotCloud 公司多年云服务技术的一次革新，并于 2013 年 3 月以 Apache 2.0 授权协议开源，主要项目代码在 GitHub 上进行维护。Docker 项目后来还加入了 Linux 基金会，并成立推动 开放容器联盟（OCI）。</p>
<p>Docker 使用 Google 公司推出的 <strong>Go 语言</strong> 进行开发实现，基于 Linux 内核的cgroup，namespace，以及 AUFS 类的 Union FS 等技术，对进程进行封装隔离，属于操作系统层面的虚拟化技术。由于隔离的进程独立于宿主和其它的隔离的进程，因此也称其为<strong>容器</strong>。</p>
<p>Docker 在容器的基础上，进行了进一步的封装，从文件系统、网络互联到进程隔离等等，极大的简化了容器的创建和维护。使得 <strong>Docker 技术比虚拟机技术更为轻便、快捷</strong>。</p>
<h1 id="Docker-和传统虚拟机"><a href="#Docker-和传统虚拟机" class="headerlink" title="Docker 和传统虚拟机"></a>Docker 和传统虚拟机</h1><p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211010092730.png" alt="传统虚拟机"></p>
<p>传统虚拟机技术是虚拟出一套硬件后，在其上运行一个完整操作系统，在该系统上再运行所需应用进程；</p>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211010093118.png" alt="Docker"></p>
<p>而容器内的应用进程直接运行于宿主的内核，容器内没有自己的内核，而且也没有进行硬件虚拟。因此容器要比传统虚拟机更为轻便。</p>
<h1 id="为什么要使用-Docker"><a href="#为什么要使用-Docker" class="headerlink" title="为什么要使用 Docker"></a>为什么要使用 Docker</h1><p><strong>Docker 优势</strong></p>
<ul>
<li>更高效的利用系统资源</li>
<li>更快速的启动时间</li>
<li>一致的运行环境</li>
<li>持续交付和部署</li>
<li>更轻松的迁移</li>
<li>更轻松的维护和扩展</li>
</ul>
<p><strong>对比传统虚拟机总结</strong></p>
<table>
<thead>
<tr>
<th>特性</th>
<th>容器</th>
<th>虚拟机</th>
</tr>
</thead>
<tbody><tr>
<td>启动</td>
<td>秒级</td>
<td>分钟级</td>
</tr>
<tr>
<td>硬盘使用</td>
<td>一般为 MB</td>
<td>一般为 GB</td>
</tr>
<tr>
<td>性能</td>
<td>接近原生</td>
<td>较弱</td>
</tr>
<tr>
<td>系统支持量</td>
<td>单机支持上千个容器</td>
<td>一般几十个</td>
</tr>
</tbody></table>
<h1 id="Docker-架构"><a href="#Docker-架构" class="headerlink" title="Docker 架构"></a>Docker 架构</h1><p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211010093354.png"></p>
<p>Docker 使用客户端-服务器 (C&#x2F;S) 架构模式，使用远程API来管理和创建 Docker容器。</p>
<h1 id="Docker-基本概念"><a href="#Docker-基本概念" class="headerlink" title="Docker 基本概念"></a>Docker 基本概念</h1><p>Docker 包括三个基本概念</p>
<ul>
<li>镜像（ Image ）</li>
<li>容器（ Container ）</li>
<li>仓库（ Repository ）</li>
</ul>
<p>理解了这三个概念，就理解了 Docker 的整个生命周期</p>
<h1 id="Docker-镜像"><a href="#Docker-镜像" class="headerlink" title="Docker 镜像"></a>Docker 镜像</h1><p>我们都知道，操作系统分为内核和用户空间。对于 Linux 而言，内核启动后，会挂载 root文件系统为其提供用户空间支持。而 Docker 镜像（Image），就相当于是一个 root 文件系统。比如官方镜像 centos:7.6 就包含了完整的一套 centos 7.6 最小系统的 root 文件系统。</p>
<p>Docker 镜像是一个特殊的文件系统，除了提供容器运行时所需的程序、库、资源、配置等文件外，还包含了一些为运行时准备的一些配置参数（如匿名卷、环境变量、用户等）。镜像不包含任何动态数据，其内容在构建之后也不会被改变。</p>
<h1 id="Docker-镜像分层存储"><a href="#Docker-镜像分层存储" class="headerlink" title="Docker 镜像分层存储"></a>Docker 镜像分层存储</h1><p>因为镜像包含操作系统完整的 root 文件系统，其体积往往是庞大的，因此在 Docker 设计时将其设计为分层存储的架构。镜像只是一个虚拟的概念，其实际体现并非由一个文件组成，而是由一组文件系统组成，或者说，由多层文件系统联合组成。</p>
<p>镜像构建时，会一层层构建，前一层是后一层的基础。每一层构建完就不会再发生改变，后一层上的任何改变只发生在自己这一层。在构建镜像的时候，需要额外小心，每一层尽量只包含该层需要添加的东西，任何额外的东西应该在该层构建结束前清理掉。</p>
<p>分层存储的特征还使得镜像的复用、定制变的更为容易。甚至可以用之前构建好的镜像作为基础层，然后进一步添加新的层，以定制自己所需的内容，构建新的镜像。</p>
<h1 id="Docker-容器"><a href="#Docker-容器" class="headerlink" title="Docker 容器"></a>Docker 容器</h1><p>镜像（ Image ）和容器（ Container ）的关系，就像 Java 中的 类 和 实例一样，镜像是静态的定义，容器是镜像运行时的实体。容器可以被创建、启动、停止、删除、暂停等。</p>
<p>前面讲过镜像使用的是分层存储，容器也是如此。每一个容器运行时，是以镜像为基础层，在其上创建一个当前容器的存储层，我们可以称这个为容器运行时读写而准备的存储层为容器存储层。</p>
<p>容器存储层的生存周期和容器一样，容器消亡时，容器存储层也随之消亡。因此，任何保存于容器存储层的信息都会随容器删除而丢失。</p>
<p>按照 Docker 最佳实践的要求，容器不应该向其存储层内写入任何数据，容器存储层要保持无状态化。所有的文件写入操作，都应该使用 Volume 数据卷、或者绑定宿主目录，在这些位置的读写会跳过容器存储层，直接对宿主（或网络存储）发生读写，其性能和稳定性更高。</p>
<p>数据卷的生存周期独立于容器，容器消亡，数据卷不会消亡。因此，使用数据卷后，容器删除或者重新运行之后，数据却不会丢失。</p>
<h1 id="Docker-仓库"><a href="#Docker-仓库" class="headerlink" title="Docker 仓库"></a>Docker 仓库</h1><p>镜像构建完成后，可以很容易的在当前宿主机上运行，但是，如果需要在其它服务器上使用这个镜像，我们就需要一个集中的存储、分发镜像的服务，Docker Registry 就是这样的服务。</p>
<p>一个 Docker Registry 中可以包含多个仓库（ Repository ）；每个仓库可以包含多个标签（ Tag ）；每个标签对应一个镜像。</p>
<p>通常，一个仓库会包含同一个软件不同版本的镜像，而标签就常用于对应该软件的各个版本。我们可以通过 &lt;仓库名&gt;:&lt;标签&gt; 的格式来指定具体是这个软件哪个版本的镜像。如果不给出标签，将以 latest 作为默认标签。</p>
<p>以 centos 镜像 为例， centos 是仓库的名字，其内包含有不同的版本标签，如， 6.9 、7.5 。我们可以通过 centos:6.9 ，或者 centos:7.5 来具体指定所需哪个版本的镜像。如果忽略了标签，比如 centos ，那将视为 centos:latest 。</p>
<p>仓库名经常以 两段式路径 形式出现，比如 study&#x2F;nginx，前者往往意味着 Docker Registry 多用户环境下的用户名，后者则往往是对应的软件名。但这并非绝对，取决于所使用的具体 Docker Registry 的软件或服务。</p>
<ul>
<li><p>Docker Registry 公开仓库</p>
<p>常用的 Registry 是官方的 Docker Hub，这也是默认的 Registry。除此以外，还有 CoreOS 的 Quay.io，CoreOS 相关的镜像存储在这里；Google 的 Google Container Registry，Kubernetes 的镜像使用的就是这个服务。</p>
<p>国内的一些云服务商提供了针对 Docker Hub 的镜像服务，这些镜像服务被称为加速器。常见的有 阿里云加速器、DaoCloud 加速器 等。使用加速器会直接从国内的地址下载 Docker Hub 的镜像，比直接从 Docker Hub 下载速度会提高很多。</p>
<p>国内也有一些云服务商提供类似于 Docker Hub 的公开服务。比如 网易云镜像服务、DaoCloud 镜像市场、阿里云镜像库 等。</p>
</li>
<li><p>私有 Docker Registry</p>
<p>除了使用公开服务外，用户还可以在本地搭建私有 Docker Registry。Docker 官方提供了Docker Registry 镜像，可以直接使用做为私有 Registry 服务。</p>
<p>开源的 Docker Registry 镜像只提供了 Docker Registry API 的服务端实现，足以支持docker 命令，不影响使用。但不包含图形界面，以及镜像维护、用户管理、访问控制等高级功能。在官方的商业化版本 Docker Trusted Registry 中，提供了这些高级功能。</p>
<p>除了官方的 Docker Registry 外，还有第三方软件实现了 Docker Registry API，甚至提供了用户界面以及一些高级功能。比如，VMWare Harbor 和 Sonatype Nexus。</p>
</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2020/08/15/14-Docker/01-Docker%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/" data-id="clmcxec8c0019u8wagxe3g8nv" data-title="Docker基本概念" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Docker/" rel="tag">Docker</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-14-Docker/02-Docker安装及加速" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2020/08/15/14-Docker/02-Docker%E5%AE%89%E8%A3%85%E5%8F%8A%E5%8A%A0%E9%80%9F/" class="article-date">
  <time class="dt-published" datetime="2020-08-15T08:50:45.000Z" itemprop="datePublished">2020-08-15</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Docker/">Docker</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2020/08/15/14-Docker/02-Docker%E5%AE%89%E8%A3%85%E5%8F%8A%E5%8A%A0%E9%80%9F/">Docker安装及加速</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="Docker-版本命名"><a href="#Docker-版本命名" class="headerlink" title="Docker 版本命名"></a>Docker 版本命名</h1><p>Docker 在 1.13 版本之后，从 2017 年的 3 月 1 日开始，版本命名规则变为如下</p>
<table>
<thead>
<tr>
<th>项目</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>版本格式</td>
<td>YY.MM</td>
</tr>
<tr>
<td>Stable 版本</td>
<td>每个季度发行</td>
</tr>
<tr>
<td>Edge 版本</td>
<td>每个月发行</td>
</tr>
<tr>
<td>当前 Docker CE Stable 版本</td>
<td>17.09</td>
</tr>
<tr>
<td>当前 Docker CE Edge 版本</td>
<td>17.11</td>
</tr>
</tbody></table>
<p>同时 Docker 划分为 CE 和 EE。CE 即社区版（免费，支持周期三个月），EE 即企业版，强调安全，付费使用。</p>
<h1 id="Docker-安装"><a href="#Docker-安装" class="headerlink" title="Docker 安装"></a>Docker 安装</h1><p>官方网站上有各种环境下的 安装指南，这里主要介绍 Docker CE 在 Linux 、Windows 10 和 MacOS 上的安装。</p>
<p>官方安装指南地址：<a target="_blank" rel="noopener" href="https://docs.docker.com/engine/installation/">https://docs.docker.com/engine/installation/</a> </p>
<h1 id="CentOS-安装-Docker"><a href="#CentOS-安装-Docker" class="headerlink" title="CentOS 安装 Docker"></a>CentOS 安装 Docker</h1><ol>
<li><p>系统要求</p>
<p>Docker CE 支持 64 位版本 CentOS 7，并且要求内核版本不低于 3.10。</p>
</li>
<li><p>卸载旧版本</p>
<p>旧版本的 Docker 称为 docker 或者 docker-engine ，使用以下命令卸载旧版本：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo yum remove docker docker-common docker-selinux docker-engine</span><br></pre></td></tr></table></figure>
</li>
<li><p>使用 yum 安装</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo yum install docker-ce</span><br></pre></td></tr></table></figure>
</li>
<li><p>使用脚本安装</p>
<p>在测试或开发环境中 Docker 官方为了简化安装流程，提供了一套便捷的安装脚本，CentOS系统上可以使用这套脚本安装：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">curl -fsSL  https://get.docker.com -o get-docker.sh</span><br><span class="line">sudo sh get-docker.sh --mirror Aliyun</span><br></pre></td></tr></table></figure>

<p>执行这个命令后，脚本就会自动的将一切准备工作做好，并且把 Docker CE 的 Edge 版本安装在系统中。</p>
</li>
<li><p>启动Docker CE</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo systemctl enable docker	#设置开启启动</span><br><span class="line">sudo systemctl start docker</span><br></pre></td></tr></table></figure>
</li>
<li><p>建立 docker 用户组</p>
<p>默认情况下， docker 命令会使用 Unix socket 与 Docker 引擎通讯。而只有 root 用户和docker 组的用户才可以访问 Docker 引擎的 Unix socket。一般 Linux 系统上不会直接使用 root 用户进行操作。因此，需要将使用 docker 的用户加入 docker用户组。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo groupadd docker	#建立 docker 组</span><br><span class="line">sudo usermod -aG docker $USER	#将当前用户加入 docker 组</span><br></pre></td></tr></table></figure>
</li>
<li><p>测试 Docker 是否安装正确</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run hello-world	#启动一个基于hello-world镜像的容器</span><br></pre></td></tr></table></figure>

<p>若能正常输出以上信息，则说明安装成功。</p>
</li>
</ol>
<h1 id="镜像加速器"><a href="#镜像加速器" class="headerlink" title="镜像加速器"></a>镜像加速器</h1><p>国内从 Docker Hub 拉取镜像有时会遇到困难，此时可以配置镜像加速器。Docker 官方和国内很多云服务商都提供了国内加速器服务，例如：</p>
<ul>
<li>Docker 官方提供的中国 registry mirror</li>
<li>阿里云加速器</li>
<li>DaoCloud 加速器</li>
<li>163 加速器</li>
</ul>
<p>接下来我们以 163 加速器为例进行介绍。</p>
<h1 id="CentOS-7-配置镜像加速"><a href="#CentOS-7-配置镜像加速" class="headerlink" title="CentOS 7 配置镜像加速"></a>CentOS 7 配置镜像加速</h1><p>对于使用 systemd 的系统，请在 &#x2F;etc&#x2F;docker&#x2F;daemon.json 中写入如下内容（如果文件不存在请新建该文件）</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">	<span class="attr">&quot;registry-mirrors&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">		<span class="string">&quot;http://hub-mirror.c.163.com&quot;</span></span><br><span class="line">	<span class="punctuation">]</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>

<p>重新启动服务生效</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo systemctl daemon-reload</span><br><span class="line">sudo systemctl restart docker</span><br></pre></td></tr></table></figure>

<h1 id="检查加速器是否生效"><a href="#检查加速器是否生效" class="headerlink" title="检查加速器是否生效"></a>检查加速器是否生效</h1><p>配置加速器之后，如果拉取镜像仍然十分缓慢，请手动检查加速器配置是否生效，在命令行执行docker info ，如果从结果中看到了如下内容，说明配置成功。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">docker info</span></span><br><span class="line">......</span><br><span class="line">Registry Mirrors:</span><br><span class="line">   http://hub-mirror.c.163.com</span><br><span class="line">Live Restore Enabled: false</span><br></pre></td></tr></table></figure>












      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2020/08/15/14-Docker/02-Docker%E5%AE%89%E8%A3%85%E5%8F%8A%E5%8A%A0%E9%80%9F/" data-id="clmcxec8f001eu8wagi6u7isx" data-title="Docker安装及加速" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Docker/" rel="tag">Docker</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-14-Docker/03-Docker常用命令" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2020/08/15/14-Docker/03-Docker%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/" class="article-date">
  <time class="dt-published" datetime="2020-08-15T08:50:45.000Z" itemprop="datePublished">2020-08-15</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Docker/">Docker</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2020/08/15/14-Docker/03-Docker%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/">Docker常用命令</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="Docker-镜像操作"><a href="#Docker-镜像操作" class="headerlink" title="Docker 镜像操作"></a>Docker 镜像操作</h1><p>Docker 运行容器前需要本地存在对应的镜像，如果本地不存在该镜像，Docker 会从镜像仓库下载该镜像。</p>
<p>接下来将介绍关于镜像的内容，包括：</p>
<ul>
<li>从仓库获取镜像；</li>
<li>管理本地主机上的镜像；</li>
<li>介绍镜像实现的基本原理。</li>
</ul>
<h1 id="获取镜像"><a href="#获取镜像" class="headerlink" title="获取镜像"></a>获取镜像</h1><ul>
<li><p>从 Docker 镜像仓库获取镜像的命令是 docker pull 。其命令格式为：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker pull [选项] [Docker Registry 地址[:端口号]/]仓库名[:标签]</span><br></pre></td></tr></table></figure>
</li>
<li><p>具体的选项可以通过 docker pull –help 命令看到，这里我们说一下镜像名称的格式。</p>
<p>Docker 镜像仓库地址：地址的格式一般是 &lt;域名&#x2F;IP&gt;[:端口号] 。默认地址是 Docker Hub。</p>
<p>仓库名：如之前所说，这里的仓库名是两段式名称，即 &lt;用户名&gt;&#x2F;&lt;软件名&gt; 。对于 Docker Hub，如果不给出用户名，则默认为 library ，也就是官方镜像。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker pull ubuntu:16.04</span><br></pre></td></tr></table></figure>
</li>
<li><p>上面的命令中没有给出 Docker 镜像仓库地址，因此将会从 Docker Hub 获取镜像。而镜像名称是 ubuntu:16.04 ，因此将会获取官方镜像 library&#x2F;ubuntu 仓库中标签为 16.04 的镜像。</p>
</li>
</ul>
<h1 id="运行镜像"><a href="#运行镜像" class="headerlink" title="运行镜像"></a>运行镜像</h1><ul>
<li><p>有了镜像后，我们就能够以这个镜像为基础启动并运行一个容器。以上面的 ubuntu:16.04 为例，如果我们打算启动里面的 bash 并且进行交互式操作的话，可以执行下面的命令。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run -it --rm ubuntu:16.04 bash</span><br></pre></td></tr></table></figure>
</li>
<li><p>-it ：这是两个参数，一个是 -i ：交互式操作，一个是 -t 终端。</p>
<p>–rm ：这个参数是说容器退出后随之将其删除。</p>
<p>ubuntu:16.04 ：这是指用 ubuntu:16.04 镜像为基础来启动容器。</p>
<p>bash ：放在镜像名后的是命令，这里我们希望有个交互式 Shell，因此用的是 bash 。</p>
<p>最后我们通过 exit 退出了这个容器。</p>
</li>
</ul>
<h1 id="列出镜像"><a href="#列出镜像" class="headerlink" title="列出镜像"></a>列出镜像</h1><ul>
<li><p>要想列出已经下载下来的镜像，可以使用 docker image ls 命令。列表包含了 仓库名 、 标签 、 镜像 ID 、 创建时间 以及 所占用的空间 。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker image ls</span><br></pre></td></tr></table></figure>
</li>
<li><p>查看镜像、容器、数据卷所占用的空间。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker system df</span><br></pre></td></tr></table></figure>
</li>
<li><p>仓库名、标签均为 &lt; none&gt; 的镜像称为 虚悬镜像(dangling image) ，显示这类镜像</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker image ls -f dangling=true</span><br></pre></td></tr></table></figure>
</li>
<li><p>一般来说，虚悬镜像已经失去了存在的价值，是可以随意删除的，可以用下面的命令删除。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker image prune</span><br></pre></td></tr></table></figure></li>
</ul>
<h1 id="删除本地镜像"><a href="#删除本地镜像" class="headerlink" title="删除本地镜像"></a>删除本地镜像</h1><ul>
<li><p>如果要删除本地的镜像，可以使用 docker image rm 命令，其格式为：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker image rm [选项] &lt;镜像1&gt; [&lt;镜像2&gt; ...]</span><br></pre></td></tr></table></figure>

<p>其中， &lt;镜像&gt; 可以是 镜像短 ID 、 镜像长 ID 、 镜像名 或者 镜像摘要 。</p>
</li>
<li><p>使用 docker image ls -q 来配合 docker image rm ，这样可以批量删除希望删除的镜像。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker image rm $(docker image ls -q ubuntu) #删除所有仓库名为 ubuntu 的镜像</span><br></pre></td></tr></table></figure>
</li>
<li><p>或者删除所有在 ubuntu:16.04 之前的镜像：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker image rm $(docker image ls -q -f before=ubuntu:16.04)</span><br></pre></td></tr></table></figure></li>
</ul>
<h1 id="Docker-容器操作"><a href="#Docker-容器操作" class="headerlink" title="Docker 容器操作"></a>Docker 容器操作</h1><p>容器是独立运行的一个或一组应用，以及它们的运行态环境。对应的，虚拟机可以理解为模拟运行的一整套操作系统（提供了运行态环境和其他系统环境）和跑在上面的应用</p>
<p>接下来将具体介绍如何来管理一个容器，包括创建、启动和停止等。</p>
<h2 id="启动容器"><a href="#启动容器" class="headerlink" title="启动容器"></a>启动容器</h2><p>启动容器有两种方式，一种是基于镜像新建一个容器并启动，另外一个是将在终止状态（ stopped ）的容器重新启动。</p>
<p>因为 Docker 的容器实是轻量级的，用户可以随时删除和新创建容器。</p>
<ol>
<li><p>新建并启动</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run</span><br></pre></td></tr></table></figure>

<p>输出一个 “Hello World”，之后终止容器。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run ubuntu:16.04 /bin/echo &#x27;Hello world&#x27;</span><br></pre></td></tr></table></figure>
</li>
<li><p>启动已终止容器</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker container start 或者 docker start  </span><br></pre></td></tr></table></figure>

<p>启动一个 bash 终端，允许用户进行交互。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run -t -i ubuntu:16.04 /bin/bash </span><br></pre></td></tr></table></figure>

<p><strong>-t</strong> 让Docker分配一个伪终端并绑定到容器的标准输入上， <strong>-i</strong> 则让容器的标准输入保持打开。</p>
<p>当利用 docker run 来创建容器时，Docker 在后台运行的标准操作包括：</p>
<ul>
<li>检查本地是否存在指定的镜像，不存在就从公有仓库下载</li>
<li>利用镜像创建并启动一个容器</li>
<li>分配一个文件系统，并在只读的镜像层外面挂载一层可读写层</li>
<li>从宿主主机配置的网桥接口中桥接一个虚拟接口到容器中去</li>
<li>从地址池配置一个 ip 地址给容器</li>
<li>执行用户指定的应用程序</li>
<li>执行完毕后容器被终止</li>
</ul>
</li>
</ol>
<h2 id="后台运行"><a href="#后台运行" class="headerlink" title="后台运行"></a>后台运行</h2><p>很多时候，需要让 Docker 在后台运行而不是直接把执行命令的结果输出在当前宿主机下。此时，可以通过添加 -d 参数来实现。</p>
<p>如果不使用 -d 参数运行容器，比如 docker run hello-world 会把日志打印在控制台；</p>
<p>如果使用了 -d 参数运行容器，比如 docker run -d hello-world 不会输出日志，只会打印容器id(输出结果可以用 docker logs 查看)；</p>
<p>注： 容器是否会长久运行，是和 docker run 指定的命令有关，和 -d 参数无关。</p>
<h2 id="停止运行的容器"><a href="#停止运行的容器" class="headerlink" title="停止运行的容器"></a>停止运行的容器</h2><p>可以使用 docker container stop 来终止一个运行中的容器。</p>
<p>终止状态的容器可以用 docker container ls -a 命令看到。</p>
<p>处于终止状态的容器，可以通过 docker container start 命令来重新启动。</p>
<p>此外， docker container restart  命令会将一个运行态的容器终止，然后再重新启动它</p>
<h2 id="进入容器"><a href="#进入容器" class="headerlink" title="进入容器"></a>进入容器</h2><ul>
<li><p>在使用 -d 参数时，容器启动后会进入后台，某些时候需要进入容器进行操作，使用docker exec 命令可以进入到运行中。</p>
</li>
<li><p>exec 命令 -i -t 参数</p>
<p>docker exec 后边可以跟多个参数，这里主要说明 -i -t 参数。</p>
<p>只用 -i 参数时，由于没有分配伪终端，界面没有我们熟悉的 Linux 命令提示符，但命令执行结果仍然可以返回。当 -i -t 参数一起使用时，则可以看到我们熟悉的 Linux 命令提示符。</p>
</li>
</ul>
<h2 id="导出和导入容器"><a href="#导出和导入容器" class="headerlink" title="导出和导入容器"></a>导出和导入容器</h2><p><strong>导出容器</strong></p>
<p>如果要导出本地某个容器，可以使用 docker export 命令。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker export 容器ID &gt; 导出文件名.tar</span><br></pre></td></tr></table></figure>

<p><strong>导入容器</strong></p>
<ul>
<li><p>可以使用 docker import 从容器快照文件中再导入为镜像</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat 导出文件名.tar | docker import - 镜像用户/镜像名:镜像版本</span><br></pre></td></tr></table></figure>
</li>
<li><p>此外，也可以通过指定 URL 或者某个目录来导入</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker import http://study.163.com/image.tgz example/imagerepo</span><br></pre></td></tr></table></figure></li>
</ul>
<h2 id="删除容器"><a href="#删除容器" class="headerlink" title="删除容器"></a>删除容器</h2><p><strong>删除容器</strong></p>
<ul>
<li><p>可以使用 docker container rm 来删除一个处于终止状态的容器。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker container rm ubuntu:16.04</span><br></pre></td></tr></table></figure>

<p>如果要删除一个运行中的容器，可以添加 -f 参数。Docker 会发送 SIGKILL 信号给容器。</p>
</li>
</ul>
<p><strong>清理所有处于终止状态的容器</strong></p>
<ul>
<li><p>用 docker container ls -a 命令可以查看所有已经创建的包括终止状态的容器，如果数量太多要一个个删除可能会很麻烦，用下面的命令可以清理掉所有处于终止状态的容器。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker container prune</span><br></pre></td></tr></table></figure></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2020/08/15/14-Docker/03-Docker%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/" data-id="clmcxec8j001hu8wa90l1dzb8" data-title="Docker常用命令" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Docker/" rel="tag">Docker</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-14-Docker/04-利用DockerFile构建私有镜像" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2020/08/15/14-Docker/04-%E5%88%A9%E7%94%A8DockerFile%E6%9E%84%E5%BB%BA%E7%A7%81%E6%9C%89%E9%95%9C%E5%83%8F/" class="article-date">
  <time class="dt-published" datetime="2020-08-15T08:50:45.000Z" itemprop="datePublished">2020-08-15</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Docker/">Docker</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2020/08/15/14-Docker/04-%E5%88%A9%E7%94%A8DockerFile%E6%9E%84%E5%BB%BA%E7%A7%81%E6%9C%89%E9%95%9C%E5%83%8F/">利用DOckerfile构建私有镜像</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="使用-Dockerfile-定制镜像"><a href="#使用-Dockerfile-定制镜像" class="headerlink" title="使用 Dockerfile 定制镜像"></a>使用 Dockerfile 定制镜像</h1><p>镜像的定制实际上就是定制每一层所添加的配置、文件。我们可以把每一层修改、安装、构建、操作的命令都写入一个脚本，这个脚本就是 Dockerfile。</p>
<p>Dockerfile 是一个文本文件，其内包含了一条条的指令，每一条指令构建一层，因此每一条指令的内容，就是描述该层应当如何构建。</p>
<p>接下来我们以官方 nginx 镜像为例，使用 Dockerfile 来定制。</p>
<ul>
<li><p>在一个空白目录中，建立一个文本文件，并命名为 Dockerfile ：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mkdir mynginx</span><br><span class="line">cd mynginx</span><br><span class="line">touch Dockerfile</span><br></pre></td></tr></table></figure>

<p>其内容为：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">FROM nginx</span><br><span class="line">RUN echo &#x27;&lt;h1&gt;Hello, Docker!&lt;/h1&gt;&#x27; &gt; /usr/share/nginx/html/index.html</span><br></pre></td></tr></table></figure>

<p>这个 Dockerfile 很简单，一共就两行。涉及到了两条指令， FROM 和 RUN 。</p>
</li>
</ul>
<h2 id="FROM-指定基础镜像"><a href="#FROM-指定基础镜像" class="headerlink" title="FROM 指定基础镜像"></a>FROM 指定基础镜像</h2><p>所谓定制镜像，一定是以一个镜像为基础，在其上进行定制。基础镜像是必须指定的，而 FROM 就是指定基础镜像，因此一个 Dockerfile 中 FROM 是必备的指令，并且必须是第一条指令。</p>
<p>在 Docker Hub 上有非常多的高质量的官方镜像，有可以直接拿来使用的服务类的镜像，如nginx 、 redis 、 mysql 、 tomcat 等；可以在其中寻找一个最符合我们最终目标的镜像为基础镜像进行定制。</p>
<p>如果没有找到对应服务的镜像，官方镜像中还提供了一些更为基础的操作系统镜像，如ubuntu 、 debian 、 centos 、 alpine 等，这些操作系统的软件库为我们提供了更广阔的扩展空间。</p>
<p>除了选择现有镜像为基础镜像外，Docker 还存在一个特殊的镜像，名为 scratch 。这个镜像是虚拟的概念，并不实际存在，它表示一个空白的镜像。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">FROM scratch</span><br><span class="line">...</span><br></pre></td></tr></table></figure>

<p>如果你以 scratch 为基础镜像的话，意味着你不以任何镜像为基础，接下来所写的指令将作为镜像第一层开始存在。</p>
<p>对于 Linux 下静态编译的程序来说，并不需要有操作系统提供运行时支持，所需的一切库都已经在可执行文件里了，因此直接 FROM scratch 会让镜像体积更加小巧。使用 Go 语言 开发的应用很多会使用这种方式来制作镜像，这也是为什么有人认为 Go是特别适合容器微服务架构的语言的原因之一。</p>
<h2 id="RUN-执行命令"><a href="#RUN-执行命令" class="headerlink" title="RUN 执行命令"></a>RUN 执行命令</h2><p>RUN 指令是用来执行命令行命令的。由于命令行的强大能力， RUN 指令在定制镜像时是最常用的指令之一。其格式有两种：</p>
<p>shell 格式： RUN &lt;命令&gt;</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">RUN echo &#x27;&lt;h1&gt;Hello, Docker!&lt;/h1&gt;&#x27; &gt; /usr/share/nginx/html/index.html</span><br></pre></td></tr></table></figure>

<p>exec 格式： RUN [“可执行文件”, “参数1”, “参数2”] </p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">RUN tar -xzf redis.tar.gz -C /usr/src/redis --strip-components=1</span><br><span class="line">RUN make -C /usr/src/redis</span><br><span class="line">RUN make -C /usr/src/redis install</span><br></pre></td></tr></table></figure>

<h1 id="构建镜像"><a href="#构建镜像" class="headerlink" title="构建镜像"></a>构建镜像</h1><ul>
<li><p>上面我们利用 Dockerfile 定制了 nginx 镜像，现在我们明白了这个 Dockerfile 的内容，接下来我们来构建这个镜像。</p>
<p>在 Dockerfile 文件所在目录执行：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker build -t nginx:v3 .</span><br></pre></td></tr></table></figure>

<p>从命令的输出结果中，我们可以清晰的看到镜像的构建过程。在 Step 2 中，RUN 指令启动了一个容器 9cdc27646c7b ，执行了所要求的命令，并最后提交了这一层 44aa4490ce2c ，随后删除了所用到的这个容器 9cdc27646c7b 。</p>
</li>
</ul>
<h1 id="Dockerfile-指令详解"><a href="#Dockerfile-指令详解" class="headerlink" title="Dockerfile 指令详解"></a>Dockerfile 指令详解</h1><p>我们已经介绍了 FROM ， RUN ，其实 Dockerfile 功能很强大，它提供了十多个指令。</p>
<p>下面我们继续讲解其他的指令。</p>
<h2 id="COPY-复制文件"><a href="#COPY-复制文件" class="headerlink" title="COPY 复制文件"></a>COPY 复制文件</h2><p>格式：</p>
<ul>
<li>COPY &lt;源路径&gt;… &lt;目标路径&gt;</li>
<li>COPY [“&lt;源路径1&gt;”,… “&lt;目标路径&gt;”]</li>
</ul>
<p>COPY 指令将从构建上下文目录中 &lt;源路径&gt; 的文件&#x2F;目录复制到新的一层的镜像内的 &lt;目标路径&gt; 位置。比如：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">COPY package.json /usr/src/app/</span><br></pre></td></tr></table></figure>

<p>&lt;源路径&gt; 可以是多个，甚至可以是通配符，如：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">COPY hom* /mydir/</span><br><span class="line">COPY hom?.txt /mydir/</span><br></pre></td></tr></table></figure>

<h2 id="ADD-更高级的复制文件"><a href="#ADD-更高级的复制文件" class="headerlink" title="ADD 更高级的复制文件"></a>ADD 更高级的复制文件</h2><p>ADD 指令和 COPY 的格式和性质基本一致。但是在 COPY 基础上增加了一些功能。</p>
<p>比如 &lt;源路径&gt; 可以是一个 URL ，这种情况下，Docker 引擎会试图去下载这个链接的文件放到 &lt;目标路径&gt; 去。</p>
<p>在 Docker 官方的 Dockerfile 最佳实践文档 中要求，尽可能的使用 COPY ，因为 COPY 的语义很明确，就是复制文件而已，而 ADD 则包含了更复杂的功能，其行为也不一定很清晰。最适合使用 ADD 的场合，就是所提及的需要自动解压缩的场合。</p>
<p>因此在 COPY 和 ADD 指令中选择的时候，可以遵循这样的原则，所有的文件复制均使用COPY 指令，仅在需要自动解压缩的场合使用 ADD 。</p>
<h2 id="CMD-容器启动命令"><a href="#CMD-容器启动命令" class="headerlink" title="CMD 容器启动命令"></a>CMD 容器启动命令</h2><p>CMD 指令的格式和 RUN 相似，也是两种格式：</p>
<ul>
<li>shell 格式： CMD &lt;命令&gt;</li>
<li>exec 格式： CMD [“可执行文件”, “参数1”, “参数2”…]</li>
<li>参数列表格式： CMD [“参数1”, “参数2”…] 。在指定了 ENTRYPOINT 指令后，用 CMD 指定具体的参数。</li>
</ul>
<p>Docker 不是虚拟机，容器就是进程。既然是进程，那么在启动容器的时候，需要指定所运行的程序及参数。 CMD 指令就是用于指定默认的容器主进程的启动命令的。</p>
<h2 id="ENTRYPOINT-入口点"><a href="#ENTRYPOINT-入口点" class="headerlink" title="ENTRYPOINT 入口点"></a>ENTRYPOINT 入口点</h2><p>ENTRYPOINT 的目的和 CMD 一样，都是在指定容器启动程序及参数。 ENTRYPOINT 在运行时也可以替代，不过比 CMD 要略显繁琐，需要通过 docker run 的参数 –entrypoint 来指定。</p>
<p>当指定了 ENTRYPOINT 后， CMD 的含义就发生了改变，不再是直接的运行其命令，而是将 CMD 的内容作为参数传给 ENTRYPOINT 指令，换句话说实际执行时，将变为：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;ENTRYPOINT&gt; &quot;&lt;CMD&gt;&quot;</span><br></pre></td></tr></table></figure>

<h2 id="ENV-设置环境变量"><a href="#ENV-设置环境变量" class="headerlink" title="ENV 设置环境变量"></a>ENV 设置环境变量</h2><p>格式有两种：</p>
<ul>
<li>ENV &lt; key&gt; &lt; value&gt;</li>
<li>ENV &lt; key1&gt;&#x3D;&lt; value1&gt; &lt; key2&gt;&#x3D;&lt; value2&gt;…</li>
</ul>
<p>这个指令很简单，就是设置环境变量而已，无论是后面的其它指令，如 RUN ，还是运行时的应用，都可以直接使用这里定义的环境变量。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ENV VERSION=1.0 DEBUG=on NAME=&quot;Happy Feet&quot;</span><br><span class="line"><span class="meta prompt_">$</span><span class="language-bash">VERSION	<span class="comment">#使用环境变量</span></span></span><br></pre></td></tr></table></figure>

<p>下列指令可以支持环境变量展开：ADD 、 COPY 、 ENV 、 EXPOSE 、 LABEL 、 USER 、 WORKDIR 、 VOLUME 、 STOPSIGNAL 、 ONBUILD 。</p>
<h2 id="ARG-构建参数"><a href="#ARG-构建参数" class="headerlink" title="ARG 构建参数"></a>ARG 构建参数</h2><p>格式： </p>
<ul>
<li><p>ARG &lt;参数名&gt;[&#x3D;&lt;默认值&gt;]</p>
<p>构建参数和 ENV 的效果一样，都是设置环境变量。所不同的是， ARG 所设置的构建环境的环境变量，在将来容器运行时是不会存在这些环境变量的。但是不要因此就使用 ARG 保存密码之类的信息，因为 docker history 还是可以看到所有值的。</p>
</li>
</ul>
<p>Dockerfile 中的 ARG 指令是定义参数名称，以及定义其默认值。该默认值可以在构建命令<br>docker build 中用 –build-arg &lt;参数名&gt;&#x3D;&lt;值&gt; 来覆盖。</p>
<h2 id="VOLUME-定义匿名卷"><a href="#VOLUME-定义匿名卷" class="headerlink" title="VOLUME 定义匿名卷"></a>VOLUME 定义匿名卷</h2><p>格式为：</p>
<ul>
<li>VOLUME [“&lt;路径1&gt;”, “&lt;路径2&gt;”…]</li>
<li>VOLUME &lt;路径&gt;</li>
</ul>
<p>容器运行时应该尽量保持容器存储层不发生写操作，对于数据库类需要保存动态数据的应用，其数据库文件应该保存于卷(volume)中，为了防止运行时用户忘记将动态文件所保存目录挂载为卷，在Dockerfile 中，我们可以事先指定某些目录挂载为匿名卷，这样在运行时如果用户不指定挂载，其应用也可以正常运行，不会向容器存储层写入大量数据。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">VOLUME /data</span><br></pre></td></tr></table></figure>

<ul>
<li><p>这里的 &#x2F;data 目录就会在运行时自动挂载为匿名卷，任何向 &#x2F;data 中写入的信息都不会记录进容器存储层，从而保证了容器存储层的无状态化。当然，运行时可以覆盖这个挂载设置。</p>
<p>比如：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run -d -v mydata:/data xxxx</span><br></pre></td></tr></table></figure>

<p>在这行命令中，就使用了 mydata 这个命名卷挂载到了 &#x2F;data 这个位置，替代了 Dockerfile 中定义的匿名卷的挂载配置。</p>
</li>
</ul>
<h2 id="EXPOSE-声明端口"><a href="#EXPOSE-声明端口" class="headerlink" title="EXPOSE 声明端口"></a>EXPOSE 声明端口</h2><p>格式为 EXPOSE &lt;端口1&gt; [&lt;端口2&gt;…] 。</p>
<p>EXPOSE 指令是声明运行时容器提供服务端口，这只是一个声明，在运行时并不会因为这个声明应用就会开启这个端口的服务。</p>
<p>在 Dockerfile 中写入这样的声明有两个好处：</p>
<ol>
<li>是帮助镜像使用者理解这个镜像服务的守护端口，以方便配置映射；</li>
<li>在运行时使随机端口映射时，也就是 docker run -P 时，会自动随机映射 EXPOSE 的端口</li>
</ol>
<p>注意：要将 EXPOSE 和在运行时使用 -p &lt;宿主端口&gt;:&lt;容器端口&gt; 区分开来。 -p ，是映射宿主端口和容器端口，换句话说，就是将容器的对应端口服务公开给外界访问，而 EXPOSE 仅仅是声明容器打算使用什么端口而已，并不会自动在宿主进行端口映射。</p>
<h2 id="WORKDIR-指定工作目录"><a href="#WORKDIR-指定工作目录" class="headerlink" title="WORKDIR 指定工作目录"></a>WORKDIR 指定工作目录</h2><p>格式为 WORKDIR &lt;工作目录路径&gt; 。</p>
<p>使用 WORKDIR 指令可以来指定工作目录（或者称为当前目录），以后各层的当前目录就被改为指定的目录，如该目录不存在， WORKDIR 会帮你建立目录。</p>
<p>之前提到一些初学者常犯的错误是把 Dockerfile 等同于 Shell 脚本来书写，这种错误的理解还可能会导致出现下面这样的错误：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">RUN cd /app</span><br><span class="line">RUN echo &quot;hello&quot; &gt; world.txt</span><br></pre></td></tr></table></figure>

<p>如果将这个 Dockerfile 进行构建镜像运行后，会发现找不到 &#x2F;app&#x2F;world.txt 文件。</p>
<p><strong>原因</strong></p>
<ul>
<li><p>在 Shell 中，连续两行是同一个进程执行环境，因此前一个命令修改的内存状态，会直接影响后一个命令</p>
</li>
<li><p>而在 Dockerfile 中，这两行 RUN 命令的执行环境根本不同，是两个完全不同的容器。</p>
<p>这就是对 Dockerfile 构建分层存储的概念不了解所导致的错误。</p>
<p>每一个 RUN 都是启动一个容器、执行命令、然后提交存储层文件变更。</p>
<p>第一层 RUN cd &#x2F;app 的执行仅仅是当前进程的工作目录变更，一个内存上的变化而已，其结果不会造成任何文件变更。而到第二层的时候，启动的是一个全新的容器，跟第一层的容器更完全没关系，自然不可能继承前一层构建过程中的内存变化。</p>
<p>因此如果需要改变以后各层的工作目录的位置，那么应该使用 WORKDIR 指令。</p>
</li>
</ul>
<h2 id="USER-指定当前用户"><a href="#USER-指定当前用户" class="headerlink" title="USER 指定当前用户"></a>USER 指定当前用户</h2><p>格式： USER &lt;用户名&gt;</p>
<p>USER 指令和 WORKDIR 相似，都是改变环境状态并影响以后的层。 WORKDIR 是改变工作目录， USER 则是改变之后层的执行 RUN , CMD 以及 ENTRYPOINT 这类命令的身份。</p>
<p>当然，和 WORKDIR 一样， USER 只是帮助你切换到指定用户而已，这个用户必须是事先建立好的，否则无法切换。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">RUN groupadd -r redis &amp;&amp; useradd -r -g redis redis</span><br><span class="line">USER redis</span><br><span class="line">RUN [ &quot;redis-server&quot; ]</span><br></pre></td></tr></table></figure>

<h2 id="HEALTHCHECK-健康检查"><a href="#HEALTHCHECK-健康检查" class="headerlink" title="HEALTHCHECK 健康检查"></a>HEALTHCHECK 健康检查</h2><ul>
<li><p>格式：</p>
<p>HEALTHCHECK [选项] CMD &lt;命令&gt; ：设置检查容器健康状况的命令</p>
<p>HEALTHCHECK NONE ：如果基础镜像有健康检查指令，可以屏蔽掉其健康检查指令</p>
<p>HEALTHCHECK 指令是告诉 Docker 应该如何进行判断容器的状态是否正常，这是 Docker 1.12引入的新指令。通过该指令指定一行命令，用这行命令来判断容器主进程的服务状态是否还正常，从而比较真实的反应容器实际状态。</p>
<p>一个镜像指定了 HEALTHCHECK 指令后，用其启动容器，初始状态会为 starting ，在执行健康检查成功后变为 healthy ，如果连续一定次数失败，则会变为unhealthy 。</p>
<p>HEALTHCHECK 支持下列选项：</p>
<ul>
<li>–interval&#x3D;&lt;间隔&gt; ：两次健康检查的间隔，默认为 30 秒；</li>
<li>–timeout&#x3D;&lt;时长&gt; ：健康检查命令运行超时时间，如果超过这个时间，本次健康检查就被视为失败，默认 30 秒；</li>
<li>–retries&#x3D;&lt;次数&gt; ：当连续失败指定次数后，则将容器状态视为 unhealthy ，默认 3次。</li>
</ul>
<p>为了帮助排障，健康检查命令的输出（包括 stdout 以及 stderr ）都会被存储于健康状态里，可以用 docker inspect 来查看。</p>
</li>
</ul>
<h2 id="ONBUILD-为他人做嫁衣裳"><a href="#ONBUILD-为他人做嫁衣裳" class="headerlink" title="ONBUILD 为他人做嫁衣裳"></a>ONBUILD 为他人做嫁衣裳</h2><ul>
<li><p>格式： ONBUILD &lt;其它指令&gt; 。</p>
<p>ONBUILD 是一个特殊的指令，它后面跟的是其它指令，比如 RUN , COPY 等，而这些指令，</p>
<p>在当前镜像构建时并不会被执行。只有当以当前镜像为基础镜像，去构建下一级镜像的时候</p>
<p>才会被执行。</p>
<p>Dockerfile 中的其它指令都是为了定制当前镜像而准备的，唯有 ONBUILD 是为了帮助别人</p>
<p>定制自己而准备的。</p>
</li>
</ul>
<h1 id="其他制作镜像方式"><a href="#其他制作镜像方式" class="headerlink" title="其他制作镜像方式"></a>其他制作镜像方式</h1><h2 id="docker-save-和-docker-load"><a href="#docker-save-和-docker-load" class="headerlink" title="docker save 和 docker load"></a>docker save 和 docker load</h2><p>Docker 还提供了 docker load 和 docker save 命令，用以将镜像保存为一个 tar 文件，然后传输到另一个位置上，再加载进来。这是在没有 Docker Registry 时的做法，现在已经不推荐，镜像迁移应该直接使用 Docker Registry，无论是直接使用 Docker Hub 还是使用内网私有 Registry 都可以。</p>
<p>例如：保存 nginx 镜像</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker save nginx | gzip &gt; nginx-latest.tar.gz</span><br></pre></td></tr></table></figure>

<p>然后我们将 nginx-latest.tar.gz 文件复制到了到了另一个机器上，再次加载镜像：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker load -i nginx-latest.tar.gz</span><br></pre></td></tr></table></figure>


      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2020/08/15/14-Docker/04-%E5%88%A9%E7%94%A8DockerFile%E6%9E%84%E5%BB%BA%E7%A7%81%E6%9C%89%E9%95%9C%E5%83%8F/" data-id="clmcxec8u001ku8wae2fe2rcp" data-title="利用DOckerfile构建私有镜像" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Docker/" rel="tag">Docker</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-14-Docker/05-Docker运行Java程序" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2020/08/15/14-Docker/05-Docker%E8%BF%90%E8%A1%8CJava%E7%A8%8B%E5%BA%8F/" class="article-date">
  <time class="dt-published" datetime="2020-08-15T08:50:45.000Z" itemprop="datePublished">2020-08-15</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Docker/">Docker</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2020/08/15/14-Docker/05-Docker%E8%BF%90%E8%A1%8CJava%E7%A8%8B%E5%BA%8F/">Docker运行Java程序</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="定制镜像"><a href="#定制镜像" class="headerlink" title="定制镜像"></a>定制镜像</h1><ul>
<li><p>准备一个没有第三方依赖的 java web 项目，可以参考示例 maven 结构项目：</p>
</li>
<li><p>把该 war 上传到安装有 docker 软件的服务器上宿主目录下。在同级目录下创建 Dockerfile</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">touch Dockerfile</span><br><span class="line">vim Dockerfile</span><br></pre></td></tr></table></figure>
</li>
<li><p>按照前面章节所学的 Dockerfile 定制镜像知识来编写 Dockerfile 文件内容</p>
</li>
</ul>
<h1 id="Java-程序-Dockerfile"><a href="#Java-程序-Dockerfile" class="headerlink" title="Java 程序 Dockerfile"></a>Java 程序 Dockerfile</h1><ul>
<li><p>Dockerfile 文件内容如下：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">基础镜像使用tomcat:7.0.88-jre8</span></span><br><span class="line">FROM tomcat:7.0.88-jre8</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">作者</span></span><br><span class="line">MAINTAINER hash &lt;hash@163.com&gt;</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">定义环境变量</span></span><br><span class="line">ENV TOMCAT_BASE /usr/local/tomcat</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">复制war包</span></span><br><span class="line">COPY ./session-web.war $TOMCAT_BASE/webapps/</span><br></pre></td></tr></table></figure>
</li>
<li><p>执行构建</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker build -t session-web:latest </span><br></pre></td></tr></table></figure>
</li>
<li><p>如果构建成功，则会显示构建的分层信息及结果</p>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211010113232.png"></p>
</li>
<li><p>构建成功后使用 docker images 命令查看本地是否有该镜像</p>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211010113302.png"></p>
</li>
</ul>
<h1 id="运行镜像"><a href="#运行镜像" class="headerlink" title="运行镜像"></a>运行镜像</h1><ul>
<li><p>镜像制作好之后我们就要需要把它运行起来了</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run --name session-web -d -p 8888:8080 session-web:latest </span><br></pre></td></tr></table></figure>
</li>
<li><p>启动后使用 netstat -na | grep 8888 验证端口是否是在监听状态</p>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211010113348.png"></p>
</li>
<li><p>浏览器中访问 <a target="_blank" rel="noopener" href="http://ip:8888/session-web/user/login">http://ip:8888/session-web/user/login</a></p>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211010113415.png"></p>
</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2020/08/15/14-Docker/05-Docker%E8%BF%90%E8%A1%8CJava%E7%A8%8B%E5%BA%8F/" data-id="clmcxec8x001nu8wad7gs5nz6" data-title="Docker运行Java程序" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Docker/" rel="tag">Docker</a></li></ul>

    </footer>
  </div>
  
</article>



  


  <nav id="page-nav">
    
    <a class="extend prev" rel="prev" href="/page/15/">&laquo; Prev</a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/14/">14</a><a class="page-number" href="/page/15/">15</a><span class="page-number current">16</span><a class="page-number" href="/page/17/">17</a><a class="page-number" href="/page/18/">18</a><a class="page-number" href="/page/19/">19</a><a class="extend next" rel="next" href="/page/17/">Next &raquo;</a>
  </nav>

</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/ActiveMQ/">ActiveMQ</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Docker/">Docker</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Dubbo/">Dubbo</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Golang/">Golang</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Java%E5%9F%BA%E7%A1%80/">Java基础</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/K8s/">K8s</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Kafka/">Kafka</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/MQ/">MQ</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/MySQL/">MySQL</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Mybatis/">Mybatis</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Netty/">Netty</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/RPC/">RPC</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/RabbitMQ/">RabbitMQ</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Redis/">Redis</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Socker/">Socker</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Spring/">Spring</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/SpringBoot/">SpringBoot</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/SpringCloud/">SpringCloud</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Tomcat/">Tomcat</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/ZooKeeper/">ZooKeeper</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Zookeeper/">Zookeeper</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/">大数据</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/">数据结构</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E9%9D%A2%E8%AF%95%E9%A2%98/">面试题</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/ActiveMQ/" rel="tag">ActiveMQ</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/CDH/" rel="tag">CDH</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Docker/" rel="tag">Docker</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Dubbo/" rel="tag">Dubbo</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Golang/" rel="tag">Golang</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/HDFS/" rel="tag">HDFS</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/K8s/" rel="tag">K8s</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/KDTree/" rel="tag">KDTree</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Kafka/" rel="tag">Kafka</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/MQ/" rel="tag">MQ</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/MapReduce/" rel="tag">MapReduce</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/MySQL/" rel="tag">MySQL</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Mybatis/" rel="tag">Mybatis</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Netty/" rel="tag">Netty</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/RPC/" rel="tag">RPC</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/RabbitMQ/" rel="tag">RabbitMQ</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Redis/" rel="tag">Redis</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Socker/" rel="tag">Socker</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Spark/" rel="tag">Spark</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Spring/" rel="tag">Spring</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/SpringBoot/" rel="tag">SpringBoot</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/SpringCloud/" rel="tag">SpringCloud</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Tomcat/" rel="tag">Tomcat</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/YARN/" rel="tag">YARN</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ZooKeeper/" rel="tag">ZooKeeper</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%A4%9A%E7%BA%BF%E7%A8%8B/" rel="tag">多线程</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%B3%A8%E8%A7%A3/" rel="tag">注解</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/" rel="tag">设计模式</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E9%9B%86%E5%90%88%E6%BA%90%E7%A0%81/" rel="tag">集合源码</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E9%9D%A2%E8%AF%95%E9%A2%98/" rel="tag">面试题</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/ActiveMQ/" style="font-size: 10px;">ActiveMQ</a> <a href="/tags/CDH/" style="font-size: 10px;">CDH</a> <a href="/tags/Docker/" style="font-size: 18px;">Docker</a> <a href="/tags/Dubbo/" style="font-size: 13px;">Dubbo</a> <a href="/tags/Golang/" style="font-size: 19px;">Golang</a> <a href="/tags/HDFS/" style="font-size: 10px;">HDFS</a> <a href="/tags/K8s/" style="font-size: 10px;">K8s</a> <a href="/tags/KDTree/" style="font-size: 10px;">KDTree</a> <a href="/tags/Kafka/" style="font-size: 10px;">Kafka</a> <a href="/tags/MQ/" style="font-size: 10px;">MQ</a> <a href="/tags/MapReduce/" style="font-size: 10px;">MapReduce</a> <a href="/tags/MySQL/" style="font-size: 10px;">MySQL</a> <a href="/tags/Mybatis/" style="font-size: 14px;">Mybatis</a> <a href="/tags/Netty/" style="font-size: 14px;">Netty</a> <a href="/tags/RPC/" style="font-size: 11px;">RPC</a> <a href="/tags/RabbitMQ/" style="font-size: 15px;">RabbitMQ</a> <a href="/tags/Redis/" style="font-size: 17px;">Redis</a> <a href="/tags/Socker/" style="font-size: 10px;">Socker</a> <a href="/tags/Spark/" style="font-size: 11px;">Spark</a> <a href="/tags/Spring/" style="font-size: 20px;">Spring</a> <a href="/tags/SpringBoot/" style="font-size: 14px;">SpringBoot</a> <a href="/tags/SpringCloud/" style="font-size: 10px;">SpringCloud</a> <a href="/tags/Tomcat/" style="font-size: 13px;">Tomcat</a> <a href="/tags/YARN/" style="font-size: 11px;">YARN</a> <a href="/tags/ZooKeeper/" style="font-size: 12px;">ZooKeeper</a> <a href="/tags/%E5%A4%9A%E7%BA%BF%E7%A8%8B/" style="font-size: 16px;">多线程</a> <a href="/tags/%E6%B3%A8%E8%A7%A3/" style="font-size: 10px;">注解</a> <a href="/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/" style="font-size: 11px;">设计模式</a> <a href="/tags/%E9%9B%86%E5%90%88%E6%BA%90%E7%A0%81/" style="font-size: 13px;">集合源码</a> <a href="/tags/%E9%9D%A2%E8%AF%95%E9%A2%98/" style="font-size: 10px;">面试题</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/01/">January 2022</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/12/">December 2021</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/05/">May 2021</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/04/">April 2021</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/03/">March 2021</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/02/">February 2021</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/01/">January 2021</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/12/">December 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/11/">November 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/10/">October 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/08/">August 2020</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2023/09/10/11-MySQL/04-MySQL%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%8A%E8%AF%BB%E5%86%99%E5%88%86%E7%A6%BB/05-%E9%AB%98%E5%8F%AF%E7%94%A8%E6%95%B0%E6%8D%AE%E5%BA%93%E9%9B%86%E7%BE%A4%E5%AE%89%E8%A3%85%E6%90%AD%E5%BB%BA%E6%89%8B%E5%86%8C/">(no title)</a>
          </li>
        
          <li>
            <a href="/2023/09/10/11-MySQL/04-MySQL%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%8A%E8%AF%BB%E5%86%99%E5%88%86%E7%A6%BB/04-Sharding-JDBC%E8%AF%BB%E5%86%99%E5%88%86%E7%A6%BB/">(no title)</a>
          </li>
        
          <li>
            <a href="/2023/09/10/11-MySQL/04-MySQL%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%8A%E8%AF%BB%E5%86%99%E5%88%86%E7%A6%BB/03-MyCat%E8%AF%BB%E5%86%99%E5%88%86%E7%A6%BB/">(no title)</a>
          </li>
        
          <li>
            <a href="/2023/09/10/11-MySQL/04-MySQL%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%8A%E8%AF%BB%E5%86%99%E5%88%86%E7%A6%BB/02-MySQL%E4%B8%BB%E4%BB%8E%E5%A4%8D%E5%88%B6/">(no title)</a>
          </li>
        
          <li>
            <a href="/2023/09/10/11-MySQL/04-MySQL%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%8A%E8%AF%BB%E5%86%99%E5%88%86%E7%A6%BB/01-MySQL%E4%B8%BB%E4%BB%8E%E5%A4%8D%E5%88%B6%E5%92%8C%E9%AB%98%E5%8F%AF%E7%94%A8/">(no title)</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2023 John Doe<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.6.4.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
</body>
</html>