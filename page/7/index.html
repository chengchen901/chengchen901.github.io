<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta property="og:type" content="website">
<meta property="og:title" content="Hexo">
<meta property="og:url" content="http://example.com/page/7/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/typeface-source-code-pro@0.0.71/index.min.css">

  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
  
<meta name="generator" content="Hexo 6.3.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hexo</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"><span class="fa fa-bars"></span></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
        
          <a class="nav-icon" href="/atom.xml" title="RSS Feed"><span class="fa fa-rss"></span></a>
        
        <a class="nav-icon nav-search-btn" title="Search"><span class="fa fa-search"></span></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://example.com"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main">
  
    <article id="post-16-Nginx/02-01-制作第一张自己的SSL证书" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/09/10/16-Nginx/02-01-%E5%88%B6%E4%BD%9C%E7%AC%AC%E4%B8%80%E5%BC%A0%E8%87%AA%E5%B7%B1%E7%9A%84SSL%E8%AF%81%E4%B9%A6/" class="article-date">
  <time class="dt-published" datetime="2023-09-10T03:59:07.780Z" itemprop="datePublished">2023-09-10</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="制作第一张自己的SSL证书"><a href="#制作第一张自己的SSL证书" class="headerlink" title="制作第一张自己的SSL证书"></a>制作第一张自己的SSL证书</h1><h2 id="准备工具"><a href="#准备工具" class="headerlink" title="准备工具"></a>准备工具</h2><p>安装keytool（jdk自带）、openssl、nginx、web服务</p>
<p>有两种方法生成数字证书，一种是用JDK带的keytool，另一种是用openssl。</p>
<p>我们将利用openssl完成以下表格内容：</p>
<table>
<thead>
<tr>
<th>完成项</th>
<th>输出文件</th>
</tr>
</thead>
<tbody><tr>
<td>CA服务器根证书</td>
<td>cacert.pem 证书</td>
</tr>
<tr>
<td>服务器证书</td>
<td>servercert.pem 证书<br />serverkey.pem 私钥</td>
</tr>
<tr>
<td>客户端证书</td>
<td>clientcert.pem 证书<br />clientkey.pem 私钥</td>
</tr>
</tbody></table>
<p>先做一个服务端单向认证，最后完成一个服务端客户端双向认证。</p>
<h2 id="制作根证书"><a href="#制作根证书" class="headerlink" title="制作根证书"></a>制作根证书</h2><p>制作一个CA私服，取名叫testca。</p>
<h3 id="准备CA工作目录"><a href="#准备CA工作目录" class="headerlink" title="准备CA工作目录"></a>准备CA工作目录</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">mkdir &quot;$HOME/testca&quot;</span><br><span class="line">cd &quot;$HOME/testca&quot;			</span><br><span class="line">mkdir newcerts private conf</span><br><span class="line">chmod g-rwx,o-rwx private</span><br><span class="line">echo &quot;01&quot; &gt; serial</span><br><span class="line">touch index.txt</span><br></pre></td></tr></table></figure>

<p>$HOME&#x2F;testca为待建CA的主目录</p>
<p>newcerts子目录将存放CA签署（颁发）过的数字证书（证书备份目录）</p>
<p>private目录用于存放CA的私钥</p>
<p>conf只是用于存放一些简化参数用的配置文件</p>
<p>serial和index.txt分别用于存放下一个证书的序列号和证书信息数据库</p>
<h3 id="生成根证书"><a href="#生成根证书" class="headerlink" title="生成根证书"></a>生成根证书</h3><h4 id="配置根证书"><a href="#配置根证书" class="headerlink" title="配置根证书"></a>配置根证书</h4><p>创建testca根证书配置文件及内容</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vi &quot;$HOME/testca/conf/gentestca.conf&quot;</span><br></pre></td></tr></table></figure>

<p>内容如下</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">###################################</span></span></span><br><span class="line">[ req ]</span><br><span class="line">default_keyfile = $ENV::HOME/testca/private/cakey.pem</span><br><span class="line">default_md = md5</span><br><span class="line">prompt = no</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">对应下面[ ca_distinguished_name ]</span></span><br><span class="line">distinguished_name = ca_distinguished_name</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">对应下面[ ca_extensions ]</span></span><br><span class="line">x509_extensions = ca_extensions</span><br><span class="line"> </span><br><span class="line">[ ca_distinguished_name ]</span><br><span class="line">organizationName = hashstudy</span><br><span class="line">organizationalUnitName  = hash	</span><br><span class="line">commonName = ca.hash.com</span><br><span class="line">emailAddress = 123456789@163.com</span><br><span class="line"> </span><br><span class="line">[ ca_extensions ]</span><br><span class="line">basicConstraints = CA:true</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">#######################################</span></span></span><br></pre></td></tr></table></figure>

<h4 id="申请根证书"><a href="#申请根证书" class="headerlink" title="申请根证书"></a>申请根证书</h4><p>CA服务器，自己给自己颁发证书，生成CA的私钥和自签名证书，即根证书。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">cd &quot;$HOME/testca&quot;</span><br><span class="line">openssl req -x509 -newkey rsa:2048 -out cacert.pem -outform PEM -days 2190 -config &quot;$HOME/testca/conf/gentestca.conf&quot;</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">req 表示发起一个证书签名请求</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">-x509 用x509结构替代cert</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">-nwekey rsa:2048，新建一个2048 bit的rsa秘钥</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">-out 输出的证书文件</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">-outform 输出的格式，DER或者PEM</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">-days 有效时间2190天</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">-config 请求的配置文件，这里是根证书的配置信息</span></span><br></pre></td></tr></table></figure>

<p>执行过程中需要输入CA私钥的保护密码，假设我们输入密码： 123456</p>
<p>查看一下自己CA证书的内容</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">openssl x509 -in cacert.pem -text -noout</span><br></pre></td></tr></table></figure>

<h4 id="准备根证书配置文件"><a href="#准备根证书配置文件" class="headerlink" title="准备根证书配置文件"></a>准备根证书配置文件</h4><p>方便以后用它来生成，在它下面的子证书</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vi &quot;$HOME/testca/conf/testca.conf&quot;</span><br></pre></td></tr></table></figure>

<p>写入文件内容</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">###################################</span></span></span><br><span class="line">[ ca ]</span><br><span class="line">default_ca      = testca                   # The default ca section</span><br><span class="line"> </span><br><span class="line">[ testca ]</span><br><span class="line">dir            = $ENV::HOME/testca         # top dir</span><br><span class="line">database       = $dir/index.txt          # index file.</span><br><span class="line">new_certs_dir  = $dir/newcerts           # new certs dir</span><br><span class="line"> </span><br><span class="line">certificate    = $dir/cacert.pem         # The CA cert</span><br><span class="line">serial         = $dir/serial             # serial no file</span><br><span class="line">private_key    = $dir/private/cakey.pem  # CA private key</span><br><span class="line">RANDFILE       = $dir/private/.rand      # random number file</span><br><span class="line"> </span><br><span class="line">default_days   = 365                     # how long to certify for</span><br><span class="line">default_crl_days= 30                     # how long before next CRL</span><br><span class="line">default_md     = md5                     # message digest method to use</span><br><span class="line">unique_subject = no                      # Set to &#x27;no&#x27; to allow creation of</span><br><span class="line">                                         # several ctificates with same subject.</span><br><span class="line">policy         = policy_any              # default policy</span><br><span class="line"> </span><br><span class="line">[ policy_any ]</span><br><span class="line">countryName             = optional</span><br><span class="line">stateOrProvinceName     = optional</span><br><span class="line">localityName            = optional</span><br><span class="line">organizationName        = optional</span><br><span class="line">organizationalUnitName  = optional</span><br><span class="line">commonName              = supplied</span><br><span class="line">emailAddress            = optional</span><br><span class="line"><span class="meta prompt_"> </span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">#######################################</span></span></span><br></pre></td></tr></table></figure>



<h2 id="单向认证"><a href="#单向认证" class="headerlink" title="单向认证"></a>单向认证</h2><p>我们可以用openssl为服务器或用户生成公钥密钥，并用上面创建的CA根证书cacert.pem签发对应的私钥（密钥）的数字证书。</p>
<h3 id="准备目录"><a href="#准备目录" class="headerlink" title="准备目录"></a>准备目录</h3><p>我们把服务器相关的东西生成到CA的$HOME&#x2F;testca&#x2F;test&#x2F;server目录里。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p &quot;$HOME/testca/test/server&quot;</span><br><span class="line">cd &quot;$HOME/testca/test/server&quot;</span><br></pre></td></tr></table></figure>

<h3 id="创建服务器私钥，并生成testca的证书请求文件"><a href="#创建服务器私钥，并生成testca的证书请求文件" class="headerlink" title="创建服务器私钥，并生成testca的证书请求文件"></a>创建服务器私钥，并生成testca的证书请求文件</h3><p>通过req命令，生成一个签名证书申请，使用rsa:1024生成新秘钥，申请证书为testkey.pem，格式为PEM，证书签名参数为-subj指定的内容</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">openssl req -newkey rsa:1024 -out serverreq.pem -keyout serverkey.pem -keyform PEM -outform PEM  -subj &quot;/O=ABCom/OU=servers/CN=servername&quot;</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">req 表示发起一个证书签名请求</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">-nwekey rsa:1024，新建一个1024 bit的rsa秘钥</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">-out 输出的证书请求文件</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">-keyout 表示存放生成私钥的文件</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">-keyform 表示生成的秘钥文件格式，这里是PEM</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">-outform 输出的格式，DER或者PEM</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">-subj 设置或修改请求证书签名主题</span></span><br></pre></td></tr></table></figure>

<p>执行命令过程中输入密钥保护密码，我们输入：949494</p>
<p>serverkey.pem为的私钥，serverreq.pem为CA签名证书请求文件。</p>
<p>查看请求文件内容</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">openssl req -in serverreq.pem -text -noout</span><br></pre></td></tr></table></figure>

<h3 id="CA签发证书"><a href="#CA签发证书" class="headerlink" title="CA签发证书"></a>CA签发证书</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">openssl ca -in serverreq.pem -out servercert.pem -config &quot;$HOME/testca/conf/testca.conf&quot;</span><br></pre></td></tr></table></figure>

<p>执行过程中需要输入CA私钥的保护密码，前面设置的123456。</p>
<p>查看证书内容</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">openssl x509 -in servercert.pem -text -noout</span><br></pre></td></tr></table></figure>

<h3 id="Nginx配置"><a href="#Nginx配置" class="headerlink" title="Nginx配置"></a>Nginx配置</h3><p>找到nginx运行的配置文件，将证书servercert.pem及私钥serverkey.pem放到配置目录下，修改内容如下</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">http <span class="punctuation">&#123;</span></span><br><span class="line">    # 配置https服务</span><br><span class="line">    server <span class="punctuation">&#123;</span></span><br><span class="line">        # 开启<span class="number">443</span>端口，ssl安全协议</span><br><span class="line">        listen       <span class="number">443</span> ssl;</span><br><span class="line">        server_name  localhost;</span><br><span class="line"></span><br><span class="line">        # 配置证书及服务器私钥</span><br><span class="line">        ssl_certificate      servercert.pem;</span><br><span class="line">        ssl_certificate_key  serverkey.pem;</span><br><span class="line"></span><br><span class="line">        # 会话参数的缓存<span class="punctuation">,</span>所有工作进程之间共享的缓存</span><br><span class="line">        ssl_session_cache    shared<span class="punctuation">:</span>SSL<span class="punctuation">:</span><span class="number">1</span>m;</span><br><span class="line">        ssl_session_timeout  <span class="number">5</span>m;</span><br><span class="line">        ssl_protocols TLSv1 TLSv1<span class="number">.1</span> TLSv1<span class="number">.2</span>;</span><br><span class="line"></span><br><span class="line">        # 启用的密码，openssl -help后，有一个ciphers命令，openssl ciphers可以看到支持的加密算法</span><br><span class="line">        ssl_ciphers  ECDHE-RSA-AES128-GCM-SHA256<span class="punctuation">:</span>HIGH<span class="punctuation">:</span>!aNULL<span class="punctuation">:</span>!MD5<span class="punctuation">:</span>!RC4<span class="punctuation">:</span>!DHE;</span><br><span class="line">        # SSLv3和TLS协议时<span class="punctuation">,</span>服务器密码优先于客户端密码</span><br><span class="line">        ssl_prefer_server_ciphers  on;</span><br><span class="line"></span><br><span class="line">        location / <span class="punctuation">&#123;</span></span><br><span class="line">            root   /data/www/;</span><br><span class="line">            index  welcome.html;</span><br><span class="line">        <span class="punctuation">&#125;</span></span><br><span class="line">    <span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>

<figure class="highlight nginx"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">worker_processes</span> <span class="number">2</span>;</span><br><span class="line">events&#123;</span><br><span class="line">    <span class="attribute">worker_connections</span> <span class="number">1024</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="section">http</span> &#123;</span><br><span class="line">    <span class="section">upstream</span> tomcatServer&#123;</span><br><span class="line">        <span class="attribute">server</span> <span class="number">192.168.254.174:8080</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment"># 配置https服务</span></span><br><span class="line">    <span class="section">server</span> &#123;</span><br><span class="line">        <span class="comment"># 开启443端口，ssl安全协议</span></span><br><span class="line">        <span class="attribute">listen</span>       <span class="number">443</span> ssl;</span><br><span class="line">        <span class="attribute">server_name</span>  localhost;</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 配置证书及服务器私钥</span></span><br><span class="line">        <span class="attribute">ssl_certificate</span>      /root/testca/test/server/servercert.pem;</span><br><span class="line">        <span class="attribute">ssl_certificate_key</span>  /root/testca/test/server/serverkey.pem;</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 会话参数的缓存,所有工作进程之间共享的缓存</span></span><br><span class="line">        <span class="attribute">ssl_session_cache</span>    shared:SSL:<span class="number">1m</span>;</span><br><span class="line">        <span class="attribute">ssl_session_timeout</span>  <span class="number">5m</span>;</span><br><span class="line">        <span class="attribute">ssl_protocols</span> TLSv1 TLSv1.<span class="number">1</span> TLSv1.<span class="number">2</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 启用的密码，openssl -help后，有一个ciphers命令，openssl ciphers可以看到支持的加密算法</span></span><br><span class="line">        <span class="attribute">ssl_ciphers</span>  ECDHE-RSA-AES128-GCM-SHA256:HIGH:!aNULL:!MD5:!RC4:!DHE;</span><br><span class="line">        <span class="comment"># SSLv3和TLS协议时,服务器密码优先于客户端密码</span></span><br><span class="line">        <span class="attribute">ssl_prefer_server_ciphers</span>  <span class="literal">on</span>;</span><br><span class="line"></span><br><span class="line">        <span class="section">location</span> / &#123;</span><br><span class="line">            <span class="attribute">proxy_pass</span> http://tomcatServer;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h2 id="双向认证"><a href="#双向认证" class="headerlink" title="双向认证"></a>双向认证</h2><p>前面我们已经制作了服务器的证书，下面我只需要制作客户端证书就可以了。</p>
<h3 id="创建客户端证书"><a href="#创建客户端证书" class="headerlink" title="创建客户端证书"></a>创建客户端证书</h3><p>用openssl创建客户端证书，只有这些CA签发的客户端证书才被服务器信任，才能通过HTTPS访问服务器。这就是“HTTPS服务器验证客户端证书”的关键配置。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p &quot;$HOME/testca/test/client&quot;</span><br><span class="line">cd &quot;$HOME/testca/test/client&quot;</span><br><span class="line"></span><br><span class="line">openssl req -newkey rsa:1024 -keyout clientkey.pem -keyform PEM -out clientreq.pem -outform PEM  -subj &quot;/O=client/OU=client/CN=client&quot;</span><br><span class="line"></span><br><span class="line">openssl ca -in clientreq.pem -out clientcert.pem -config &quot;$HOME/testca/conf/testca.conf&quot;</span><br></pre></td></tr></table></figure>

<p>设定执行过程中的密码都是：654321</p>
<h3 id="制作PKCS12格式的客户端证书"><a href="#制作PKCS12格式的客户端证书" class="headerlink" title="制作PKCS12格式的客户端证书"></a>制作PKCS12格式的客户端证书</h3><p>PKCS12格式的证书包含私钥和公钥内容，它是描述个人信息交换的语法标准。描述了将用户<a target="_blank" rel="noopener" href="https://baike.baidu.com/item/%E5%85%AC%E9%92%A5">公钥</a>、私钥、证书和其他相关信息打包的语法或格式。我们制作的这个PKCS#12文件将包含密钥、证书和颁发该证书的CA证书。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">openssl pkcs12 -export -in clientcert.pem -inkey clientkey.pem -out client.p12 -name hash -CAfile &quot;$HOME/testca/cacert.pem&quot;</span><br></pre></td></tr></table></figure>

<p>我们将导出密码也设置为：654321</p>
<p>将生成的.p12文件导入到浏览器个人证书列表或USB秘钥U盘中，客户端就能用它来通过服务的验证。</p>
<h3 id="Nginx配置-1"><a href="#Nginx配置-1" class="headerlink" title="Nginx配置"></a>Nginx配置</h3><p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/yelao/p/9486882.html">参考资料</a></p>
<p>找到nginx运行的配置文件，将服务器证书servercert.pem、钥serverkey.pem、ca根证书cacert.pem放到配置目录下，修改内容如下</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">http <span class="punctuation">&#123;</span></span><br><span class="line">    # 配置https服务</span><br><span class="line">    server <span class="punctuation">&#123;</span></span><br><span class="line">        # 开启<span class="number">443</span>端口，ssl安全协议</span><br><span class="line">        listen       <span class="number">443</span> ssl;</span><br><span class="line">        server_name  localhost;</span><br><span class="line"></span><br><span class="line">        # 配置证书及服务器私钥</span><br><span class="line">        ssl_certificate      servercert.pem;</span><br><span class="line">        ssl_certificate_key  serverkey.pem;</span><br><span class="line">    	</span><br><span class="line">    	</span><br><span class="line">        ssl_client_certificate  cacert.pem;	# 根级证书公钥，用于验证各个二级client</span><br><span class="line">    	ssl_verify_client on;	#开启客户端证书验证</span><br><span class="line"></span><br><span class="line">        # 会话参数的缓存<span class="punctuation">,</span>所有工作进程之间共享的缓存</span><br><span class="line">        ssl_session_cache    shared<span class="punctuation">:</span>SSL<span class="punctuation">:</span><span class="number">1</span>m;</span><br><span class="line">        ssl_session_timeout  <span class="number">5</span>m;</span><br><span class="line">        ssl_protocols TLSv1 TLSv1<span class="number">.1</span> TLSv1<span class="number">.2</span>;</span><br><span class="line"></span><br><span class="line">        # 启用的密码</span><br><span class="line">        ssl_ciphers  ECDHE-RSA-AES128-GCM-SHA256<span class="punctuation">:</span>HIGH<span class="punctuation">:</span>!aNULL<span class="punctuation">:</span>!MD5<span class="punctuation">:</span>!RC4<span class="punctuation">:</span>!DHE;</span><br><span class="line">        # SSLv3和TLS协议时<span class="punctuation">,</span>服务器密码优先于客户端密码</span><br><span class="line">        ssl_prefer_server_ciphers  on;</span><br><span class="line"></span><br><span class="line">        location / <span class="punctuation">&#123;</span></span><br><span class="line">            root   /data/www/;</span><br><span class="line">            index  welcome.html;</span><br><span class="line">        <span class="punctuation">&#125;</span></span><br><span class="line">    <span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>

<figure class="highlight nginx"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">worker_processes</span> <span class="number">2</span>;</span><br><span class="line">events&#123;</span><br><span class="line">    <span class="attribute">worker_connections</span> <span class="number">1024</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="section">http</span> &#123;</span><br><span class="line">    <span class="section">upstream</span> tomcatServer&#123;</span><br><span class="line">        <span class="attribute">server</span> <span class="number">192.168.254.174:8080</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment"># 配置https服务</span></span><br><span class="line">    <span class="section">server</span> &#123;</span><br><span class="line">        <span class="comment"># 开启443端口，ssl安全协议</span></span><br><span class="line">        <span class="attribute">listen</span>       <span class="number">443</span> ssl;</span><br><span class="line">        <span class="attribute">server_name</span>  localhost;</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 配置证书及服务器私钥</span></span><br><span class="line">        <span class="attribute">ssl_certificate</span>      /root/testca/test/server/servercert.pem;</span><br><span class="line">        <span class="attribute">ssl_certificate_key</span>  /root/testca/test/server/serverkey.pem;</span><br><span class="line"></span><br><span class="line">        <span class="attribute">ssl_client_certificate</span>  /root/testca/cacert.pem; <span class="comment"># 根级证书公钥，用于验证各个二级client</span></span><br><span class="line">        <span class="attribute">ssl_verify_client</span> <span class="literal">on</span>;   <span class="comment">#开启客户端证书验证</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 会话参数的缓存,所有工作进程之间共享的缓存</span></span><br><span class="line">        <span class="attribute">ssl_session_cache</span>    shared:SSL:<span class="number">1m</span>;</span><br><span class="line">        <span class="attribute">ssl_session_timeout</span>  <span class="number">5m</span>;</span><br><span class="line">        <span class="attribute">ssl_protocols</span> TLSv1 TLSv1.<span class="number">1</span> TLSv1.<span class="number">2</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 启用的密码，openssl -help后，有一个ciphers命令，openssl ciphers可以看到支持的加密算法</span></span><br><span class="line">        <span class="attribute">ssl_ciphers</span>  ECDHE-RSA-AES128-GCM-SHA256:HIGH:!aNULL:!MD5:!RC4:!DHE;</span><br><span class="line">        <span class="comment"># SSLv3和TLS协议时,服务器密码优先于客户端密码</span></span><br><span class="line">        <span class="attribute">ssl_prefer_server_ciphers</span>  <span class="literal">on</span>;</span><br><span class="line"></span><br><span class="line">        <span class="section">location</span> / &#123;</span><br><span class="line">            <span class="attribute">proxy_pass</span> http://tomcatServer;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h2 id="CA的日常操作"><a href="#CA的日常操作" class="headerlink" title="CA的日常操作"></a>CA的日常操作</h2><h3 id="签发证书"><a href="#签发证书" class="headerlink" title="签发证书"></a>签发证书</h3><p>假设收到一个证书请求文件名为req.pem，文件格式应该是PKCS#10格式（标准证书请求格式）</p>
<p>先查看证书请求的内容</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">openssl req -in req.pem -text -noout</span><br></pre></td></tr></table></figure>

<p>签发证书</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">openssl ca -in req.pem -out cert.pem -config &quot;$HOME/testca/conf/testca.conf&quot;</span><br></pre></td></tr></table></figure>

<p>执行过程中会要求输入访问CA的私钥密码，前面设置的123456</p>
<p>命令执行完毕，cert.pem就是签发好的证书，在$HOME&#x2F;testca&#x2F;newcerts里也会有一个相同的证书副本，文件名为证书序列号。</p>
<p>查看生成的证书的内容</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">openssl x509 -in cert.pem -text -noout</span><br></pre></td></tr></table></figure>

<h3 id="作废证书"><a href="#作废证书" class="headerlink" title="作废证书"></a>作废证书</h3><p>由于用户私钥泄露或其他情况，需要吊销一个未过期的证书。假设需要被吊销的证书文件为cert.pem，则执行以下命令吊销证书：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">openssl ca -revoke cert.pem -config &quot;$HOME/testca/conf/testca.conf&quot;</span><br></pre></td></tr></table></figure>

<h3 id="生成证书作废列表CRL"><a href="#生成证书作废列表CRL" class="headerlink" title="生成证书作废列表CRL"></a>生成证书作废列表CRL</h3><p>公开被吊销的证书列表，可以生成证书吊销列表（CRL），执行命令如下</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">openssl ca -gencrl -out testca.crl -config &quot;$HOME/testca/conf/testca.conf&quot;</span><br></pre></td></tr></table></figure>

<p>还可用-crldays和-crlhours参数来说明下一个吊销列表将在未来某个时候（多少天或多少小时后）发布。</p>
<p>查看检查testca.crl的内容</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">openssl crl -in testca.crl -text -noout</span><br></pre></td></tr></table></figure>

<p>服务端如何检查该客户端证书是否已经被吊销？我们可以通过检查CRL（Certification Revocation List）即证书吊销列表来做这个工作。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2023/09/10/16-Nginx/02-01-%E5%88%B6%E4%BD%9C%E7%AC%AC%E4%B8%80%E5%BC%A0%E8%87%AA%E5%B7%B1%E7%9A%84SSL%E8%AF%81%E4%B9%A6/" data-id="clmcxecbl002mu8wa4w4cfpzd" data-title="" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-16-Nginx/02-00-Nginx的WEB安全体系" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/09/10/16-Nginx/02-00-Nginx%E7%9A%84WEB%E5%AE%89%E5%85%A8%E4%BD%93%E7%B3%BB/" class="article-date">
  <time class="dt-published" datetime="2023-09-10T03:59:07.774Z" itemprop="datePublished">2023-09-10</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="数据加密技术简述"><a href="#数据加密技术简述" class="headerlink" title="数据加密技术简述"></a>数据加密技术简述</h1><h2 id="Http安全如何？"><a href="#Http安全如何？" class="headerlink" title="Http安全如何？"></a>Http安全如何？</h2><p><strong>HTTP</strong></p>
<p>HTTP协议，即超文本传输协议(Hypertext transfer protocol)。是一种详细规定了浏览器和万维网</p>
<p>(WWW &#x3D; World Wide Web)服务器之间互相通信的规则，通过因特网传送万维网文档的数据传送协议。</p>
<p>明文传输，容易泄密，容易被拦截</p>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20220829213059.png"></p>
<p>如何解决Http的这个安全问题？——加密技术</p>
<h2 id="摘要算法"><a href="#摘要算法" class="headerlink" title="摘要算法"></a>摘要算法</h2><p><strong>摘要算法</strong></p>
<p>JDK8支持的加密种类：<a target="_blank" rel="noopener" href="https://docs.oracle.com/javase/8/docs/technotes/guides/security/StandardNames.html#MessageDigest">https://docs.oracle.com/javase/8/docs/technotes/guides/security/StandardNames.html#MessageDigest</a></p>
<p>消息摘要算法用来验证数据的完整性，分为三类</p>
<p>MD(Message Digest)：消息摘要</p>
<p>SHA(Secure Hash Algorithm)：安全散列</p>
<p>MAC(Message Authentication Code)：消息认证码</p>
<p>最常见的登录密码加密处理，MD5加密，MD5属于消息摘要类。</p>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20220829213158.png"></p>
<p>可以用来保证内容不被篡改：<strong>密码加密、文件校验码</strong></p>
<h2 id="对称加密"><a href="#对称加密" class="headerlink" title="对称加密"></a>对称加密</h2><p><strong>对称加密</strong></p>
<p>DES(Data Encryption Standard，数据加密算法) </p>
<p>PBE(Password-based encryption，基于密码验证) </p>
<p>AES(Advanced Encryption Standard，高级加密标准)</p>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20220829213315.png"></p>
<p><strong>存在的问题</strong>：A服务器加密，B服务器解密，秘钥在网络传输中不安全</p>
<h2 id="网络上的加密算法—非对称加密"><a href="#网络上的加密算法—非对称加密" class="headerlink" title="网络上的加密算法—非对称加密"></a>网络上的加密算法—非对称加密</h2><p><strong>非对称加密</strong></p>
<p>用在不可信的环境下</p>
<p>RSA(算法的名字以发明者的名字命名：Ron Rivest, AdiShamir 和Leonard Adleman) </p>
<p>DH(Diffie-Hellman算法，密钥一致协议) </p>
<p>DSA(Digital Signature Algorithm，数字签名) </p>
<p>ECC(Elliptic Curves Cryptography，椭圆曲线密码编码学) </p>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20220829214316.png"></p>
<h2 id="Https协议"><a href="#Https协议" class="headerlink" title="Https协议"></a>Https协议</h2><p><strong>SSL&#x2F;TLS</strong></p>
<p>SSL(Secure Socket Layer 安全套接层)是基于HTTPS下的一个协议加密层。HTTP在传输数据时使用的是明文，是不安全的，为了解决这一隐患网景公司推出了SSL安全套接字协议层，SSL是基于HTTP之下TCP之上的一个协议层，是基于HTTP标准并对TCP传输数据时进行加密，所以HPPTS是HTTP+SSL&#x2F;TCP的简称。TLS(Transport Layer Security)即安全传输层协议，保障应用程序之间通信的安全性。SSL 3.0版本改名叫TLS，所以TLS是从SSL转过来的。</p>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/Untitled%20Diagram.drawio.png"></p>
<p><strong>HTTPS</strong></p>
<p>HTTPS（全称：Hyper Text Transfer Protocol over Secure Socket Layer超文本传输安全协议），是以安全为目标的HTTP通道，简单讲是HTTP的安全版。HTTP下加入SSL层，HTTPS的安全基础是SSL，因此加密的详细内容就需要SSL。</p>
<h3 id="Https协议架构组成"><a href="#Https协议架构组成" class="headerlink" title="Https协议架构组成"></a>Https协议架构组成</h3><p><strong>安全保证：</strong></p>
<p><strong>1. 身份认证</strong>，认证用户或服务器，确保数据发送到正确的客户机或服务器</p>
<p><strong>2. 内容加密</strong>，加密数据以防止数据中途被窃取</p>
<p><strong>3. 数据完整性</strong>，维护数据的完整性，确保数据在传输过程中不被篡改</p>
<p>https使用了<strong>摘要算法、对称加密算法、非对称加密算法</strong></p>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20220829221502.png"></p>
<h3 id="Https与Http的区别"><a href="#Https与Http的区别" class="headerlink" title="Https与Http的区别"></a>Https与Http的区别</h3><ol>
<li><p>https协议需要到CA申请数字证书。</p>
</li>
<li><p>http是超文本传输协议，信息是明文传输；https 则是具有安全性的ssl加密传输协议。</p>
</li>
<li><p>http和https使用的是完全不同的连接方式，用的端口也不一样，前者是80，后者是443。</p>
</li>
<li><p>http的连接很简单，是无状态的；HTTPS协议是由SSL+HTTP协议构建的可进行加密传输、身份认证的网络协议，比http协议安全</p>
</li>
</ol>
<p>HTTPS对数据进行加解密，决定了它比http慢</p>
<h1 id="Https如何保证数据传输安全"><a href="#Https如何保证数据传输安全" class="headerlink" title="Https如何保证数据传输安全"></a>Https如何保证数据传输安全</h1><h2 id="SSL-TLS的握手过程—秘钥协商"><a href="#SSL-TLS的握手过程—秘钥协商" class="headerlink" title="SSL&#x2F;TLS的握手过程—秘钥协商"></a>SSL&#x2F;TLS的握手过程—秘钥协商</h2><p>https分为单向认证和双向认证，这里客户端被动接受服务段证书，是一个单向认证过程</p>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20220829221940.png"></p>
<h2 id="Https协议安全保证的原因"><a href="#Https协议安全保证的原因" class="headerlink" title="Https协议安全保证的原因"></a>Https协议安全保证的原因</h2><p><strong>注意：</strong></p>
<p>SSL协议在握手阶段使用的是非对称加密，在传输阶段使用的是对称加密，也就是说在SSL上传送的数据是使用对称密钥加密的！因为非对称加密的速度缓慢，耗费资源。</p>
<p>其实当客户端和主机使用非对称加密方式建立连接后，客户端和主机已经决定好了在传输过程使用的对称加密算法和关键的对称加密密钥，由于这个过程本身是安全可靠的，也即对称加密密钥是不可能被窃取盗用的，因此，保证了在传输过程中对数据进行对称加密也是安全可靠的，因为除了客户端和主机之外，不可能有第三方窃取并解密出对称加密密钥！</p>
<p>如果有人窃听通信，他可以知道双方选择的加密方法，以及三个随机数中的两个。整个通话的安全，只取决于第三个随机数能不能被破解。</p>
<p>https实际就是在TCP层与http层之间加入了SSL&#x2F;TLS来为上层的安全保驾护航，主要用到对称加密、非对称加密、证书，等技术进行客户端与服务器的数据加密传输，最终达到保证整个通信的安全性。</p>
<h1 id="制作你的第一个SSL证书"><a href="#制作你的第一个SSL证书" class="headerlink" title="制作你的第一个SSL证书"></a>制作你的第一个SSL证书</h1><h2 id="CA和数字证书"><a href="#CA和数字证书" class="headerlink" title="CA和数字证书"></a>CA和数字证书</h2><p><strong>怎么能确定客户端所得到的公钥一定是从目标主机那里发布的，而且没有被篡改过呢？</strong></p>
<p>需要有一个权威的值得信赖的第三方机构CA(一般是由政府审核并授权的机构)来统一对外发放主机机构的公钥，只要请求方这种机构获取公钥，就避免了上述问题的发生。</p>
<p><strong>数字证书的颁发过程</strong></p>
<p>用户首先产生自己的密钥对，并将公共密钥及部分个人身份信息传送给认证中心。认证中心在核实身份后，将执行一些必要的步骤，以确信请求确实由用户发送而来，然后，认证中心将发给用户一个数字证书，该证书内包含用户的个人信息和他的公钥信息，同时还附有认证中心的签名信息(根证书私钥签名)。用户就可以使用自己的数字证书进行相关的各种活动。数字证书由独立的证书发行机构发布，数字证书各不相同，每种证书可提供不同级别的可信度。</p>
<h2 id="浏览器CA认证流程"><a href="#浏览器CA认证流程" class="headerlink" title="浏览器CA认证流程"></a>浏览器CA认证流程</h2><p><strong>浏览器默认都会内置CA根证书，其中根证书包含了CA的公钥</strong></p>
<ol>
<li><p>证书颁发的机构是伪造的：浏览器不认识，直接认为是危险证书</p>
</li>
<li><p>证书颁发的机构是确实存在的，于是根据CA名，找到对应内置的CA根证书、CA的公钥。用CA的公钥，对伪造的证书的摘要进行解密，发现解不了，认为是危险证书。</p>
</li>
<li><p>对于篡改的证书，使用CA的公钥对数字签名进行解密得到摘要A，然后再根据签名的Hash算法计算出证书的摘要B，对比A与B，若相等则正常，若不相等则是被篡改过的。</p>
</li>
<li><p>证书可在其过期前被吊销，通常情况是该证书的私钥已经失密。较新的浏览器如Chrome、Firefox、Opera和Internet Explorer都实现了在线证书状态协议（OCSP）以排除这种情形：浏览器将网站提供的证书的序列号通过OCSP发送给证书颁发机构，后者会告诉浏览器证书是否还是有效的。</p>
</li>
</ol>
<p>1、2点是对伪造证书进行的，3是对于篡改后的证书验证，4是对于过期失效的验证。</p>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20220918154515.png"></p>
<h2 id="OpenSSL"><a href="#OpenSSL" class="headerlink" title="OpenSSL"></a>OpenSSL</h2><p>OpenSSL是个多功能命令行工具、他可以实现加密解密、甚至还可以当CA来用、可以让你创建证书、吊销证书。</p>
<p>OpenSSL由应用、TLS、加密、引擎四个组件层组成。加密组件通过引擎组件来扩展，TLS依赖加密组件，应用组件则依赖TLS和加密组件。</p>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20220918154820.png"></p>
<p>更多资料参考官网：<a target="_blank" rel="noopener" href="https://www.openssl.org/">https://www.openssl.org/</a></p>
<h2 id="动手制作证书"><a href="#动手制作证书" class="headerlink" title="动手制作证书"></a>动手制作证书</h2><p>SSL 证书就是遵守 SSL协议，由受信任的数字证书颁发机构CA，在验证服务器身份后颁发，具有服务器身份验证和</p>
<p>数据传输加密功能的文件，当前大多数的ssl证书是收费的。</p>
<p><strong>免费证书</strong></p>
<p>也可以到阿里云、腾讯云申请1年的免费的简单证书。</p>
<p>下面，我们将通过OpenSSL自建一个CA服务，自己给自己的服务颁发数字证书，学习整个证书签发过程</p>
<p>​	具体操作参考《 制作第一张自己的SSL证书.md》手册</p>
<h1 id="Nginx中实现高性能Https"><a href="#Nginx中实现高性能Https" class="headerlink" title="Nginx中实现高性能Https"></a>Nginx中实现高性能Https</h1><h2 id="Nginx中配置HTTPS"><a href="#Nginx中配置HTTPS" class="headerlink" title="Nginx中配置HTTPS"></a>Nginx中配置HTTPS</h2><figure class="highlight nginx"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="section">server</span> &#123;</span><br><span class="line">	<span class="attribute">listen</span> <span class="number">443</span> ssl;</span><br><span class="line">	<span class="attribute">server_name</span> www.example.com;</span><br><span class="line">	<span class="attribute">ssl_certificate</span> www.example.com.crt; <span class="comment"># 发送到客户端具有PEM格式的公共证书</span></span><br><span class="line">	<span class="attribute">ssl_certificate_key</span> www.example.com.key; <span class="comment"># 具有PEM格式的服务器端私钥，注意访问权限设置</span></span><br><span class="line">	<span class="attribute">ssl_protocols</span> TLSv1 TLSv1.<span class="number">1</span> TLSv1.<span class="number">2</span>; <span class="comment"># ssl支持的协议</span></span><br><span class="line">	<span class="attribute">ssl_ciphers</span> HIGH:!aNULL:!MD5; <span class="comment"># 以OpenSS L库所理解的格式指定</span></span><br><span class="line">	...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="Https服务的优化"><a href="#Https服务的优化" class="headerlink" title="Https服务的优化"></a>Https服务的优化</h2><p><strong>优化方法</strong></p>
<p>SSL操作消耗额外的CPU资源。在多处理器系统上，应该运行多个工作进程，而不小于可用CPU核的数量。CPU密集型操作是SSL握手。有两种方法可以使每个客户端的这些操作的数量最小化：<br>第一个是通过启用Keepalive连接来通过一个连接发送多个请求。<br>第二个是重用SSL会话参数，以避免用于并行和后续连接的SSL握手。</p>
<p><strong>Nginx中为了减少处理器负载，建议</strong></p>
<ol>
<li><p>将工作进程的数量设置为等于处理器的数目，worker_processes</p>
</li>
<li><p>启用维持生命的连接，keepalive_timeout</p>
</li>
<li><p>启用共享会话缓存，禁用内置会话缓存，<a target="_blank" rel="noopener" href="https://nginx.org/en/docs/http/configuring_https_servers.html#single_http_https_server">ssl_session_cache</a> shared:SSL:10m;</p>
</li>
</ol>
<p>off，禁用、</p>
<p>none，客户端可重用服务端不缓存</p>
<p>builtin，在openssl中重构缓存，<code>builtin</code>[:<code>size</code>]</p>
<p>shared，在worker进程间共享的缓存，<code>shared</code>:<code>name</code>:<code>size</code></p>
<ol start="4">
<li>增加会话生存期(默认为5分钟)，ssl_session_timeout</li>
</ol>
<p>配置示例</p>
<figure class="highlight nginx"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 简单指令以;结尾</span></span><br><span class="line"><span class="attribute">worker_processes</span>  <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 大括号属于块指令</span></span><br><span class="line"><span class="section">events</span> &#123;</span><br><span class="line">    <span class="attribute">worker_connections</span>  <span class="number">1024</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="section">http</span> &#123;</span><br><span class="line">    <span class="comment"># 上游服务</span></span><br><span class="line">    <span class="section">upstream</span> backend &#123;</span><br><span class="line">        <span class="attribute">server</span> <span class="number">192.168.120.58:8080</span>;</span><br><span class="line">        <span class="attribute">server</span> <span class="number">192.168.120.103:8080</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 配置https服务</span></span><br><span class="line">    <span class="section">server</span> &#123;</span><br><span class="line">        <span class="attribute">listen</span>       <span class="number">80</span>;    <span class="comment"># 访问80，走http协议</span></span><br><span class="line">        <span class="attribute">listen</span>       <span class="number">443</span> ssl;   <span class="comment"># 访问443端口，走https安全协议</span></span><br><span class="line">        <span class="attribute">server_name</span>  localhost;</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 配置证书及服务器私钥</span></span><br><span class="line">        <span class="attribute">ssl_certificate</span>      servercert.pem;</span><br><span class="line">        <span class="attribute">ssl_certificate_key</span>  serverkey.pem;</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 会话参数的缓存,所有工作进程之间共享的缓存</span></span><br><span class="line">        <span class="attribute">ssl_session_cache</span>    shared:SSL:<span class="number">1m</span>;</span><br><span class="line">        <span class="attribute">ssl_session_timeout</span>  <span class="number">5m</span>;</span><br><span class="line">        <span class="attribute">ssl_protocols</span> TLSv1 TLSv1.<span class="number">1</span> TLSv1.<span class="number">2</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 启用的密码</span></span><br><span class="line">        <span class="attribute">ssl_ciphers</span>  ECDHE-RSA-AES128-GCM-SHA256:HIGH:!aNULL:!MD5:!RC4:!DHE;</span><br><span class="line">        <span class="comment"># SSLv3和TLS协议时,服务器密码优先于客户端密码</span></span><br><span class="line">        <span class="attribute">ssl_prefer_server_ciphers</span>  <span class="literal">on</span>;</span><br><span class="line"></span><br><span class="line">        <span class="section">location</span> / &#123;</span><br><span class="line">            <span class="attribute">proxy_pass</span> http://backend;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>






      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2023/09/10/16-Nginx/02-00-Nginx%E7%9A%84WEB%E5%AE%89%E5%85%A8%E4%BD%93%E7%B3%BB/" data-id="clmcxecbb002ku8wa2cewe4os" data-title="" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-16-Nginx/01-Nginx入门" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/09/10/16-Nginx/01-Nginx%E5%85%A5%E9%97%A8/" class="article-date">
  <time class="dt-published" datetime="2023-09-10T03:59:07.767Z" itemprop="datePublished">2023-09-10</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="Nginx入门"><a href="#Nginx入门" class="headerlink" title="Nginx入门"></a>Nginx入门</h1><p><strong>分流限流</strong>、多读写少用缓存，写多读少用缓冲。</p>
<h2 id="什么是Nginx"><a href="#什么是Nginx" class="headerlink" title="什么是Nginx"></a>什么是Nginx</h2><p>一个俄国人用C语言编写的，开源的高性能的HTTP和反向代理服务软件。</p>
<p>Nginx主要应用于静态资源服务、反向代理服务、API服务 </p>
<ol>
<li>静态资源主要借助于服务器本地文件系统来完成</li>
</ol>
<p> 2. 反向代理可以做到Nginx强大的性能、缓存、负载均衡 </p>
<ol start="3">
<li>API服务通过集成nginx_lua模块来实现，比如 OpenResty就是用nginx和lua集成特性，整合了大量常用的第三方模块</li>
</ol>
<p>  </p>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20220605145630.png"></p>
<h2 id="正向代理-反向代理"><a href="#正向代理-反向代理" class="headerlink" title="正向代理&#x2F;反向代理"></a>正向代理&#x2F;反向代理</h2><p>正向代理，代表客户端进行网络或服务访问</p>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20220605145726.png"></p>
<p>反向代理，代表服务端接收客户端的请求</p>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20220605145745.png"></p>
<h2 id="为什么会选Nginx？"><a href="#为什么会选Nginx？" class="headerlink" title="为什么会选Nginx？"></a>为什么会选Nginx？</h2><p>在高并发的互联网行业，硬件负载均衡器、软件负载均衡器的选择</p>
<p><strong>高并发、高性能</strong> </p>
<p>基于NIO非阻塞事件模型处理网络请求，slab内存管理机制。 </p>
<p><strong>可扩展性好</strong> </p>
<p>核心模块+扩展模块+第三方插件，丰富的模块及第三方插件是Nginx生命力顽强的原因。</p>
<p> <strong>高可靠性</strong></p>
<p> 部署后常年稳定运行 </p>
<p><strong>热部署</strong> </p>
<p>无需重启，更新配置文件 </p>
<p><strong>开源BSD协议</strong>  </p>
<h2 id="Nginx常规操作"><a href="#Nginx常规操作" class="headerlink" title="Nginx常规操作"></a>Nginx常规操作</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">./nginx -h <span class="comment">#显示帮助文档</span></span><br><span class="line">./nginx -c /usr/local/nginx/conf/nginx.conf <span class="comment">#指定配置文件</span></span><br><span class="line">./nginx -s quit <span class="comment">#比stop更优雅的退出</span></span><br><span class="line">./nginx -s reload <span class="comment">#热更新配置文件，无需重启</span></span><br><span class="line"></span><br><span class="line">./nginx -s reopen <span class="comment">#打开新的日志文件</span></span><br><span class="line">./nginx -t <span class="comment">#测试配置文件是否正确</span></span><br><span class="line">./nginx -v <span class="comment">#版本信息</span></span><br><span class="line">./nginx -V <span class="comment">#版本信息及配置选项</span></span><br></pre></td></tr></table></figure>

<h2 id="Nginx的配置文件"><a href="#Nginx的配置文件" class="headerlink" title="Nginx的配置文件"></a>Nginx的配置文件</h2><p>Nginx有一个非常强大的配置文件，可以配置应用对应的模块指令，类似Perl语法风格。配置文件指今主要分为两大块∶简单指令、块指令。  </p>
<p><strong>简单指令</strong></p>
<p>简单的指令由名称和参数组成，用空格分隔，以分号;结尾</p>
<p><strong>块指令</strong></p>
<p>块指令，以大括号{}包围的一组附加指令，块指令在大括号内可以有其他指令，则称为上下文。在任意上下文之外的指令称为主上下文，http、events指令放在主上下文，server在http中，location在server中。  </p>
<h2 id="动静分离-静态资源服务"><a href="#动静分离-静态资源服务" class="headerlink" title="动静分离-静态资源服务"></a>动静分离-静态资源服务</h2><p><strong>动静分离术</strong></p>
<p>Web服务当中，html、js、图片、css等静态资源放在webapp目录下。当静态资源越 来越大时，一个网页获取大量静态资源时，影响到web服务的整体性能。 </p>
<p>通过Nginx将静态资源独立部署，减轻web 服务的压力。</p>
<p> 动态计算的数据则通过web服务来获取。 </p>
<figure class="highlight perl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">worker_processes <span class="number">1</span>;</span><br><span class="line">events&#123;</span><br><span class="line">    worker_connections <span class="number">1024</span>;</span><br><span class="line">&#125;</span><br><span class="line">http&#123;</span><br><span class="line">    server&#123;</span><br><span class="line">        location /&#123;</span><br><span class="line">            root /data/www/;</span><br><span class="line">            <span class="keyword">index</span> index.html;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="Nginx的配置热更新"><a href="#Nginx的配置热更新" class="headerlink" title="Nginx的配置热更新"></a>Nginx的配置热更新</h2><p>Nginx中有Master、Worker两种进程。</p>
<p>Master进程负责加载配置、接收命令、监控子进程</p>
<p>Worker进程负责处理网络请求  </p>
<p><strong>Nginx如何做到配置文件热更新?</strong> </p>
<ol>
<li>master检查配置文件的正确性，若是错误则返回错误 信息，nginx继续用原配置文件进行工作</li>
</ol>
<p> 2. Nginx启动新的worker进程，采用新的配置文件</p>
<ol start="3">
<li>Nginx将新的请求分配新的workeri进程</li>
</ol>
<p> 4. Nginx等待以前的worker进程的全部请求已经都返回后， 关闭相关worker进程 </p>
<ol start="5">
<li>重复上面过程，直到全部旧的worker进程都被关闭掉</li>
</ol>
<p>  </p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost nginx]# ps -ef | grep nginx</span><br><span class="line">root      56240      1  0 15:15 ?        00:00:00 nginx: master process ./nginx -c conf/nginx_static.conf</span><br><span class="line">nobody    56241  56240  0 15:15 ?        00:00:00 nginx: worker process</span><br></pre></td></tr></table></figure>

<h1 id="官方文档阅读"><a href="#官方文档阅读" class="headerlink" title="官方文档阅读"></a>官方文档阅读</h1><p>nginx官网：<a target="_blank" rel="noopener" href="https://nginx.org/">https://nginx.org/</a></p>
<h2 id="Nginx官方文档整体结构"><a href="#Nginx官方文档整体结构" class="headerlink" title="Nginx官方文档整体结构"></a>Nginx官方文档整体结构</h2><p>文档主要有∶首页、关于、下载页面、安全、使用文档、faq、博客等部分  </p>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20220605152529.png"></p>
<p>作为开发人员，主要关注，下载页面、使用文档、博客这几个部分。其中使用文档和博客是最为重要的内容。 </p>
<p><strong>博客</strong>中涉及一些高级的特性和商业支持。<strong>使用文档</strong>包含，Nginx常规操作介绍、功能场景操作、贡献提交源代码、Nginx模块参考。</p>
<p> <strong>常规操作</strong> </p>
<p>包含如何编译安装Nginx、初学者指南、管理员指南、Nginx操作、Nginx的连接处理事件类型、等信息。</p>
<p> <strong>功能场景配置</strong></p>
<p> 一般只有在对应的功能场景下使用配置，例如负载均衡反向代理、HTTPS服务   </p>
<h2 id="Nginx模块"><a href="#Nginx模块" class="headerlink" title="Nginx模块"></a>Nginx模块</h2><p>Nginx整体设计采用∶<strong>微核心+插件形式</strong></p>
<p> 很多优秀的开源组件都在采用这样的设计思想，为第三方扩展提供了强大的支撑。也是ngin受欢迎的原因之一。  </p>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20220605172034.png"></p>
<h2 id="核心模块常用指令"><a href="#核心模块常用指令" class="headerlink" title="核心模块常用指令"></a>核心模块常用指令</h2><p>核心功能指令</p>
<table>
<thead>
<tr>
<th>指令</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td>user</td>
<td>指定用来运行nginx的用户名，跟linux权限系统相关</td>
</tr>
<tr>
<td>use</td>
<td>指定处理网络IO的事件模型：select、kqueue、poll、epool</td>
</tr>
<tr>
<td>worker_processes</td>
<td>指定运行多少个工作进程</td>
</tr>
<tr>
<td>events</td>
<td>指定连接处理相关的指令块：use、worker_connections等简单指令</td>
</tr>
<tr>
<td>worker_connections</td>
<td>Worker进程能够处理的最大连接数</td>
</tr>
</tbody></table>
<p>http核心指令</p>
<table>
<thead>
<tr>
<th>指令</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td>http</td>
<td>表示http模块的块指令</td>
</tr>
<tr>
<td>server</td>
<td>表示一个虚拟服务</td>
</tr>
<tr>
<td>listen</td>
<td>监听端口号，有大量可选项，ssl、http2等</td>
</tr>
<tr>
<td>location</td>
<td>设置访问的URL，7种玩法</td>
</tr>
</tbody></table>
<h2 id="location7种玩法"><a href="#location7种玩法" class="headerlink" title="location7种玩法"></a>location7种玩法</h2><p>最常用又最容易弄错的location指令，它可以由前级字符串或正则表达式定义  </p>
<table>
<thead>
<tr>
<th>方式</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td>&#x3D;</td>
<td>URI和位置的精确匹配，如找到完全匹配，则搜索终止，优先级最高</td>
</tr>
<tr>
<td>&#x2F;xxx</td>
<td>字符串前缀匹配，匹配到 ，记住，仍会继续匹配正则表达式</td>
</tr>
<tr>
<td>^~</td>
<td>字符串前缀匹配，有匹配，记住，但后继不再做正则表达式匹配，会使用最长前缀的匹配配置</td>
</tr>
<tr>
<td>~*</td>
<td>不区分大小写的匹配正则表达式</td>
</tr>
<tr>
<td>~</td>
<td>区分大小写的匹配正则表达式</td>
</tr>
<tr>
<td>&#x2F;</td>
<td>最后的接盘侠，所有指令都不匹配的时候则用它。相当于java中switch的default</td>
</tr>
<tr>
<td>@</td>
<td>非常规请求，用于请求重定向。自己不能嵌套，也不能嵌套location指令</td>
</tr>
</tbody></table>
<h2 id="location匹配规则"><a href="#location匹配规则" class="headerlink" title="location匹配规则"></a>location匹配规则</h2><p>当一个请求过来时候，location怎么进行匹配? </p>
<p>首先 nginx检查使用了前缀字符串定义的location指令块</p>
<p>​	 选中并记住具有最长匹配前缀的location指令块</p>
<p>​	 location如有^~匹配，则不查找正则表达式，使用该location</p>
<p> 然后 按照它们在配置文件中的出现顺序检查正则表达式 </p>
<p>​	正则表达式的搜索在第一次匹配时终止，并使用相应的配置 </p>
<p>​	如果未找到与正则表达式的匹配，则使用先前记住的最长匹配前缀位置的配置 </p>
<p>最后还没有找到?接盘侠&#x2F;  </p>
<p><a target="_blank" rel="noopener" href="https://nginx.org/en/docs/http/ngx_http_core_module.html#http">官网描述</a>：</p>
<p>在按名称搜索虚拟服务器期间，如果名称匹配多个指定变体（例如通配符名称和正则表达式匹配），将选择第一个匹配的变体，按以下优先级顺序：</p>
<ol>
<li>确切的名字</li>
<li>以星号开头的最长通配符名称，例如“ <code>*.example.com</code>”</li>
<li>以星号结尾的最长通配符名称，例如“ <code>mail.*</code>”</li>
<li>第一个匹配的正则表达式（在配置文件中的出现顺序）</li>
</ol>
<h1 id="实现高并发分流"><a href="#实现高并发分流" class="headerlink" title="实现高并发分流"></a>实现高并发分流</h1><h2 id="单台服务的能力"><a href="#单台服务的能力" class="headerlink" title="单台服务的能力"></a>单台服务的能力</h2><p><strong>模拟一个耗时服务</strong></p>
<p><code>http://hostname:port/bench/1000</code></p>
<p><strong>使用压测工具压测</strong></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">安装ab工具</span></span><br><span class="line">yum -y install httpd-tools</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">总共10w次请求，100的并发</span></span><br><span class="line">ab -n 100000 -c 1000 http://localhost:8080/bench/100</span><br></pre></td></tr></table></figure>

<p><strong>测试结果</strong></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">平均吞吐率（总请求数/总时间），括号中的mean表示这是一个平均值</span></span><br><span class="line">Requests per second:    1913.12 [#/sec] (mean)</span><br></pre></td></tr></table></figure>

<h2 id="Nginx复制均衡"><a href="#Nginx复制均衡" class="headerlink" title="Nginx复制均衡"></a>Nginx复制均衡</h2><p><strong>负载均衡涉及的模块</strong></p>
<p><a target="_blank" rel="noopener" href="https://nginx.org/en/docs/http/ngx_http_upstream_module.html">Module ngx_http_upstream_module</a></p>
<p><a target="_blank" rel="noopener" href="https://nginx.org/en/docs/http/ngx_http_proxy_module.html">Module ngx_http_proxy_module</a></p>
<h2 id="Nginx完成反向代理负载均衡"><a href="#Nginx完成反向代理负载均衡" class="headerlink" title="Nginx完成反向代理负载均衡"></a>Nginx完成反向代理负载均衡</h2><p><strong>upstream</strong></p>
<p>http_upstream模块定义一组服务，能被proxy_pass指令引用。</p>
<p> upstream name{…} 在http指令块中，定义一组可以进行负载均衡的上游服务器，并给取个组名。服务可以是不同的端口、TCP、Unix域名套接字。  </p>
<figure class="highlight perl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">upstream tomcatServer&#123;</span><br><span class="line">	server backend1.example.com weight=<span class="number">5</span>;</span><br><span class="line">	server <span class="number">127.0</span>.<span class="number">01</span>:<span class="number">8080</span> max_fails=<span class="number">3</span> fail_timeout=<span class="number">30</span>s;</span><br><span class="line">	server unix:<span class="regexp">/tmp/</span>backend3;</span><br><span class="line">	server backup1.example.com backup;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>通过upstream，就能为成千上百台的后端服务集群提供强有力的支撑</p>
<p><strong>proxy_pass</strong></p>
<p>表示将请求传递到另外一个服务</p>
<figure class="highlight perl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># server在http的上下文中</span></span><br><span class="line">server&#123;</span><br><span class="line">	<span class="comment">#location在server上下文中</span></span><br><span class="line">	location / &#123;</span><br><span class="line">		proxy_pass http:<span class="regexp">//</span>tomcatServer;</span><br><span class="line">		health_check;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="再次压测"><a href="#再次压测" class="headerlink" title="再次压测"></a>再次压测</h2><p><strong>修改Nginx配置并启动</strong></p>
<figure class="highlight perl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">worker_processes <span class="number">2</span>;</span><br><span class="line">events&#123;</span><br><span class="line">    worker_connections <span class="number">1024</span>;</span><br><span class="line">&#125;</span><br><span class="line">http&#123;</span><br><span class="line">    upstream tomcatServer&#123;</span><br><span class="line">        server <span class="number">192.168</span>.<span class="number">254.174</span>:<span class="number">8080</span>;</span><br><span class="line">        server <span class="number">192.168</span>.<span class="number">254.174</span>:<span class="number">8081</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    server&#123;</span><br><span class="line">		<span class="keyword">listen</span> <span class="number">9000</span>;</span><br><span class="line">        location / &#123;</span><br><span class="line">            proxy_pass http:<span class="regexp">//</span>tomcatServer;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>.&#x2F;nginx -c &#x2F;usr&#x2F;local&#x2F;nginx&#x2F;conf&#x2F;nginx_upstream01.conf</p>
<p><strong>使用压测工具压测</strong></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">安装ab工具</span></span><br><span class="line">yum -y install httpd-tools</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">总共10w次请求，100的并发</span></span><br><span class="line">ab -n 100000 -c 1000 http://localhost:9000/bench/100</span><br></pre></td></tr></table></figure>

<p><strong>测试结果</strong></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">平均吞吐率（总请求数/总时间），括号中的mean表示这是一个平均值</span></span><br><span class="line">Requests per second:    1953.74 [#/sec] (mean)</span><br></pre></td></tr></table></figure>

<h2 id="Nginx负载均衡策略"><a href="#Nginx负载均衡策略" class="headerlink" title="Nginx负载均衡策略"></a>Nginx负载均衡策略</h2><table>
<thead>
<tr>
<th>策略</th>
<th>均衡性</th>
<th>一致性</th>
<th>容灾性</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td>round-robin</td>
<td>☆☆☆</td>
<td>☆</td>
<td>☆☆☆</td>
<td>默认的处理反向代理服务的方法，还能根据加权值来传递请求。通用性强，无特殊需求的场景下均可</td>
</tr>
<tr>
<td>hash</td>
<td>☆☆☆</td>
<td>☆☆☆</td>
<td>☆☆</td>
<td></td>
</tr>
<tr>
<td>ip_hash</td>
<td>☆☆</td>
<td>☆☆☆</td>
<td>☆☆☆</td>
<td>根据客户端ip地址在服务间进行分发请求。要求ip一致性的场景</td>
</tr>
<tr>
<td>random</td>
<td>☆☆</td>
<td>☆</td>
<td>☆☆</td>
<td>将请求传递给随机选择的服务器</td>
</tr>
<tr>
<td>least_conn</td>
<td>☆☆☆</td>
<td>☆</td>
<td>☆☆☆</td>
<td>将请求传递到活动连接数量最少的服务器，同时考虑服务器的权重。自适应强，适合服务差异复杂的情况</td>
</tr>
<tr>
<td>least_time</td>
<td>☆☆</td>
<td>☆</td>
<td>☆☆☆</td>
<td>将请求传递给具有最小平均响应时间和最少活动连接数的服务，同时考虑服务器权重。自适应强，适合网络环境复杂的情况</td>
</tr>
</tbody></table>
<figure class="highlight perl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">worker_processes <span class="number">2</span>;</span><br><span class="line">events&#123;</span><br><span class="line">    worker_connections <span class="number">1024</span>;</span><br><span class="line">&#125;</span><br><span class="line">http&#123;</span><br><span class="line">    upstream tomcatServer&#123;</span><br><span class="line">    	<span class="comment"># 负载均衡策略，默认为round-robin</span></span><br><span class="line">	    random;</span><br><span class="line">        server <span class="number">192.168</span>.<span class="number">254.174</span>:<span class="number">8080</span>;</span><br><span class="line">        server <span class="number">192.168</span>.<span class="number">254.174</span>:<span class="number">8081</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    server&#123;</span><br><span class="line">		<span class="keyword">listen</span> <span class="number">9000</span>;</span><br><span class="line">        location / &#123;</span><br><span class="line">            proxy_pass http:<span class="regexp">//</span>tomcatServer;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>


      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2023/09/10/16-Nginx/01-Nginx%E5%85%A5%E9%97%A8/" data-id="clmcxecba002ju8wa2q2dgmcp" data-title="" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-16-Nginx/00-Nginx安装" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/09/10/16-Nginx/00-Nginx%E5%AE%89%E8%A3%85/" class="article-date">
  <time class="dt-published" datetime="2023-09-10T03:59:07.760Z" itemprop="datePublished">2023-09-10</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="Nginx安装手册"><a href="#Nginx安装手册" class="headerlink" title="Nginx安装手册"></a>Nginx安装手册</h1><h2 id="快速开始，windows安装"><a href="#快速开始，windows安装" class="headerlink" title="快速开始，windows安装"></a>快速开始，windows安装</h2><p>进入<a target="_blank" rel="noopener" href="https://nginx.org/en/download.html">Nginx下载页面</a></p>
<p>选择nginx&#x2F;Windows-xxx.zip的安装包下载，xxx表示最新的版本</p>
<p>解压zip到你的程序安装目录</p>
<p>进入nginx-xxx目录，双击nginx.exe启动Nginx服务</p>
<p><strong>非常简单快速，适合学习初用，快速搭建</strong></p>
<h2 id="linux安装"><a href="#linux安装" class="headerlink" title="linux安装"></a>linux安装</h2><p>linux下面有两种安装方式，二进制安装、通过源码编译安装。</p>
<h3 id="二进制安装"><a href="#二进制安装" class="headerlink" title="二进制安装"></a>二进制安装</h3><p>不同的平台都有支持，具体参考<a target="_blank" rel="noopener" href="https://nginx.org/en/linux_packages.html">官网说明</a>，这里以centos7为示例，使用yum进行安装。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">检查更新yum依赖</span></span><br><span class="line">sudo yum install yum-utils</span><br></pre></td></tr></table></figure>

<p>添加yum的nginx仓库地址，（可以省略该步骤）</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">sudo vim /etc/yum.repos.d/nginx.repo</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">进入编辑模式，输入下面内容</span></span><br><span class="line">[nginx-stable]</span><br><span class="line">name=nginx stable repo</span><br><span class="line">baseurl=http://nginx.org/packages/centos/$releasever/$basearch/</span><br><span class="line">gpgcheck=1</span><br><span class="line">enabled=1</span><br><span class="line">gpgkey=https://nginx.org/keys/nginx_signing.key</span><br><span class="line"></span><br><span class="line">[nginx-mainline]</span><br><span class="line">name=nginx mainline repo</span><br><span class="line">baseurl=http://nginx.org/packages/mainline/centos/$releasever/$basearch/</span><br><span class="line">gpgcheck=1</span><br><span class="line">enabled=0</span><br><span class="line">gpgkey=https://nginx.org/keys/nginx_signing.key</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">保存即可</span></span><br></pre></td></tr></table></figure>

<p>安装nginx，也可以直接使用该命令进行安装</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo yum install nginx</span><br></pre></td></tr></table></figure>



<h3 id="通过源码编译安装"><a href="#通过源码编译安装" class="headerlink" title="通过源码编译安装"></a>通过源码编译安装</h3><p>通过源码编译安装，能够集成一些默认没有安装的模块以及第三方插件。下面我们来编译一个携带ssl和echo模块的Nginx。</p>
<h4 id="安装依赖"><a href="#安装依赖" class="headerlink" title="安装依赖"></a>安装依赖</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">yum install -y gcc		# GCC编译器，用来编译C语言程序</span><br><span class="line">yum install -y gcc-c++		# C++编译器，用来编译C++语言程序</span><br><span class="line">yum install -y pcre pcre-devel		# Perl库兼容正则表达式，Nginx的HTTP模块要靠它来解析正则表达式</span><br><span class="line">yum install -y zlib zlib-devel		# zlib库，用于对HTTP包的内容做gzip格式的压缩</span><br><span class="line">yum install -y openssl openssl-devel	# OpenSSL开发库，用于Http的SSL协议，需要源码编译</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">统一执行前面的命令</span></span><br><span class="line">sudo yum install -y gcc gcc-c++ pcre pcre-devel zlib zlib-devel openssl openssl-devel</span><br></pre></td></tr></table></figure>

<p>Nginx需要通过openssl源码库来进行安装ssl协议，下载openssl源码，并解压</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">wget https://github.com/openssl/openssl/archive/OpenSSL_1_0_2k.tar.gz	# 下载openssl</span><br><span class="line">tar -xzvf OpenSSL_1_0_2k.tar.gz # 解压openssl</span><br><span class="line"></span><br><span class="line">wget https://github.com/openresty/echo-nginx-module/archive/v0.61.tar.gz</span><br><span class="line">tar -xzvf v0.61.tar.gz</span><br></pre></td></tr></table></figure>

<h4 id="配置编译选项"><a href="#配置编译选项" class="headerlink" title="配置编译选项"></a>配置编译选项</h4><p>编译安装，通过configure文件来操作，用于一些特殊需求、特殊依赖的安装方式，操作也不是那么难。可以<a target="_blank" rel="noopener" href="https://nginx.org/en/docs/configure.html">参考官网</a>，也可以参考configure的帮助文档。</p>
<p>先了解目录内容，进入Nginx目录，输入ls查看文件内容，里面有一个configure可执行文件</p>
<p>在当前默认输入下面命令，就可以看到对应的帮助文档</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./configure --help</span><br></pre></td></tr></table></figure>

<p>这里列出几个常用的选项</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">--prefix= #指向安装目录</span><br><span class="line">--sbin-path #指向（执行）程序文件（nginx）</span><br><span class="line">--conf-path= #指向配置文件（nginx.conf）</span><br><span class="line">--error-log-path= #指向错误日志目录</span><br><span class="line">--pid-path= #指向 pid 文件（nginx.pid）</span><br><span class="line">--lock-path= #指向 lock 文件（nginx.lock）（安装文件锁定，防止安装文件被别人利用，或自己误操作。）</span><br><span class="line">--user= #指定程序运行时的非特权用户</span><br><span class="line">--group= #指定程序运行时的非特权用户组</span><br><span class="line">--builddir= #指向编译目录</span><br><span class="line">--without-xxx	#禁用默认编译启用的xxx模块</span><br><span class="line">--with-yyy	#启用默认不启用的yyy模块</span><br></pre></td></tr></table></figure>

<p>–without&#x2F;–with命令，它好比我们的maven资源管理，在父项目的pom中<code>&lt;dependencyManagement&gt;</code>定义好了依赖的资源，子项目如果需要使用通过<code>&lt;dependency&gt;</code>引用即可。</p>
<p>操作示例</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">sudo ./configure \</span><br><span class="line">	--prefix=/usr/local/nginx \</span><br><span class="line">    --sbin-path=/usr/local/nginx/nginx \</span><br><span class="line">    --conf-path=/usr/local/nginx/conf/nginx.conf \</span><br><span class="line">    --error-log-path=/usr/local/nginx/logs/error.log \</span><br><span class="line">    --pid-path=/usr/local/nginx/pid/nginx.pid \</span><br><span class="line">    --with-http_ssl_module \</span><br><span class="line">    --with-openssl=/root/openssl-OpenSSL_1_0_2k \</span><br><span class="line">    --add-module=/root/echo-nginx-module-0.61</span><br></pre></td></tr></table></figure>

<p>configure命令做了大量的“幕后”工作，包括检测操作系统内核和已经安装的软件，参数的解析，中间目录的生成以及根据各种参数生成一些C源码文件、Makefile文件等。</p>
<h4 id="执行编译"><a href="#执行编译" class="headerlink" title="执行编译"></a>执行编译</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo make</span><br></pre></td></tr></table></figure>

<p>make命令根据configure命令生成的Makefile文件编译Nginx工程，并生成目标文件、最终的二进制文件。</p>
<h4 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo make install</span><br></pre></td></tr></table></figure>

<p>make install命令根据configure执行时的参数将Nginx部署到指定的安装目录，包括相关目录的建立和二进制文件、配置文件的复制。</p>
<h4 id="FAQ"><a href="#FAQ" class="headerlink" title="FAQ"></a>FAQ</h4><p>make 执行报错</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost nginx-1.16.0]# sudo make</span><br><span class="line">make -f objs/Makefile</span><br><span class="line">make[1]: 进入目录“/root/nginx-1.16.0”</span><br><span class="line">cd /root/openssl-OpenSSL_1_0_2k \</span><br><span class="line">&amp;&amp; if [ -f Makefile ]; then make clean; fi \</span><br><span class="line">&amp;&amp; ./config --prefix=/root/openssl-OpenSSL_1_0_2k/.openssl no-shared no-threads  \</span><br><span class="line">&amp;&amp; make \</span><br><span class="line">&amp;&amp; make install_sw LIBDIR=lib</span><br><span class="line">Operating system: x86_64-whatever-linux2</span><br><span class="line">You need Perl 5.</span><br><span class="line">make[1]: *** [/root/openssl-OpenSSL_1_0_2k/.openssl/include/openssl/ssl.h] 错误 1</span><br><span class="line">make[1]: 离开目录“/root/nginx-1.16.0”</span><br><span class="line">make: *** [build] 错误 2</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">wget https://www.cpan.org/src/5.0/perl-5.30.1.tar.gz</span><br><span class="line">tar -xzf perl-5.30.1.tar.gz</span><br><span class="line">cd perl-5.30.1</span><br><span class="line">./Configure -des -Dprefix=$HOME/localperl</span><br><span class="line">make</span><br><span class="line">make test # 耗时较长</span><br><span class="line">make install</span><br></pre></td></tr></table></figure>


      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2023/09/10/16-Nginx/00-Nginx%E5%AE%89%E8%A3%85/" data-id="clmcxecb2002iu8wag5a5cia6" data-title="" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-13-大数据/04-Flink/01-Flink基础" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/09/10/13-%E5%A4%A7%E6%95%B0%E6%8D%AE/04-Flink/01-Flink%E5%9F%BA%E7%A1%80/" class="article-date">
  <time class="dt-published" datetime="2023-09-10T03:59:07.659Z" itemprop="datePublished">2023-09-10</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="初识-Flink"><a href="#初识-Flink" class="headerlink" title="初识 Flink"></a>初识 Flink</h1><h2 id="大数据计算引擎发展"><a href="#大数据计算引擎发展" class="headerlink" title="大数据计算引擎发展"></a>大数据计算引擎发展</h2><ol>
<li><p>首先第一代的计算引擎，无疑就是 Hadoop 承载的 MapReduce。</p>
</li>
<li><p>支持 DAG 的框架被划分为第二代计算引擎,如 Tez 。</p>
</li>
<li><p>接下来就是以 Spark 为代表的第三代的计算引擎。</p>
</li>
<li><p>Flink 的诞生就被归在了第四代：流计算、批流合一。</p>
</li>
</ol>
<h2 id="Flink-是什么"><a href="#Flink-是什么" class="headerlink" title="Flink 是什么"></a>Flink 是什么</h2><p>官网：<a target="_blank" rel="noopener" href="https://flink.apache.org/">https://flink.apache.org/</a></p>
<p>ApacheFlink 是一个开源的、分布式的、流式实时计算框架。</p>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20220409213530.png"></p>
<h2 id="Flink-发展历程"><a href="#Flink-发展历程" class="headerlink" title="Flink 发展历程"></a>Flink 发展历程</h2><ol>
<li>2008：柏林理工大学一个研究性项目Stratosphere，Next Generation Big DataAnalytics Platform（目标是建立下一代大数据分析引擎）；</li>
<li>2014-04-16，Stratosphere成为Apache孵化项目，从Stratosphere 0.6开始，正式更名为Flink。由Java语言编写；</li>
<li>2014-08-26，Flink 0.6发布；</li>
<li>2014-11-04，Flink 0.7.0发布，介绍了最重要的特性：StreamingAPI； ⑤ 2016-03-08，Flink 1.0.0，支持Scala；提供了 RocksDB状态后端的支持；</li>
<li>2017-02-06，Flink 1.2.0发布；</li>
<li>2017-06-01，Flink 1.3.0发布，支持了增量检查点机制。</li>
<li>2017-11-29，Flink 1.4.0发布，提供了端到端的 exactly-once 的语义保证。 ⑨ 2018-05-25，Flink 1.5.0发布，引入了本地状态恢复的机制；</li>
<li>2018-08-08，Flink 1.6.0发布；</li>
<li>2018-11-30，Flink 1.7.0发布，是第一个完全支持 Scala2.12 的版本。</li>
<li>2019-04-09，Flink 1.8.0发布，不发布带有Hadoop的二进制安装包。</li>
<li>2019-08-22，Flink 1.9.0发布，首次合并阿里内部版本Blink重要功能。</li>
<li>2020-02-11，Flink 1.10.0 发布， 是一个历时非常长、代码变动非常大的版本，也是 Flink 社区迄今为止规模最大的一次版本升级, 与 Blink 整合正式完成。</li>
</ol>
<h2 id="Flink-的特性"><a href="#Flink-的特性" class="headerlink" title="Flink 的特性"></a>Flink 的特性</h2><p>flink作为第四代大数据计算引擎，具备了优秀的特性，比如：高性能，低延迟，支持事件时间和无序事件，内存管理，可扩展性等特性。以下列举了flink几个比较重要的特性： </p>
<ol>
<li>事件驱动型 </li>
<li>流与批的世界观不同 </li>
<li>分层API</li>
<li>支持有状态计算 </li>
<li>支持exactly-once语义 </li>
<li>支持事件时间</li>
</ol>
<h3 id="Flink-的重要特点-事件驱动型"><a href="#Flink-的重要特点-事件驱动型" class="headerlink" title="Flink 的重要特点-事件驱动型"></a>Flink 的重要特点-事件驱动型</h3><p>事件驱动型(Event-driven)：事件驱动型应用以事件为单位进行处理，它从一个或多个事件流提取数据，并根据到来的事件触发计算、状态更新或其他外部动作。比较典型的就是以kafka为代表的消息队列几乎都是事件驱动型应用。</p>
<p>与事件驱动型之不同的就是SparkStreaming微批次驱动(时间驱动型)，如图：</p>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/streaming-flow.png"></p>
<p>Flink事件驱动型：</p>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20220409214421.png"></p>
<h3 id="Flink-的重要特点-流与批的世界观"><a href="#Flink-的重要特点-流与批的世界观" class="headerlink" title="Flink 的重要特点-流与批的世界观"></a>Flink 的重要特点-流与批的世界观</h3><ol>
<li>批处理的特点是有界、持久、大量，非常适合需要访问全套记录才能完成的计算工作</li>
<li>流处理的特点是无界、实时, 无需针对整个数据集执行操作，而是对通过系统传输的每个数据项执行操作，一般用于实时统计。在flink的世界观中，一切都是由流组成的，离线数据是有界限的流，实时数据是一个没有界限的流，这就是所谓的有界流和无界流。<ul>
<li>无界数据流：无界数据流有一个开始但是没有结束，它们不会在生成时终止并提供数据，必须连续处理无界流，也就是说必须在获取后立即处理event。</li>
<li>有界数据流：有界数据流有明确定义的开始和结束，可以在执行任何计算之前通过获取所有数据来处理有界流。</li>
</ul>
</li>
</ol>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20220409215351.png"></p>
<h3 id="Flink-的重要特点-分层-API"><a href="#Flink-的重要特点-分层-API" class="headerlink" title="Flink 的重要特点-分层 API"></a>Flink 的重要特点-分层 API</h3><p>Flink提供了三层API。每个API在简洁性和表达性之间提供了不同的权衡，并且针对不同的用例。</p>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20220409215604.png"></p>
<ol>
<li>Stateful Stream Processing<ol>
<li>它位于最底层，是Core API 的底层实现。</li>
<li>它是嵌入到Stream流里面的处理函数（processFunction）。</li>
<li>当Core API满足不了用户需求，可以利用低阶API构建一些新的组件或者算子。</li>
<li>它虽然灵活性高，但开发比较复杂，需要具备一定的编码能力。</li>
</ol>
</li>
<li>Core API<ol>
<li>DataSet API 是批处理API。 对静态数据进行批处理操作，将静态数据抽象成分布式的数据集，用户可以方便地使用Flink提供的各种操作符对分布式数据集进行处理，支持Java、Scala和Python。</li>
<li>DataStream API是流处理API。对数据流进行流处理操作，将流式的数据抽象成分布式的数据流，用户可以方便地对分布式数 据流进行各种操作，支持JavaScala。</li>
</ol>
</li>
<li>Table API &amp; SQL<ol>
<li>SQL 构建在Table 之上，都需要构建Table 环境。</li>
<li>不同的类型的Table 构建不同的Table 环境中。</li>
<li>Table 可以与DataStream或者DataSet进行相互转换。</li>
<li>Streaming SQL不同于存储的SQL，最终会转化为流式执行 计划。</li>
</ol>
</li>
</ol>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20220409230639.png"></p>
<h3 id="Flink-的重要特点-支持有状态计算"><a href="#Flink-的重要特点-支持有状态计算" class="headerlink" title="Flink 的重要特点-支持有状态计算"></a>Flink 的重要特点-支持有状态计算</h3><p>Flink计算引擎支持状态管理，所谓状态管理就是在流式计算过程中将算子的中间结果保存在内存或者 文件系统中，等下一个事件进入算子后可以让当前事件的值与历史值进行汇总累计。 </p>
<p>Flink 维护了两种状态：</p>
<ol>
<li><p>operator state：维护业务执行情况相关状态</p>
</li>
<li><p>keyed state：维护业务执行数据相关状态</p>
</li>
</ol>
<p>同时flink的checkpoint机制可以将状态进行持久化，能够做到失败恢复，具备优秀的容错性关于flink状态管理</p>
<h3 id="Flink-的重要特点-支持exactly-once语义"><a href="#Flink-的重要特点-支持exactly-once语义" class="headerlink" title="Flink 的重要特点-支持exactly-once语义"></a>Flink 的重要特点-支持exactly-once语义</h3><p>exactly-once是消息message 传递分为三种语义中的一种，消息传递三种语义如下：</p>
<ol>
<li><p>At Most once:就是保证提交的数据最多处理一次，存在数据丢失；</p>
</li>
<li><p>At Least once: 就是保证提交的数据最少处理一次，存在重复处理问题；</p>
</li>
<li><p>Exactly once: 就是保证最后的数据处理的结果和数据摄入时没有数据的丢失与重复；</p>
</li>
</ol>
<h3 id="Flink-的重要特点-支持事件时间"><a href="#Flink-的重要特点-支持事件时间" class="headerlink" title="Flink 的重要特点-支持事件时间"></a>Flink 的重要特点-支持事件时间</h3><p>目前大多数框架时间窗口计算，都是采用当前系统时间，以时间为单位进行的聚合计算只能反应数据到达计算引擎的时间，而并不是实际业务时间</p>
<p>Flink 支持不同的时间概念，包括： </p>
<ol>
<li>Event Time ：事件时间 </li>
<li>Ingestion Time ：消息提取时间 </li>
<li>Processing Time ：处理时间</li>
</ol>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20220409230442.png"></p>
<h2 id="Flink-vs-Storm-vs-SparkStreaming"><a href="#Flink-vs-Storm-vs-SparkStreaming" class="headerlink" title="Flink vs Storm vs SparkStreaming"></a>Flink vs Storm vs SparkStreaming</h2><table>
<thead>
<tr>
<th>产品</th>
<th>模型</th>
<th>API</th>
<th>保证次数</th>
<th>容错机制</th>
<th>状态管理</th>
<th>延时</th>
<th>吞吐量</th>
</tr>
</thead>
<tbody><tr>
<td>storm</td>
<td>Native（数据进入立即处理）</td>
<td>组合式（基础API）</td>
<td>At-least-once</td>
<td>Record ACKs（ack机制）</td>
<td>无</td>
<td>Low</td>
<td>Low</td>
</tr>
<tr>
<td>Spark streaming</td>
<td>mirco-batching</td>
<td>声明式（提供封装后的高阶函数，例如count函数）</td>
<td>Exectly-once</td>
<td>RDD Checkpoint（基于RDD做Checkpoint）</td>
<td>基于DStream</td>
<td>Medium</td>
<td>High</td>
</tr>
<tr>
<td>Flink</td>
<td>Native</td>
<td>声明式</td>
<td>Exectly-once</td>
<td>Checkpoint（flink的一种快照）</td>
<td>基于操作</td>
<td>Low</td>
<td>High</td>
</tr>
</tbody></table>
<h3 id="Flink-vs-storm-对比图"><a href="#Flink-vs-storm-对比图" class="headerlink" title="Flink vs storm 对比图"></a>Flink vs storm 对比图</h3><p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20220409231708.png"></p>
<h3 id="实时框架如何选择"><a href="#实时框架如何选择" class="headerlink" title="实时框架如何选择"></a>实时框架如何选择</h3><ol>
<li><p>需要关注流数据是否需要进行状态管理</p>
</li>
<li><p>At-least-once或者Exectly-once消息投递模式是否有特殊要求</p>
</li>
<li><p>对于数据量小，并且需要低延迟的场景，建议使用storm</p>
</li>
<li><p>如果你的项目已经使用了spark，并且秒级别的实时处理可以满足需求的话，建议使用sparkStreaming</p>
</li>
<li><p>要求消息投递语义为 ExactlyOnce的场景；数据量较大，要求高吞吐低延迟的场景；需要进行状态管理或窗口统计的场景，建议使用flink</p>
</li>
</ol>
<h2 id="Flink商业应用"><a href="#Flink商业应用" class="headerlink" title="Flink商业应用"></a>Flink商业应用</h2><ol>
<li><p>针对数据分析团队提供实时流处理服务。通过flink数据分析平台提供实时数据分析服务，及时发现问题。</p>
</li>
<li><p>实时数据仓库</p>
</li>
<li><p>大屏监控、实时风控、实时推荐</p>
</li>
</ol>
<h1 id="Flink-系统基本组件与集群安装部署"><a href="#Flink-系统基本组件与集群安装部署" class="headerlink" title="Flink 系统基本组件与集群安装部署"></a>Flink 系统基本组件与集群安装部署</h1><h2 id="Flink-系统基本组件"><a href="#Flink-系统基本组件" class="headerlink" title="Flink 系统基本组件"></a>Flink 系统基本组件</h2><p>Flink 整个系统主要由两个组件组成，分别为 JobManager 和 TaskManager，Flink 架构也遵循 Master-Slave架构设计原则，JobManager 为 Master 节点，TaskManager 为 Worker（Slave）节点。</p>
<h2 id="Flink-集群安装部署模式"><a href="#Flink-集群安装部署模式" class="headerlink" title="Flink 集群安装部署模式"></a>Flink 集群安装部署模式</h2><p>部署模式：</p>
<ol>
<li><p>Standalone cluster</p>
</li>
<li><p>Yarn</p>
</li>
<li><p>单机模式</p>
</li>
</ol>
<h2 id="Flink-Standalone-集群部署"><a href="#Flink-Standalone-集群部署" class="headerlink" title="Flink-Standalone 集群部署"></a>Flink-Standalone 集群部署</h2><ol>
<li><p>依赖环境： </p>
<ol>
<li><p>JDK环境，1.8.x或者更高，配置好JAVA_HOME;</p>
</li>
<li><p>主机名和hosts配置文件集群内完全对应，准确配置; </p>
<ol>
<li><p>确定各节点hostname 正确设置：</p>
<p>vi &#x2F;etc&#x2F;hostname</p>
</li>
<li><p>设置hosts（ip 根据自己服务器修改）：</p>
<p>vi &#x2F;etc&#x2F;hosts</p>
<p>192.168.254.171 master01<br>192.168.254.172 worker01<br>192.168.254.173 worker02</p>
</li>
</ol>
</li>
<li><p>集群之间保证通信正常，关闭防火墙; </p>
<ol>
<li><p>关闭SELinux:<br>vi &#x2F;etc&#x2F;selinux&#x2F;config，修改如下：<br>SELINUX&#x3D;disabled</p>
</li>
<li><p>关闭防火墙：<br>systemctl stop firewalld<br>systemctl disable firewalld</p>
</li>
</ol>
</li>
<li><p>集群所有节点配置ssh免密; </p>
<ol>
<li>生成密钥：<br>ssh-keygen -t rsa（默认位于 ~&#x2F;.ssh&#x2F;）</li>
<li>拷贝公钥到所有机器：<br>ssh-copy-id root@master01<br>ssh-copy-id root@worker01<br>ssh-copy-id root@worker02</li>
<li>测试免密登录：<br>ssh master01<br>ssh worker01<br>ssh worker02</li>
</ol>
</li>
<li><p>集群配置时间同步服务（ntp）。</p>
<p>将master设置为主服务器（在master节点操作）：</p>
<ol>
<li><p>vi &#x2F;etc&#x2F;ntp.conf，内容如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">driftfile /var/lib/ntp/ntp.drift #草稿文件</span><br><span class="line">允许内网其他机器同步时间（192.168.254.0 修改为自己的ip掩码）</span><br><span class="line">restrict 192.168.254.0 mask 255.255.255.0 nomodify notrap</span><br><span class="line"> </span><br><span class="line"># Use public servers from the pool.ntp.org project.</span><br><span class="line"># 中国这边最活跃的时间服务器 : [http://www.pool.ntp.org/zone/cn](http://www.pool.ntp.org/zone/cn)</span><br><span class="line">server 210.72.145.44 perfer   # 中国国家受时中心</span><br><span class="line">server 202.112.10.36             # 1.cn.pool.ntp.org</span><br><span class="line">server 59.124.196.83             # 0.asia.pool.ntp.org</span><br><span class="line"> </span><br><span class="line"># allow update time by the upper server </span><br><span class="line"># 允许上层时间服务器主动修改本机时间</span><br><span class="line">restrict 210.72.145.44 nomodify notrap noquery</span><br><span class="line">restrict 202.112.10.36 nomodify notrap noquery</span><br><span class="line">restrict 59.124.196.83 nomodify notrap noquery</span><br><span class="line"> </span><br><span class="line"># 外部时间服务器不可用时，以本地时间作为时间服务</span><br><span class="line">server  127.127.1.0     # local clock</span><br><span class="line">fudge   127.127.1.0 stratum 10</span><br></pre></td></tr></table></figure>
</li>
<li><p>重启服务： service ntpd restart</p>
</li>
<li><p>查看同步状态： netstat -tlunp | grep ntp</p>
</li>
</ol>
<p>设置slave到master 的同步（在slave节点操作）：</p>
<ol>
<li><p>vi &#x2F;etc&#x2F;ntp.conf，内容如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">driftfile /var/lib/ntp/ntp.drift # 草稿文件</span><br><span class="line"></span><br><span class="line">statsdir /var/log/ntpstats/</span><br><span class="line">statistics loopstats peerstats clockstats</span><br><span class="line">filegen loopstats file loopstats type day enable</span><br><span class="line">filegen peerstats file peerstats type day enable</span><br><span class="line">filegen clockstats file clockstats type day enable</span><br><span class="line"></span><br><span class="line"># 让NTP Server为内网的ntp服务器（192.168.254.171修改为master节点ip）</span><br><span class="line">server 192.168.254.171</span><br><span class="line">fudge 192.168.254.171 stratum 5</span><br><span class="line"></span><br><span class="line"># 不允许来自公网上ipv4和ipv6客户端的访问</span><br><span class="line">restrict -4 default kod notrap nomodify nopeer noquery </span><br><span class="line">restrict -6 default kod notrap nomodify nopeer noquery</span><br><span class="line"></span><br><span class="line"># Local users may interrogate the ntp server more closely.</span><br><span class="line">restrict 127.0.0.1</span><br><span class="line">restrict ::1</span><br><span class="line"></span><br></pre></td></tr></table></figure>
</li>
<li><p>重启服务： service ntpd restart</p>
</li>
<li><p>手动同步： ntpdate -u 192.168.254.171</p>
</li>
</ol>
</li>
</ol>
</li>
<li><p>Flink软件准备</p>
<p>下载地址：<a target="_blank" rel="noopener" href="https://archive.apache.org/dist/flink/">https://archive.apache.org/dist/flink/</a></p>
<p>我这里使用 flink-1.11.2-bin-scala_2.12.tgz 版本，下载地址：<a target="_blank" rel="noopener" href="https://archive.apache.org/dist/flink/flink-1.11.2/flink-1.11.2-bin-scala_2.12.tgz">https://archive.apache.org/dist/flink/flink-1.11.2/flink-1.11.2-bin-scala_2.12.tgz</a></p>
</li>
<li><p>集群规划 </p>
<p>一主两从：</p>
<p>master01:JobManager </p>
<p>worker01:TaskManager </p>
<p>worker02:TaskManager</p>
</li>
</ol>
<h3 id="Flink-Standalone-安装步骤"><a href="#Flink-Standalone-安装步骤" class="headerlink" title="Flink-Standalone-安装步骤"></a>Flink-Standalone-安装步骤</h3><ol>
<li><p>解压安装包：tar -zxvf flink-1.11.2-bin-scala_2.12.tgz</p>
</li>
<li><p>修改conf&#x2F;flink-conf.yaml</p>
<p>jobmanager.rpc.address: 192.168.254.171</p>
</li>
<li><p>修改<strong>conf&#x2F;workers</strong></p>
<p>192.168.254.172</p>
<p>192.168.254.173</p>
</li>
<li><p>拷贝到其他节点</p>
<p>scp -r &#x2F;root&#x2F;flink-1.11.2 worker01:&#x2F;root&#x2F;</p>
<p>scp -r &#x2F;root&#x2F;flink-1.11.2 worker02:&#x2F;root&#x2F;</p>
</li>
<li><p>配置环境变量 &#x2F;etc&#x2F;profile</p>
<p>#flink</p>
<p>export FLINK_HOME&#x3D;&#x2F;root&#x2F;flink-1.11.2</p>
<p>export PATH&#x3D;$FLINK_HOME&#x2F;bin:$PATH</p>
<p>启动flink集群</p>
<p>start-cluster.sh</p>
</li>
<li><p>访问: <a target="_blank" rel="noopener" href="http://192.168.254.171:8081/">http://192.168.254.171:8081</a></p>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20220409234720.png"></p>
</li>
</ol>
<h3 id="可选配置"><a href="#可选配置" class="headerlink" title="可选配置"></a>可选配置</h3><p>可选配置： </p>
<ul>
<li>每个JobManager（jobmanager.memory.process.size）的可用内存量</li>
<li>每个TaskManager（taskmanager.memory.process.size）的可用内存量</li>
<li>每台机器的可用CPU数量（taskmanager.numberOfTaskSlots）</li>
<li>集群中的CPU总数（parallelism.default）</li>
<li>临时目录（taskmanager.tmp.dirs）</li>
</ul>
<h3 id="Flink-Standalone-集群节点重启及扩容"><a href="#Flink-Standalone-集群节点重启及扩容" class="headerlink" title="Flink-Standalone-集群节点重启及扩容"></a>Flink-Standalone-集群节点重启及扩容</h3><ol>
<li><p>启动jobmanager</p>
<p>如果集群中的jobmanager进程挂了，执行下面命令启动。</p>
<p>jobmanager.sh start</p>
<p>jobmanager.sh stop</p>
</li>
<li><p>启动taskmanager</p>
<p>添加新的taskmanager节点或者重启taskmanager节点</p>
<p>taskmanager.sh start</p>
<p>taskmanager.sh stop</p>
</li>
<li><p>一次性全部停止</p>
<p>stop-cluster.sh</p>
</li>
</ol>
<h3 id="Flink-Standalone-集群任务提交测试"><a href="#Flink-Standalone-集群任务提交测试" class="headerlink" title="Flink-Standalone-集群任务提交测试"></a>Flink-Standalone-集群任务提交测试</h3><p>Flink安装包的examples目录下有很多官方开发好的案例，我们可以拿来测试我们的集群SocketWindowWordCount.jar：官方这个例子是通过监听socket的方式测试。</p>
<p>独立运行模式： </p>
<ul>
<li><p>启动监听：</p>
<p>nc -l 9000</p>
</li>
<li><p>在集群节点上运行</p>
<p>flink run &#x2F;root&#x2F;flink-1.11.2&#x2F;examples&#x2F;streaming&#x2F;SocketWindowWordCount.jar –hostname 192.168.254.172 –port 9000</p>
</li>
<li><p>到taskmanager或者webui下面看标准输出日志</p>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20220409235809.png"></p>
</li>
</ul>
<h3 id="Flink-Standalone-集群任务提交方式"><a href="#Flink-Standalone-集群任务提交方式" class="headerlink" title="Flink-Standalone-集群任务提交方式"></a>Flink-Standalone-集群任务提交方式</h3><p>常用提交方式分为local，standalone，yarn三种。 </p>
<ul>
<li>local：本地提交项目，可纯粹的在本地单节点运行，也可以将本地代码提交到远端flink集群运行。</li>
<li>standalone：flink集群自己完成资源调度，不依赖于其他资源调度器，需要手动启动flink集群。 </li>
<li>yarn：依赖于hadoop yarn资源调度器，由yarn负责资源调度，不需要手动启动flink集群。需要先启动yarn和hdfs。又分为yarn-session和yarn-cluster两种方式。</li>
</ul>
<h2 id="Flink-on-Yarn-集群部署"><a href="#Flink-on-Yarn-集群部署" class="headerlink" title="Flink on Yarn 集群部署"></a>Flink on Yarn 集群部署</h2><ol>
<li>Flink on Yarn集群部署（有条件自己尝试）<ol>
<li>至少需要Apache Hadoop 2.2以上版本，并在每个节点上配置好HADOOP_HOME</li>
<li>启动HDFS和 Yarn集群</li>
<li>由于Flink从1.8版本开始Flink官方是将hadoop以及其他的一些依赖包没有编译进Flink的lib目录中，所以需要自己编译编译flink-shaded，并将编译好的Flink依赖 的hadoop相关jar包上传到Flink节点客 户端</li>
</ol>
</li>
<li>具体编译步骤<ol>
<li>下载flink-shaded源码包，并解压</li>
<li>使用IDEA开发工具打开flink-shaded项目进行编译，然后打成jar包</li>
<li>将hadoop相关的jar包上传到flink客户端lib目录下</li>
</ol>
</li>
</ol>
<h2 id="Flink-on-Yarn-两种使用方式"><a href="#Flink-on-Yarn-两种使用方式" class="headerlink" title="Flink on Yarn 两种使用方式"></a>Flink on Yarn 两种使用方式</h2><ol>
<li><p>yarn session模式(先启动集群，然后再提交任务)。所有job共用一套集群，资源按需申请一次，任务结束后回收空闲的TaskManager相关计算资源; </p>
<ol>
<li><p>启动一个一直运行的flink集群:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./bin/yarn-session.sh -d -jm 1024 -tm 2048 -s 2 </span><br></pre></td></tr></table></figure>
</li>
<li><p>执行任务: </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./bin/flink run -d -p 3 ./examples/streaming/TopSpeedWindowing.jar</span><br></pre></td></tr></table></figure>
</li>
<li><p>默认查找当前yarn集群中已有的yarn-session信息中的jobmanager【&#x2F;tmp&#x2F;.yarn-properties-root】</p>
</li>
</ol>
</li>
<li><p>yarn cluster开辟资源+提交任务：每一个job都重新启动一个Flink集群，完成后结束Flink集群，释放资源。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./bin/flink run -d -m yarn-cluster -p 2 -yjm 1024m -ytm 1024m./examples/streaming/TopSpeedWindowing.jar</span><br></pre></td></tr></table></figure></li>
</ol>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20220410113222.png"></p>
<h2 id="命令参数"><a href="#命令参数" class="headerlink" title="命令参数"></a>命令参数</h2><p>启动yarn-session：yarn-session.sh -n 4 -tm 8192 -s 8</p>
<p>-n,–container 指YARN container分配的个数(即TaskManagers的个数) </p>
<p>-jm,–jobManagerMemory 指JobManager Containe的内存大小，单位为MB</p>
<p>-tm,–taskManagerMemory 指每个TaskManagerContainer的内存大小，单位为MB</p>
<p>-s 指每个TaskManager的slot个数。</p>
<p>flink run参数：flink run命令执行模板： </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">flink run [option] &lt;jar-file&gt; &lt;arguments&gt;</span><br><span class="line"></span><br><span class="line">-c,--class &lt;classname&gt; : 需要指定的main方法的类</span><br><span class="line"></span><br><span class="line">-m,--jobmanager : yarn-cluster集群</span><br><span class="line"></span><br><span class="line">-d,--detached : 在后台运行</span><br><span class="line"></span><br><span class="line">-p,--parallelism &lt;parallelism&gt; : job需要指定<span class="built_in">env</span>的并行度，这个一般都需要设置。 </span><br><span class="line"></span><br><span class="line">-C,--classpath &lt;url&gt; : 向每个用户代码添加url，他是通过UrlClassLoader加载。url需要指定文件的schema如（file://） </span><br><span class="line"></span><br><span class="line">-s,--fromSavepoint &lt;savepointPath&gt; : 基于savepoint保存下来的路径，进行恢复。</span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">flink run -m yarn-cluster参数 </span><br><span class="line"></span><br><span class="line">-yd,--yarndetached : 后台 </span><br><span class="line"></span><br><span class="line">-yjm,--yarnjobManager : jobmanager的内存</span><br><span class="line"></span><br><span class="line">-ytm,--yarntaskManager : taskmanager的内存</span><br><span class="line"></span><br><span class="line">-yn,--yarncontainer : TaskManager的个数</span><br><span class="line"></span><br><span class="line">-yid,--yarnapplicationId : job依附的applicationId</span><br><span class="line"></span><br><span class="line">-ynm,--yarnname : application的名称 </span><br><span class="line"></span><br><span class="line">-ys,--yarnslots : 分配的slots个数</span><br><span class="line"></span><br><span class="line">flink run -m yarn-cluster -yd -yjm 1024m -ytm 1024m -ynm &lt;name&gt; -ys 1 &lt;jar&gt; &lt;arguments&gt;</span><br></pre></td></tr></table></figure>

<h2 id="Flink-提交任务流程初步分析"><a href="#Flink-提交任务流程初步分析" class="headerlink" title="Flink 提交任务流程初步分析"></a>Flink 提交任务流程初步分析</h2><p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20220410114215.png"></p>
<h1 id="Flink开发环境搭建"><a href="#Flink开发环境搭建" class="headerlink" title="Flink开发环境搭建"></a>Flink开发环境搭建</h1><h2 id="Flink-开发内容"><a href="#Flink-开发内容" class="headerlink" title="Flink 开发内容"></a>Flink 开发内容</h2><p>flink的API包含：DataStream、DataSet、Table、SQLAPI</p>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20220410115656.png"></p>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20220409230639.png"></p>
<p>datastream 和 dataset api是Flink的核心api，因此Flink的开发内容为面向这两个Api的：</p>
<ol>
<li>基于DataStreamAPI的流处理</li>
<li>基于DataSetAPI批处理</li>
</ol>
<h2 id="Flink-开发步骤"><a href="#Flink-开发步骤" class="headerlink" title="Flink 开发步骤"></a>Flink 开发步骤</h2><ol>
<li><p>获取一个Flink程序执行环境(execution environment)</p>
</li>
<li><p>加载&#x2F;创建初始数据（数据源source）</p>
</li>
<li><p>指定此数据的转换（transformation）</p>
</li>
<li><p>指定放置计算结果的位置</p>
</li>
<li><p>触发程序执行</p>
</li>
</ol>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20220410115734.png"></p>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20220410115755.png"></p>
<h2 id="Flink开发环境搭建-1"><a href="#Flink开发环境搭建-1" class="headerlink" title="Flink开发环境搭建"></a>Flink开发环境搭建</h2><ol>
<li><p>开发工具</p>
<ol>
<li>官方建议使用Intellij IDEA，因为它默认集成scala和maven环境，使用更加方便； </li>
<li>Flink支持java和scala两种语言开发，但是个人建议，使用scala，因为实现起来更加简洁； </li>
<li>使用maven管理项目依赖； </li>
<li>JDK使用1.8+</li>
</ol>
</li>
<li><p>Flink - scala依赖</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-scala_2.12<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.10.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-streaming-scala_2.12<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.10.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure></li>
</ol>
<h2 id="Flink入门程序开发"><a href="#Flink入门程序开发" class="headerlink" title="Flink入门程序开发"></a>Flink入门程序开发</h2><ol>
<li>需求：从指定的socket端口实时获取数据，进行词频统计，将结果进行打印输出；</li>
<li>需求：读取文件中的数据，进行词频统计，将结果进行打印输出；</li>
</ol>
<p><strong>Java 代码</strong></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.study.flink.java;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.functions.FlatMapFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.functions.MapFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.tuple.Tuple;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.tuple.Tuple2;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.DataStreamSource;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.KeyedStream;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.util.Collector;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 1. 需求：从指定的socket端口实时获取数据，进行词频统计，将结果进行打印输出；</span></span><br><span class="line"><span class="comment"> * 2. 需求：读取文件中的数据，进行词频统计，将结果进行打印输出；</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">DataStreamWordCountDemo</span>  &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">        <span class="keyword">if</span> (args == <span class="literal">null</span> || args.length &lt; <span class="number">2</span>) &#123;</span><br><span class="line">            System.err.println(<span class="string">&quot;Usage: DataStreamWordCountDemo &lt;hostname&gt; &lt;port&gt;&quot;</span>);</span><br><span class="line">            System.exit(<span class="number">1</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="type">String</span> <span class="variable">hostname</span> <span class="operator">=</span> args[<span class="number">0</span>];</span><br><span class="line">        String port= args[<span class="number">1</span>];</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 1、获取执行环境</span></span><br><span class="line">        <span class="type">StreamExecutionEnvironment</span> <span class="variable">env</span> <span class="operator">=</span> StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2、创建初始的数据流</span></span><br><span class="line">        DataStreamSource&lt;String&gt; ds = env.socketTextStream(hostname, Integer.parseInt(port));</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 3、使用 transform 算子对初始的流进行转换操作</span></span><br><span class="line">        <span class="comment">// 对每一行输入的字符进行切分成一个一个单词</span></span><br><span class="line">        SingleOutputStreamOperator&lt;String&gt; wordsDS = ds.flatMap(<span class="keyword">new</span> <span class="title class_">FlatMapFunction</span>&lt;String, String&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">flatMap</span><span class="params">(String lineStr, Collector&lt;String&gt; out)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">                String[] words = lineStr.split(<span class="string">&quot; &quot;</span>);</span><br><span class="line">                <span class="keyword">for</span> (String word : words) &#123;</span><br><span class="line">                    out.collect(word);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">        <span class="comment">// 将每个单词组成元组（单词，数量）</span></span><br><span class="line">        SingleOutputStreamOperator&lt;Tuple2&lt;String, Integer&gt;&gt; tuple2DS = wordsDS.map(<span class="keyword">new</span> <span class="title class_">MapFunction</span>&lt;String, Tuple2&lt;String, Integer&gt;&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="keyword">public</span> Tuple2&lt;String, Integer&gt; <span class="title function_">map</span><span class="params">(String value)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">new</span> <span class="title class_">Tuple2</span>&lt;&gt;(value, <span class="number">1</span>);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">        <span class="comment">// 按单词进行分区，相同的单词需要到同一个分区</span></span><br><span class="line">        KeyedStream&lt;Tuple2&lt;String, Integer&gt;, Tuple&gt; keybyDS = tuple2DS.keyBy(<span class="number">0</span>);</span><br><span class="line">        <span class="comment">// 对key相同的value值进行累计求和</span></span><br><span class="line">        SingleOutputStreamOperator&lt;Tuple2&lt;String, Integer&gt;&gt; wordCountDS = keybyDS.sum(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 4、将操作的结果保存到目的地</span></span><br><span class="line">        wordCountDS.print();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 5、触发程序执行</span></span><br><span class="line">        env.execute(<span class="string">&quot;DataStreamWordCountDemoJava&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><strong>Scala 代码</strong></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.study.flink.scala</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala.&#123;<span class="type">DataStream</span>, <span class="type">StreamExecutionEnvironment</span>, createTypeInformation&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 1. 需求：从指定的socket端口实时获取数据，进行词频统计，将结果进行打印输出；</span></span><br><span class="line"><span class="comment"> * 2. 需求：读取文件中的数据，进行词频统计，将结果进行打印输出；</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">DataStreamWordCountDemo</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">if</span> (args.length &lt; <span class="number">2</span>) &#123;</span><br><span class="line">      <span class="type">System</span>.err.println(<span class="string">&quot;Usage: DataStreamWordCountDemo &lt;hostname&gt; &lt;port&gt;&quot;</span>)</span><br><span class="line">      <span class="type">System</span>.exit(<span class="number">1</span>)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> hostname = args(<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">val</span> port = args(<span class="number">1</span>).toInt</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 1、获取执行环境</span></span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 2、获取数据，创建数据流</span></span><br><span class="line">    <span class="keyword">val</span> lineStrDs = env.socketTextStream(hostname, port)</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 3、计算词频统计</span></span><br><span class="line"><span class="comment">     * hello flink hello kafka</span></span><br><span class="line"><span class="comment">     * hello hadoop</span></span><br><span class="line"><span class="comment">     * [hello, flink, hello, kafka],[hello, hadoop]</span></span><br><span class="line"><span class="comment">     * [(hello,1),(flink,1), (hello,1),(kafka,1)]</span></span><br><span class="line"><span class="comment">     * [(hello,1),(hello,1)], [(flink,1)], [(kafka,1)]</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">val</span> wordCountDs = lineStrDs</span><br><span class="line">      .flatMap(x =&gt; x.split(<span class="string">&quot; &quot;</span>))</span><br><span class="line">      .map(x =&gt; (x, <span class="number">1</span>))</span><br><span class="line">      .keyBy(<span class="number">0</span>)</span><br><span class="line">      .sum(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 4、输出：控制台</span></span><br><span class="line">    wordCountDs.print()</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 5、触发程序执行</span></span><br><span class="line">    env.execute(<span class="string">&quot;DataStreamWordCountDemoScala&quot;</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="打包集群运行"><a href="#打包集群运行" class="headerlink" title="打包集群运行"></a>打包集群运行</h2><p>Standalone集群上运行</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">flink run -c com.study.flink.java.DataStreamWordCountDemo /root/flink-study-1.0-SNAPSHOT-jar-with-dependencies.jar 192.168.254.172 9000</span><br><span class="line"></span><br><span class="line">flink run -c com.study.flink.scala.DataStreamWordCountDemo /root/flink-study-1.0-SNAPSHOT-jar-with-dependencies.jar 192.168.254.172 9000</span><br></pre></td></tr></table></figure>

<p>打包项目至yarn集群上运行需要注意：</p>
<ol>
<li>flink集群上有的jar包在项目中需要使用scope属性provided排除</li>
</ol>
<p>提交流处理任务执行命令：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">flink run -d -m yarn-cluster -yjm 1024 -ytm 1024 -c com.study.flink.java.DataStreamWordCountDemo /root/flink-study-1.0-SNAPSHOT-jar-with-dependencies.jar 192.168.254.172 9000</span><br></pre></td></tr></table></figure>

<p>提交流批处理任务执行命令：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/flink run -m yarn-cluster -yjm 1024 -ytm 1024 -c com.study.flink.java.DataStreamWordCountDemo /root/flink-study-1.0-SNAPSHOT-jar-with-dependencies.jar --inputpath hdfs://xxxx:xxxx/flink/wc/wc.txt</span><br></pre></td></tr></table></figure>

<h1 id="思考和操作"><a href="#思考和操作" class="headerlink" title="思考和操作"></a>思考和操作</h1><ol>
<li><p>理解flink的特性</p>
</li>
<li><p>安装flink集群</p>
<p>思考：如何搭建HA的集群</p>
</li>
<li><p>搭建开发环境，并提交一个任务</p>
</li>
</ol>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2023/09/10/13-%E5%A4%A7%E6%95%B0%E6%8D%AE/04-Flink/01-Flink%E5%9F%BA%E7%A1%80/" data-id="clmcxec840017u8waheg8840f" data-title="" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-13-大数据/03-Spark/04-StructuredStreaming" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/09/10/13-%E5%A4%A7%E6%95%B0%E6%8D%AE/03-Spark/04-StructuredStreaming/" class="article-date">
  <time class="dt-published" datetime="2023-09-10T03:59:07.652Z" itemprop="datePublished">2023-09-10</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="什么是-Structured-Streaming？"><a href="#什么是-Structured-Streaming？" class="headerlink" title="什么是 Structured Streaming？"></a>什么是 Structured Streaming？</h1><h2 id="什么是“流式计算”？"><a href="#什么是“流式计算”？" class="headerlink" title="什么是“流式计算”？"></a>什么是“流式计算”？</h2><p>批量计算：Hadoop、Spark</p>
<ul>
<li>有限的数据</li>
<li>离线：T+1</li>
</ul>
<p>流式计算：Storm、SparkStreaming、Flink</p>
<ul>
<li>无限的数据</li>
<li>实时：T+s&#x2F;T+m</li>
</ul>
<h2 id="Spark中的流式计算解决方案"><a href="#Spark中的流式计算解决方案" class="headerlink" title="Spark中的流式计算解决方案"></a>Spark中的流式计算解决方案</h2><p>Spark Streaming(已停更)-&gt;  Structured Streaming</p>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20220404173049.png"></p>
<h2 id="Structured-Streaming-vs-Spark-Streaming"><a href="#Structured-Streaming-vs-Spark-Streaming" class="headerlink" title="Structured Streaming vs Spark Streaming"></a>Structured Streaming vs Spark Streaming</h2><ol>
<li>支持event-time，提供处理迟到数据的机制</li>
<li>基于Dataset&#x2F;DataFrame，和批处理API统一</li>
</ol>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20220404173207.png"></p>
<h2 id="初识Structured-Streaming"><a href="#初识Structured-Streaming" class="headerlink" title="初识Structured Streaming"></a>初识Structured Streaming</h2><h3 id="实时版的WordCount程序"><a href="#实时版的WordCount程序" class="headerlink" title="实时版的WordCount程序"></a>实时版的WordCount程序</h3><p><a target="_blank" rel="noopener" href="http://spark.incubator.apache.org/docs/latest/structured-streaming-programming-guide.html">官网文档</a></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions._</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">SparkSession</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> spark = <span class="type">SparkSession</span></span><br><span class="line">  .builder</span><br><span class="line">  .appName(<span class="string">&quot;StructuredNetworkWordCount&quot;</span>)</span><br><span class="line">  .getOrCreate()</span><br><span class="line">  </span><br><span class="line"><span class="keyword">import</span> spark.implicits._</span><br><span class="line"></span><br><span class="line"><span class="comment">// Create DataFrame representing the stream of input lines from connection to localhost:9999</span></span><br><span class="line"><span class="keyword">val</span> lines = spark.readStream</span><br><span class="line">  .format(<span class="string">&quot;socket&quot;</span>)</span><br><span class="line">  .option(<span class="string">&quot;host&quot;</span>, <span class="string">&quot;localhost&quot;</span>)</span><br><span class="line">  .option(<span class="string">&quot;port&quot;</span>, <span class="number">9999</span>)</span><br><span class="line">  .load()</span><br><span class="line"></span><br><span class="line"><span class="comment">// Split the lines into words</span></span><br><span class="line"><span class="keyword">val</span> words = lines.as[<span class="type">String</span>].flatMap(_.split(<span class="string">&quot; &quot;</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">// Generate running word count</span></span><br><span class="line"><span class="keyword">val</span> wordCounts = words.groupBy(<span class="string">&quot;value&quot;</span>).count()</span><br><span class="line"></span><br><span class="line"><span class="comment">// Start running the query that prints the running counts to the console</span></span><br><span class="line"><span class="keyword">val</span> query = wordCounts.writeStream</span><br><span class="line">  .outputMode(<span class="string">&quot;complete&quot;</span>)</span><br><span class="line">  .format(<span class="string">&quot;console&quot;</span>)</span><br><span class="line">  .start()</span><br><span class="line"></span><br><span class="line">query.awaitTermination()</span><br></pre></td></tr></table></figure>

<h3 id="图解WordCount的计算过程"><a href="#图解WordCount的计算过程" class="headerlink" title="图解WordCount的计算过程"></a>图解WordCount的计算过程</h3><p>数据源-&gt;数据切分点-&gt;输入表-&gt;查询-&gt;结果表-&gt;输出</p>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20220404173526.png"></p>
<h1 id="如何编写Streaming程序？"><a href="#如何编写Streaming程序？" class="headerlink" title="如何编写Streaming程序？"></a>如何编写Streaming程序？</h1><h2 id="一个完整的Streaming程序"><a href="#一个完整的Streaming程序" class="headerlink" title="一个完整的Streaming程序"></a>一个完整的Streaming程序</h2><p>官网代码：<a target="_blank" rel="noopener" href="https://github.com/apache/spark/blob/v3.2.1/examples/src/main/scala/org/apache/spark/examples/sql/streaming/StructuredNetworkWordCount.scala">https://github.com/apache/spark/blob/v3.2.1/examples/src/main/scala/org/apache/spark/examples/sql/streaming/StructuredNetworkWordCount.scala</a></p>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20220404173846.png"></p>
<h2 id="真实的使用场景"><a href="#真实的使用场景" class="headerlink" title="真实的使用场景"></a>真实的使用场景</h2><ul>
<li>实时监控</li>
<li>实时风控</li>
<li>实时营销&#x2F;推荐</li>
</ul>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20220404185548.png"></p>
<h2 id="Input"><a href="#Input" class="headerlink" title="Input"></a>Input</h2><h3 id="数据源有哪些？"><a href="#数据源有哪些？" class="headerlink" title="数据源有哪些？"></a>数据源有哪些？</h3><table>
<thead>
<tr>
<th>Source</th>
<th>Options</th>
</tr>
</thead>
<tbody><tr>
<td>File</td>
<td>path：文件目录<br />maxFilesPerTrigger：每次计算多少个文件<br />latestFirst：是否先处理最新的新文件<br />fileNameOnly：判断新文件时只基于文件名称（不同路径下同名文件会去重），而不是全路径</td>
</tr>
<tr>
<td>Socket</td>
<td>host：IP&#x2F;域名<br />port：端口</td>
</tr>
<tr>
<td>Rate</td>
<td>rowsPerSecond：每秒产生多少条记录<br />rampUpTime：达到速率rowsPerSecond所用时间<br />numPartitions：分区数</td>
</tr>
<tr>
<td>Kafka</td>
<td>参考“Kafka集成”</td>
</tr>
</tbody></table>
<p>org.apache.spark.sql.execution.streaming.Source</p>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20220404190143.png"></p>
<h2 id="算子"><a href="#算子" class="headerlink" title="算子"></a>算子</h2><h3 id="可以对数据进行哪些计算？"><a href="#可以对数据进行哪些计算？" class="headerlink" title="可以对数据进行哪些计算？"></a>可以对数据进行哪些计算？</h3><table>
<thead>
<tr>
<th>计算类型</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>基础操作</td>
<td>Selection（筛选过滤），Projection（多对一聚合、一对多发散）</td>
</tr>
<tr>
<td><strong>窗口操作</strong></td>
<td>基于EventTime和Watermark</td>
</tr>
<tr>
<td>Join操作</td>
<td>Stream - Static<br />Stream - Stream<br />Static - Stream</td>
</tr>
<tr>
<td>聚合操作</td>
<td>agg、count、sum、avg</td>
</tr>
<tr>
<td><strong>状态操作</strong></td>
<td>mapGroupsWithState<br />flatMapGroupsWithState</td>
</tr>
</tbody></table>
<p>org.apache.spark.sql.Dataset</p>
<p>在流式计算中比较特殊的有<strong>窗口操作、状态操作</strong></p>
<p>根据<strong>状态操作</strong>进行<strong>持续计算</strong><br>    计算效率上来说，流式计算比批量计算效率更高，持续在算，每次计算数据量小，对于集群压力是比较小的，提高集群利用率，不需要大量资源</p>
<p>批量计算PV：T+1计算+实时T<br>流式计算PV：SUM（每分钟计算）</p>
<p>实际工作场景中，流式计算需要保证每分钟计算是正确的，需要做到这一点也是有代价的，所以通常我们还是以批量计算这种方式，基于数据可靠性和成本之间来做选择</p>
<h3 id="哪些操作类型不支持？"><a href="#哪些操作类型不支持？" class="headerlink" title="哪些操作类型不支持？"></a>哪些操作类型不支持？</h3><p><a target="_blank" rel="noopener" href="http://spark.incubator.apache.org/docs/latest/structured-streaming-programming-guide.html#unsupported-operations">官网文档</a></p>
<p>There are a few DataFrame&#x2F;Dataset operations that are not supported with streaming DataFrames&#x2F;Datasets. Some of them are as follows.</p>
<ul>
<li>Multiple streaming aggregations (i.e. a chain of aggregations on a streaming DF) are not yet supported on streaming Datasets.</li>
<li>Limit and take the first N rows are not supported on streaming Datasets.</li>
<li>Distinct operations on streaming Datasets are not supported.</li>
<li>Deduplication operation is not supported after aggregation on a streaming Datasets.</li>
<li>Sorting operations are supported on streaming Datasets only after an aggregation and in Complete Output Mode.</li>
<li>Few types of outer joins on streaming Datasets are not supported. See the <a target="_blank" rel="noopener" href="http://spark.incubator.apache.org/docs/latest/structured-streaming-programming-guide.html#support-matrix-for-joins-in-streaming-queries">support matrix in the Join Operations section</a> for more details.</li>
</ul>
<p>In addition, there are some Dataset methods that will not work on streaming Datasets. They are actions that will immediately run queries and return results, which does not make sense on a streaming Dataset. Rather, those functionalities can be done by explicitly starting a streaming query (see the next section regarding that).</p>
<ul>
<li><code>count()</code> - Cannot return a single count from a streaming Dataset. Instead, use <code>ds.groupBy().count()</code> which returns a streaming Dataset containing a running count.</li>
<li><code>foreach()</code> - Instead use <code>ds.writeStream.foreach(...)</code> (see next section).</li>
<li><code>show()</code> - Instead use the console sink (see next section).</li>
</ul>
<p>If you try any of these operations, you will see an <code>AnalysisException</code> like “operation XYZ is not supported with streaming DataFrames&#x2F;Datasets”. While some of them may be supported in future releases of Spark, there are others which are fundamentally hard to implement on streaming data efficiently. For example, sorting on the input stream is not supported, as it requires keeping track of all the data received in the stream. This is therefore fundamentally hard to execute efficiently.</p>
<p><strong>翻译</strong></p>
<p>有一些DataFrame&#x2F;Dataset操作不支持流式DataFrame&#x2F;Dataset。其中一些如下。</p>
<ul>
<li>流数据集还不支持多个流聚合(即流DF上的聚合链)。</li>
<li>限制和取前N行不支持流数据集。</li>
<li>不支持流式数据集上的不同操作。</li>
<li>流数据集聚合后，不支持重复数据删除操作。</li>
<li>只有在聚合和完全输出模式下，流数据集才支持排序操作。</li>
<li>流数据集上很少类型的外部连接是不支持的。有关更多细节，请参阅<a target="_blank" rel="noopener" href="http://spark.incubator.apache.org/docs/latest/structured-streaming-programming-guide.html#support-matrix-for-joins-in-streaming-queries">Join Operations部分中的支持矩阵</a>。</li>
</ul>
<p>此外，还有一些数据集方法不能用于流数据集。它们是立即运行查询并返回结果的操作，这在流数据集上是没有意义的。相反，这些功能可以通过显式启动流查询来实现(请参阅下一节)。</p>
<ul>
<li>‘ count() ‘ -不能从流数据集返回单个计数。相反，使用’ ds.groupBy().count() ‘返回一个包含运行计数的流数据集。</li>
<li>‘ foreach() ‘ -使用’ ds.writeStream.foreach(…)’(见下一节)。</li>
<li>‘ show() ‘ -使用控制台接收器(见下一节)。</li>
</ul>
<p>如果你尝试任何这些操作，你会看到一个’ AnalysisException ‘像“操作XYZ不支持流数据帧&#x2F;数据集”。虽然其中一些可能会在未来的Spark版本中得到支持，但还有一些从根本上来说很难在流数据上有效地实现。例如，不支持对输入流进行排序，因为它需要跟踪流中接收到的所有数据。因此，从根本上来说，这很难有效地执行。</p>
<h3 id="Trigger"><a href="#Trigger" class="headerlink" title="Trigger"></a>Trigger</h3><h4 id="何时计算数据？"><a href="#何时计算数据？" class="headerlink" title="何时计算数据？"></a>何时计算数据？</h4><table>
<thead>
<tr>
<th>Trigger Type</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>Default</td>
<td>未显示指定时，采用“微批次”模式，上一个批次计算完成，下一个批次立即执行</td>
</tr>
<tr>
<td>微批次：定时</td>
<td>按指定时间间隔生成batch，无新数据不生产batch</td>
</tr>
<tr>
<td>微批次：一次性</td>
<td>只计算一次即停止运行</td>
</tr>
<tr>
<td>持续处理</td>
<td>不间断运行</td>
</tr>
</tbody></table>
<p>org.apache.spark.sql.streaming.Trigger</p>
<h3 id="Continuous-Processing的局限性"><a href="#Continuous-Processing的局限性" class="headerlink" title="Continuous Processing的局限性"></a>Continuous Processing的局限性</h3><p>As of Spark 2.4, only the following type of queries are supported in the continuous processing mode.</p>
<ul>
<li>Operations: Only map-like Dataset&#x2F;DataFrame operations are supported in continuous mode, that is, only projections (select, map, flatMap, mapPartitions, etc.) and selections (where, filter, etc.).<ul>
<li>All SQL functions are supported except aggregation functions (since aggregations are not yet supported), current_timestamp() and current_date() (deterministic computations using time is challenging).</li>
</ul>
</li>
<li>Sources:<ul>
<li>Kafka source: All options are supported.</li>
<li>Rate source: Good for testing. Only options that are supported in the continuous mode are numPartitions and rowsPerSecond.</li>
</ul>
</li>
<li>Sinks:<ul>
<li>Kafka sink: All options are supported.</li>
<li>Memory sink: Good for debugging.</li>
<li>Console sink: Good for debugging. All options are supported. Note that the console will print every checkpoint interval that you have specified in the continuous trigger.</li>
</ul>
</li>
</ul>
<p><strong>翻译</strong></p>
<p>从Spark 2.4开始，连续处理模式只支持以下类型的查询。</p>
<ul>
<li><p>Operations:在连续模式下只支持类似map的Dataset&#x2F;DataFrame操作，即只支持投影(select, map, flatMap, mapPartitions等)和选择(where, filter等)。</p>
<ul>
<li>除了聚合函数(因为聚合还不支持)、current_timestamp()和current_date()(使用时间进行确定性计算是一个挑战)外，所有的SQL函数都支持。</li>
</ul>
</li>
<li><p>来源:</p>
<ul>
<li>Kafka source:支持所有选项。</li>
<li>速率来源:适合测试。连续模式下支持的选项只有numPartitions和rowsPerSecond。</li>
</ul>
</li>
<li><p>接收器:</p>
<ul>
<li>Kafka sink:支持所有选项。</li>
<li>内存接收器:适合调试。</li>
<li>控制台接收器:适合调试。支持所有选项。请注意，控制台将打印您在连续触发器中指定的每个检查点间隔。</li>
</ul>
</li>
</ul>
<h2 id="事件时间"><a href="#事件时间" class="headerlink" title="事件时间"></a>事件时间</h2><h3 id="代码示例"><a href="#代码示例" class="headerlink" title="代码示例"></a>代码示例</h3><p>每个5分钟统计最近10分钟之内的单词count数</p>
<p>8:30<br>第一个窗口：8:30-8:40<br>第二个窗口：8:35-8:45</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> spark.implicits._</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> words = ... <span class="comment">// streaming DataFrame of schema &#123; timestamp: Timestamp, word: String &#125;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Group the data by window and word and compute the count of each group</span></span><br><span class="line"><span class="keyword">val</span> windowedCounts = words.groupBy(</span><br><span class="line">  window($<span class="string">&quot;timestamp&quot;</span>, <span class="string">&quot;10 minutes&quot;</span>, <span class="string">&quot;5 minutes&quot;</span>),</span><br><span class="line">  $<span class="string">&quot;word&quot;</span></span><br><span class="line">).count()</span><br></pre></td></tr></table></figure>

<h3 id="计算过程"><a href="#计算过程" class="headerlink" title="计算过程"></a>计算过程</h3><p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20220404211930.png"></p>
<h3 id="处理延迟数据"><a href="#处理延迟数据" class="headerlink" title="处理延迟数据"></a>处理延迟数据</h3><p>12:06分产生的数据，12:16才到达计算引擎</p>
<ol>
<li>严格按照窗口时间执行计算：数据丢失，结果不准确</li>
<li>等待所有数据到达再执行计算：计算时间无限推迟，有大量状态需要保存</li>
</ol>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20220404212012.png"></p>
<h3 id="乱序数据处理：水位线"><a href="#乱序数据处理：水位线" class="headerlink" title="乱序数据处理：水位线"></a>乱序数据处理：水位线</h3><p>引入水位线：窗口允许的最大延迟时间</p>
<p>trigger：5分钟触发一次</p>
<p>watermark：10分钟</p>
<p>蓝色线：实际数据</p>
<p>橙色线：水位线</p>
<p>以12:10要开始计算为例，如果<strong>数据到达时间</strong>小于<strong>开始计算时间+watermark时间</strong>则放入到窗口处理</p>
<p>如下图12:08数据在大约12:17左右到达，在12:10计算窗口内，水位线为10分钟，也就是在12:20之前到达的数据都会放入12:10计算窗口内。下图12:04数据在12:22左右到达，超过12:20则不放入12:10计算窗口。</p>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20220404212107.png"> </p>
<h2 id="结果输出"><a href="#结果输出" class="headerlink" title="结果输出"></a>结果输出</h2><ul>
<li>Complete：全量输出，输出<strong>整个</strong>Result Table数据</li>
<li>Update：增量输出，只输出Result Table中<strong>被修改</strong>的数据</li>
<li>Append : 追加输出，只输出<strong>新添加</strong>进Result Table的数据。根据水位线判断数据不会再发生变换则输出数据</li>
</ul>
<p>org.apache.spark.sql.streaming.OutputMode</p>
<h3 id="Output-Mode-Complete"><a href="#Output-Mode-Complete" class="headerlink" title="Output Mode - Complete"></a>Output Mode - Complete</h3><p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20220404222459.png"></p>
<h3 id="Output-Mode-Append"><a href="#Output-Mode-Append" class="headerlink" title="Output Mode - Append"></a>Output Mode - Append</h3><p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20220404222511.png"></p>
<p>在Append模式下，Structured Streaming需要知道，某一条key的结果什么时候不会再更新了。当确认结果不会再更新的时候，就可以将结果进行输出 。（依靠watermark确认结果不再进行更新）</p>
<h3 id="Output-Mode-Update"><a href="#Output-Mode-Update" class="headerlink" title="Output Mode - Update"></a>Output Mode - Update</h3><p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20220404222633.png"></p>
<h3 id="Output-Mode-不同的Output-Mode对应不同的Query类型"><a href="#Output-Mode-不同的Output-Mode对应不同的Query类型" class="headerlink" title="Output Mode - 不同的Output Mode对应不同的Query类型"></a>Output Mode - 不同的Output Mode对应不同的Query类型</h3><table>
<thead>
<tr>
<th>Query Type</th>
<th>Output Mode</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>aggregation:基于event-time并带watermark</td>
<td>Append<br />Update<br />Complete</td>
<td>唯一支持3种模式的Query</td>
</tr>
<tr>
<td>aggregation:其他</td>
<td>Update<br />Complete</td>
<td></td>
</tr>
<tr>
<td>mapGroupsWithState</td>
<td>Update</td>
<td></td>
</tr>
<tr>
<td>flatMapGroupsWithState</td>
<td>Append</td>
<td></td>
</tr>
<tr>
<td>joins</td>
<td>Append</td>
<td></td>
</tr>
<tr>
<td>除上之外的Query</td>
<td>Append<br />Update</td>
<td>非聚合查询如支持complete模式，会造成Result Table爆炸</td>
</tr>
</tbody></table>
<h3 id="Output-Sink-将数据输出到哪里？"><a href="#Output-Sink-将数据输出到哪里？" class="headerlink" title="Output Sink - 将数据输出到哪里？"></a>Output Sink - 将数据输出到哪里？</h3><table>
<thead>
<tr>
<th>Sink</th>
<th>Output Mode</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>File</td>
<td>Append</td>
<td></td>
</tr>
<tr>
<td>Kafka</td>
<td>Update<br />Complete<br />Append</td>
<td>参考“Kafka集成文档”</td>
</tr>
<tr>
<td>Foreach</td>
<td>Update<br />Complete<br />Append</td>
<td>逐行处理，逻辑自由，输出自由</td>
</tr>
<tr>
<td>Console</td>
<td>Update<br />Complete<br />Append</td>
<td>测试常用</td>
</tr>
<tr>
<td>Memory</td>
<td>Complete<br />Append</td>
<td>非聚合查询如支持complete模式，会造成Result Table爆炸</td>
</tr>
</tbody></table>
<p>org.apache.spark.sql.execution.streaming.Sink</p>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20220404224251.png"></p>
<h2 id="管理-监控"><a href="#管理-监控" class="headerlink" title="管理&amp;监控"></a>管理&amp;监控</h2><p>对Query进行监控和管理</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> query = df.writeStream.format(<span class="string">&quot;console&quot;</span>).start()   <span class="comment">// get the query object</span></span><br><span class="line"></span><br><span class="line">query.id          <span class="comment">// get the unique identifier of the running query that persists across restarts from checkpoint data</span></span><br><span class="line"></span><br><span class="line">query.runId       <span class="comment">// get the unique id of this run of the query, which will be generated at every start/restart</span></span><br><span class="line"></span><br><span class="line">query.name        <span class="comment">// get the name of the auto-generated or user-specified name</span></span><br><span class="line"></span><br><span class="line">query.explain()   <span class="comment">// print detailed explanations of the query</span></span><br><span class="line"></span><br><span class="line">query.stop()      <span class="comment">// stop the query</span></span><br><span class="line"></span><br><span class="line">query.awaitTermination()   <span class="comment">// block until query is terminated, with stop() or with error</span></span><br><span class="line"></span><br><span class="line">query.exception       <span class="comment">// the exception if the query has been terminated with error</span></span><br><span class="line"></span><br><span class="line">query.recentProgress  <span class="comment">// an array of the most recent progress updates for this query</span></span><br><span class="line"></span><br><span class="line">query.lastProgress    <span class="comment">// the most recent progress update of this streaming query</span></span><br></pre></td></tr></table></figure>

<h1 id="持续增量计算的原理"><a href="#持续增量计算的原理" class="headerlink" title="持续增量计算的原理"></a>持续增量计算的原理</h1><h2 id="核心流程"><a href="#核心流程" class="headerlink" title="核心流程"></a>核心流程</h2><ol>
<li>获取数据源 offset</li>
<li>获取当前offset，跟数据源 offset 进行对比，得到当前应该消费的 offset，将 offset 写入到 offsetLog 中，进行状态保存，方便后面恢复<br>例如：数据源为7，当前为3，则从3开始处理。如果已经消费完没有数据则不需要处理</li>
<li>根据 offset 和需要处理的数据生成 logicalPlan，整个 DAG 的图</li>
<li>logicalPlan 优化完后生成 optimizedLogicalPlan</li>
<li>执行物理计划，记录执行状态</li>
<li>执行完成输出结果到 sink 端，写入成功会记录到 batchCommitLog 中，告诉 source 进行 commit，把 offset 进行提交</li>
</ol>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20220404224556.png"></p>
<h2 id="StreamExecution"><a href="#StreamExecution" class="headerlink" title="StreamExecution"></a>StreamExecution</h2><p>org.apache.spark.sql.execution.streaming.StreamExecution</p>
<p>成员变量：</p>
<ul>
<li>sources：streaming data 的产生端（比如 kafka 等）</li>
<li>logicalPlan：DataFrame&#x2F;Dataset 的一系列变换（即计算逻辑）</li>
<li>sink：最终结果写出的接收端（比如 file system 等）</li>
<li>currentBatchId：当前执行的 id</li>
<li>batchCommitLog：已经成功处理的批次有哪些</li>
<li>offsetLog，availableOffsets，committedOffsets：当前执行需要处理的 source data 的 meta 信息</li>
<li>offsetSeqMetadata：当前执行的 watermark 信息（event time 相关）等</li>
</ul>
<h2 id="状态存储-StateStore"><a href="#状态存储-StateStore" class="headerlink" title="状态存储 StateStore"></a>状态存储 StateStore</h2><p>State通过Key-Value结构存放数据，Key&#x3D;operator+partition+version(批次号)</p>
<ul>
<li>分布式实现</li>
<li>状态分片</li>
<li>状态分版本</li>
<li>批量读入和写出分片</li>
</ul>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20220404224653.png"></p>
<p>org.apache.spark.sql.execution.streaming.state.StateStore（未完成）</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/* CRUD 增删改查 */</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 查询一条 key-value</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get</span></span>(key: <span class="type">UnsafeRow</span>): <span class="type">UnsafeRow</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 新增、或修改一条 key-value</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">put</span></span>(key: <span class="type">UnsafeRow</span>, value: <span class="type">UnsafeRow</span>): <span class="type">Unit</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 删除一条符合条件的 key-value</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">remove</span></span>(condition: <span class="type">UnsafeRow</span> =&gt; <span class="type">Boolean</span>): <span class="type">Unit</span></span><br><span class="line"><span class="comment">// 根据 key 删除 key-value</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">remove</span></span>(key: <span class="type">UnsafeRow</span>): <span class="type">Unit</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/* 批量操作相关 */</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 提交当前执行批次的所有修改，将刷出到 HDFS，成功后版本将自增</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">commit</span></span>(): <span class="type">Long</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 放弃当前执行批次的所有修改</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">abort</span></span>(): <span class="type">Unit</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 当前状态分片、当前版本的所有 key-value 状态</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">iterator</span></span>(): <span class="type">Iterator</span>[<span class="type">UnsafeRowPair</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment">// 当前状态分片、当前版本比上一个版本的所有增量更新</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">updates</span></span>(): <span class="type">Iterator</span>[<span class="type">StoreUpdate</span>]</span><br></pre></td></tr></table></figure>



<h2 id="如何保证端到端的Exactly-Once语义？"><a href="#如何保证端到端的Exactly-Once语义？" class="headerlink" title="如何保证端到端的Exactly-Once语义？"></a>如何保证端到端的Exactly-Once语义？</h2><p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20220404224822.png"></p>
<h1 id="动手操作"><a href="#动手操作" class="headerlink" title="动手操作"></a>动手操作</h1><ol>
<li>比较Spark Streaming 和 Structured Streaming的异同点 </li>
<li>将WordCount的结果输出到MySQL或HBase </li>
<li>利用StateStore完成Top N 和去重运算</li>
</ol>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2023/09/10/13-%E5%A4%A7%E6%95%B0%E6%8D%AE/03-Spark/04-StructuredStreaming/" data-id="clmcxec7s0016u8wa4j9tdciw" data-title="" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-13-大数据/03-Spark/03-SparkShuffle&amp;SQL&amp;MLlib" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/09/10/13-%E5%A4%A7%E6%95%B0%E6%8D%AE/03-Spark/03-SparkShuffle&SQL&MLlib/" class="article-date">
  <time class="dt-published" datetime="2023-09-10T03:59:07.646Z" itemprop="datePublished">2023-09-10</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="Spark-Shuffle-解析"><a href="#Spark-Shuffle-解析" class="headerlink" title="Spark Shuffle 解析"></a>Spark Shuffle 解析</h1><h2 id="宽依赖和窄依赖"><a href="#宽依赖和窄依赖" class="headerlink" title="宽依赖和窄依赖"></a>宽依赖和窄依赖</h2><ul>
<li>窄依赖<ul>
<li>没有数据shuffling</li>
<li>所有父RDD中的Partition均会和子RDD的Partition关系是一对一</li>
</ul>
</li>
</ul>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20220220221711.png"></p>
<ul>
<li>宽依赖<ul>
<li>有数据shuffling</li>
<li>所有父RDD中的Partition会被切分，根据key的不同划分到子RDD的Partition中</li>
</ul>
</li>
</ul>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20220220221827.png"></p>
<h2 id="回顾-Stage"><a href="#回顾-Stage" class="headerlink" title="回顾 Stage"></a>回顾 Stage</h2><p>什么是Stage</p>
<ul>
<li>一个Job会被拆分为多组Task，每组Task被称为一个Stage</li>
</ul>
<p>划分依据</p>
<ul>
<li>以shuffle操作作为边界，遇到一个宽依赖就分一个stage</li>
</ul>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20220220222050.png"></p>
<h2 id="引起shuffle的算子"><a href="#引起shuffle的算子" class="headerlink" title="引起shuffle的算子"></a>引起shuffle的算子</h2><ul>
<li>repartition</li>
<li>groupByKey</li>
<li>combineByKey</li>
<li>reduceByKey</li>
<li>aggregateByKey</li>
</ul>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20220306195805.png"></p>
<ul>
<li>intersection</li>
<li>join</li>
<li>cogroup</li>
</ul>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20220306195849.png"></p>
<h2 id="MapReduce-Shuffle-回顾"><a href="#MapReduce-Shuffle-回顾" class="headerlink" title="MapReduce Shuffle 回顾"></a>MapReduce Shuffle 回顾</h2><p>map 数据映射，reduce 数据聚合</p>
<ul>
<li>Map端完成之后会暴露一个Http Server共Reduce端获取数据</li>
<li>Reduce启动拷贝线程从各个Map端拷贝结果<ul>
<li>有大量的网络I&#x2F;O开销</li>
</ul>
</li>
<li>一边拷贝一边进行Merge操作（归并排序）</li>
</ul>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20220306200055.png"></p>
<h2 id="Spark-Shuffle-两阶段"><a href="#Spark-Shuffle-两阶段" class="headerlink" title="Spark Shuffle 两阶段"></a>Spark Shuffle 两阶段</h2><p>为什么 spark 不把 shuffle 结果直接写到内存而是写到磁盘？</p>
<ol>
<li>一般大数据场景数据都是TB、PB级别，全部写到内存可能会内存溢出，内存放不下</li>
<li>即便内存放下，由于程序、网路等原因导致节点宕机，整个 shuffle 要重新计算，成本较高</li>
</ol>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20220306200203.png"></p>
<h2 id="Shuffle-是个昂贵的操作"><a href="#Shuffle-是个昂贵的操作" class="headerlink" title="Shuffle 是个昂贵的操作"></a>Shuffle 是个昂贵的操作</h2><ul>
<li><p>数据分区</p>
<p>一个RDD到另一个RDD转换就需要进行数据分区，基本上90%的分区都是HashPartition，剩下为RangePartition，sortByKey底层就是RangePartition</p>
</li>
<li><p>序列化反序列化</p>
<p>数据落到磁盘上就会涉及到序列化和反序列化，序列化反序列化会非常消耗cpu，优化可以从序列化反序列化去考虑</p>
</li>
<li><p>数据压缩</p>
<p>分布式计算情况下，节点与节点之间交互需要通过网络传输，网络带宽资源是有限的，需要对数据进行一定程度压缩，并不是所有数据都需要压缩，因为压缩数据也是需要耗费cpu的，当传输成本大于cpu成本可以考虑开启压缩</p>
</li>
<li><p>磁盘IO</p>
<p>大数据场景写入数据是有序的可以减少磁盘寻道的时间，这样吞吐量也高</p>
</li>
</ul>
<h2 id="Shuffle-实现进化历史"><a href="#Shuffle-实现进化历史" class="headerlink" title="Shuffle 实现进化历史"></a>Shuffle 实现进化历史</h2><p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20220306200304.png"></p>
<h2 id="Shuffle-相关组件"><a href="#Shuffle-相关组件" class="headerlink" title="Shuffle 相关组件"></a>Shuffle 相关组件</h2><p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20220306200419.png"></p>
<h2 id="Hash-Shuffle"><a href="#Hash-Shuffle" class="headerlink" title="Hash Shuffle"></a>Hash Shuffle</h2><p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20220306200503.png"></p>
<h2 id="Hash-Shuffle-存在的问题？"><a href="#Hash-Shuffle-存在的问题？" class="headerlink" title="Hash Shuffle 存在的问题？"></a>Hash Shuffle 存在的问题？</h2><p><strong>打开文件过多</strong></p>
<ul>
<li>假设N(Shuffle Map Task) &#x3D; 1000，N(Following Task) &#x3D; 500，则会生成50万个小文件。</li>
<li>每打开一个文件需要占用一定量内存空间，假设一个句柄占据100K，则50万个小文件需要占据50GB。</li>
</ul>
<p><strong>大量随机IO</strong></p>
<ul>
<li>Shuffle Map Task随机写。</li>
<li>Following Task随机读。</li>
<li>机械磁盘的随机性能远不如顺序写入性能。</li>
</ul>
<p><strong>频繁GC</strong></p>
<ul>
<li>打开关闭文件</li>
<li>大量的序列化反序列化操作</li>
</ul>
<h2 id="Sort-Shuffle"><a href="#Sort-Shuffle" class="headerlink" title="Sort Shuffle"></a>Sort Shuffle</h2><p>先在内存中进行分区，快写满时进行一个排序写入到磁盘，当这个行为发送n次，说明在磁盘生成n个文件。把n个文件合并成一个大的文件，merge sort可以快速进行排序，因为磁盘上的文件已经是有序的，还会生成一个index，下游在读取的时候会从这里读取数据偏移量再从文件中偏移量位置读取数据。</p>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20220306200734.png"></p>
<h2 id="Sort-Shuffle-存在的问题？"><a href="#Sort-Shuffle-存在的问题？" class="headerlink" title="Sort Shuffle 存在的问题？"></a>Sort Shuffle 存在的问题？</h2><p><strong>需要进行排序</strong></p>
<ul>
<li>Spill写入文件的时候需要按照partitionId对Key进行排序</li>
<li>在merge阶段需要对进行归并排序合并成一个大文件</li>
</ul>
<p><strong>大量序列化反序列化</strong></p>
<ul>
<li>Spill写入文件时</li>
<li>合并文件进行归并排序时</li>
</ul>
<h2 id="Sort-Shuffle-改进-Tungsten-计划"><a href="#Sort-Shuffle-改进-Tungsten-计划" class="headerlink" title="Sort Shuffle 改进 Tungsten 计划"></a>Sort Shuffle 改进 Tungsten 计划</h2><h3 id="改进措施1：更高级的内存管理"><a href="#改进措施1：更高级的内存管理" class="headerlink" title="改进措施1：更高级的内存管理"></a>改进措施1：更高级的内存管理</h3><ul>
<li><p>使用sun.misc.Unsafe直接管理内存，消除GC</p>
</li>
<li><p>例如一个简单字符串abcd，实际占用24个字节</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">java.lang.String object internals:</span><br><span class="line">OFFSET SIZE TYPE DESCRIPTION VALUE</span><br><span class="line">0 4 (object header) ...</span><br><span class="line">4 4 (object header) ...</span><br><span class="line">8 4 (object header) ...</span><br><span class="line">12 4 char[] String.value []</span><br><span class="line">16 4 int String.hash 0</span><br><span class="line">20 4 int String.hash32 0</span><br><span class="line">Instance size: 24 bytes (reported by Instrumentation API)</span><br></pre></td></tr></table></figure></li>
</ul>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20220306200953.png"></p>
<h3 id="改进措施2：缓存感知计算"><a href="#改进措施2：缓存感知计算" class="headerlink" title="改进措施2：缓存感知计算"></a>改进措施2：缓存感知计算</h3><p>核心：尽可能把数据放到靠近cpu的缓存中</p>
<p>下图有一个ptr的数组，里面存放的是指针，指针指向key value存储的地方，我们排序的时候移动的是每一个指针的位置，ptr2大于ptr3，我们交互的是这俩个指针的，cpu在做计算的时候是先访问数组，再访问对应key value，往往我们可以把数组放到L1、L2、L3中，但是key value信息比较大，往往是在Memory中，所以cpu会在L1、L2、L3和Memory中来回奔波。我们可以把key和指针存放到一起，指针指向的是value，交换的时候直接在数组中进行交换，因为key类型都是固定的，例如long、int，量可控，把key和指针放到一起就更有可能把整个排序数组放到cpu的缓存中去。tungsten 实现后有3倍的性能提升</p>
<ul>
<li>提高缓存命中率</li>
<li>速度 L1&gt;L2&gt;L3&gt;内存&gt;磁盘</li>
</ul>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20220306201108.png"></p>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20220306201150.png"></p>
<h2 id="But-使用Tungsten限制条件"><a href="#But-使用Tungsten限制条件" class="headerlink" title="But 使用Tungsten限制条件"></a>But 使用Tungsten限制条件</h2><ol>
<li>在shuffle阶段不能有aggregate：例如reduceByKey</li>
<li>分区数不能超过2<sup>24</sup> − 1</li>
</ol>
<h2 id="Shuffle调优"><a href="#Shuffle调优" class="headerlink" title="Shuffle调优"></a>Shuffle调优</h2><ol>
<li><p>调整spill频率</p>
<ul>
<li>spark.shuffle.file.buffer<ul>
<li>map task的内存缓冲调节参数，默认是32kb</li>
</ul>
</li>
<li>spark.shuffle.memoryFraction<ul>
<li>reduce端聚合内存占比，默认0.2</li>
</ul>
</li>
<li>如何调整？<ul>
<li>通过监控平台观察shuffle write和shuffle read的运行次数</li>
</ul>
</li>
</ul>
</li>
<li><p>尽量减少Shuffle次数</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 两次shuffle</span></span><br><span class="line">rdd.map(...).repartition(<span class="number">1000</span>).reduceByKey(_ + _, <span class="number">3000</span>)</span><br><span class="line"><span class="comment">// 一次shuffle</span></span><br><span class="line">rdd.map(...).reduceByKey(_ + _, <span class="number">3000</span>)</span><br></pre></td></tr></table></figure></li>
</ol>
<h1 id="Spark-SQL-原理"><a href="#Spark-SQL-原理" class="headerlink" title="Spark SQL 原理"></a>Spark SQL 原理</h1><h2 id="Spark-SQL"><a href="#Spark-SQL" class="headerlink" title="Spark SQL"></a>Spark SQL</h2><p>Spark一个模块，用于处理结构化数据</p>
<ul>
<li>无缝集成SQL查询</li>
<li>统一数据访问方法</li>
<li>Hive高度集成</li>
<li>支持JDBC和ODBC</li>
<li>多语言Scala Python</li>
</ul>
<h2 id="Spark-SQL-Example"><a href="#Spark-SQL-Example" class="headerlink" title="Spark SQL Example"></a>Spark SQL Example</h2><p>在Spark程序中无缝集成SQL查询</p>
<ol>
<li>df.registerTempTable(“course_score”)</li>
<li>sqlContext.sql(“select name, avg(score) from course_score group by name”).show()</li>
</ol>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20220306201729.png"></p>
<h2 id="Spark-SQL-原理-1"><a href="#Spark-SQL-原理-1" class="headerlink" title="Spark SQL 原理"></a>Spark SQL 原理</h2><p><strong>score表</strong></p>
<table>
<thead>
<tr>
<th>字段名称</th>
<th>字段类型</th>
</tr>
</thead>
<tbody><tr>
<td>id</td>
<td>LONG</td>
</tr>
<tr>
<td>match_score</td>
<td>INT</td>
</tr>
<tr>
<td>english_score</td>
<td>INT</td>
</tr>
</tbody></table>
<p><strong>people表</strong></p>
<table>
<thead>
<tr>
<th>字段名称</th>
<th>字段类型</th>
</tr>
</thead>
<tbody><tr>
<td>id</td>
<td>LONG</td>
</tr>
<tr>
<td>age</td>
<td>INT</td>
</tr>
<tr>
<td>name</td>
<td>STRING</td>
</tr>
</tbody></table>
<p>求大于18岁的同学总成绩平均分</p>
<ol>
<li>首先由SqlParser将sql解析为一棵树，这棵树称为没有分析的执行计划</li>
<li>Analyzer分析器分析没有分析的执行计划得到逻辑执行树，包含执行的关键信息，可以拿这棵树进行执行了，但是还需要再进行优化。</li>
<li>Optimizer进行sql优化，对于分析后的这棵树进行等价的交互，保证执行结果的正确性</li>
<li>Planner将优化后的逻辑执行树生成一个物理的执行计划</li>
<li>Prepare for Execution将Spark Plan转换为一个可以执行的计划</li>
<li>最终将给execute转换为RDD进行计算</li>
</ol>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20220306201938.png"></p>
<h2 id="SQL-Parser"><a href="#SQL-Parser" class="headerlink" title="SQL Parser"></a>SQL Parser</h2><p>SqlBase.g4文件：<a target="_blank" rel="noopener" href="https://github.com/apache/spark/blob/v2.4.0/sql/catalyst/src/main/antlr4/org/apache/spark/sql/catalyst/parser/SqlBase.g4">https://github.com/apache/spark/blob/v2.4.0/sql/catalyst/src/main/antlr4/org/apache/spark/sql/catalyst/parser/SqlBase.g4</a></p>
<p>idea安装antlr插件，输入SELECT * FROM SCORE，注意要全大写，然后点击文件49行的singleStatement右键Test Rule singleStatement就可以得到抽象语法树（AST）</p>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20220306202002.png"></p>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20220306202048.png"></p>
<h2 id="SQL-Analyzer"><a href="#SQL-Analyzer" class="headerlink" title="SQL Analyzer"></a>SQL Analyzer</h2><ol>
<li>经过Parser之后的LogicalPlan并不知道如何执行</li>
<li>需要绑定不确定的属性和关系<ul>
<li>解析表名<ul>
<li>临时表</li>
<li>临时view</li>
<li>hive table</li>
<li>hive view</li>
</ul>
</li>
<li>解析具体的schema结构<ul>
<li>字段类型</li>
<li>存储位置</li>
</ul>
</li>
</ul>
</li>
</ol>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20220306202628.png"></p>
<h2 id="SQL-Optimizer"><a href="#SQL-Optimizer" class="headerlink" title="SQL Optimizer"></a>SQL Optimizer</h2><ol>
<li><p>不同用户提交SQL质量不同</p>
</li>
<li><p>保证SQL需要被高效执行</p>
</li>
<li><p>Spark SQL主要采用规则优化方法</p>
<ol>
<li><p>谓词下推</p>
<p>减少参与计算的数据量</p>
</li>
<li><p>列值裁剪</p>
<p>减少Filter与Project操作的中间结果集数据量</p>
</li>
<li><p>常量累加</p>
<p>减少重复计算</p>
</li>
<li><p>Join重排序</p>
<p>尽可能地将带有条件过滤的子执行计划下推到执行树的最底层</p>
</li>
</ol>
</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">// Operator push down</span><br><span class="line">PushProjectionThroughUnion,</span><br><span class="line">ReorderJoin,</span><br><span class="line">EliminateOuterJoin,</span><br><span class="line">PushPredicateThroughJoin,</span><br><span class="line">PushDownPredicate,</span><br><span class="line">LimitPushDown(conf),</span><br><span class="line">ColumnPruning,</span><br><span class="line">InferFiltersFromConstraints,</span><br><span class="line"></span><br><span class="line">// Operator combine</span><br><span class="line">CollapseRepartition,</span><br><span class="line">CollapseProject,</span><br><span class="line">CollapseWindow,</span><br><span class="line">CombineFilters,</span><br><span class="line">CombineLimits,</span><br><span class="line">CombineUnions,</span><br><span class="line"></span><br><span class="line">// Constant folding and strength reduction</span><br><span class="line">NullPropagation(conf),</span><br><span class="line">FoldablePropagation,</span><br><span class="line">OptimizeIn(conf),</span><br><span class="line">ConstantFolding,</span><br><span class="line">ReorderAssociativeOperator,</span><br><span class="line">LikeSimplification,</span><br><span class="line">BooleanSimplification,</span><br><span class="line">SimplifyConditionals,</span><br><span class="line">RemoveDispensableExpressions,</span><br><span class="line">SimplifyBinaryComparison,</span><br><span class="line">PruneFilters,</span><br><span class="line">EliminateSorts,</span><br><span class="line">SimplifyCasts,</span><br><span class="line">SimplifyCaseConversionExpressions,</span><br><span class="line">RewriteCorrelatedScalarSubquery,</span><br><span class="line">EliminateSerialization,</span><br><span class="line">RemoveRedundantAliases,</span><br><span class="line">RemoveRedundantProject,</span><br><span class="line">SimplifyCreateStructOps,</span><br><span class="line">SimplifyCreateArrayOps,</span><br><span class="line">SimplifyCreateMapOps</span><br></pre></td></tr></table></figure>

<h3 id="SQL-Optimizer-谓词下推"><a href="#SQL-Optimizer-谓词下推" class="headerlink" title="SQL Optimizer 谓词下推"></a>SQL Optimizer 谓词下推</h3><p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20220306202933.png"></p>
<h3 id="SQL-Optimizer-列值裁剪"><a href="#SQL-Optimizer-列值裁剪" class="headerlink" title="SQL Optimizer 列值裁剪"></a>SQL Optimizer 列值裁剪</h3><p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20220306203028.png"></p>
<h2 id="SQL-Planner"><a href="#SQL-Planner" class="headerlink" title="SQL Planner"></a>SQL Planner</h2><ol>
<li><p>与RDD进行绑定</p>
</li>
<li><p>选取执行策略</p>
<p>FileSourceStrategy</p>
<p>DataSourceStrategy</p>
<p>SpecialLimits</p>
<p>Aggregation</p>
<p>JoinSelection</p>
<p>InMemoryScans</p>
<p>BasicOperators</p>
</li>
</ol>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20220306203120.png"></p>
<h2 id="RDD-DataFrame"><a href="#RDD-DataFrame" class="headerlink" title="RDD DataFrame"></a>RDD DataFrame</h2><p>Spark本身并不清楚RDD中Person内部的结构</p>
<ul>
<li>执行时无法优化</li>
</ul>
<p>DataFrame提供了详细的结构信息</p>
<ul>
<li>分布式Row对象集合</li>
<li>算子更加丰富</li>
<li>执行计划优化</li>
</ul>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20220404112704.png"></p>
<h2 id="Spark-SQL-实战"><a href="#Spark-SQL-实战" class="headerlink" title="Spark SQL 实战"></a>Spark SQL 实战</h2><p>创建DataFrame</p>
<ul>
<li>createDataFrame方法</li>
<li>RDD调用toDF方法 </li>
<li>直接读取JSON </li>
<li>从Hive表中读取</li>
</ul>
<p>执行业务逻辑</p>
<ul>
<li>sqlContext、sparkSession执行</li>
<li>DataFrame API</li>
</ul>
<h3 id="Spark-SQL-实战-1-6"><a href="#Spark-SQL-实战-1-6" class="headerlink" title="Spark SQL 实战 1.6"></a>Spark SQL 实战 1.6</h3><ol>
<li><p>初始化SparkContext</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">conf.setAppName(<span class="string">&quot;DataFrameSQL&quot;</span>).setMaster(<span class="string">&quot;local[4]&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br></pre></td></tr></table></figure>
</li>
<li><p>初始化SQLContext</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> sqlContext = <span class="keyword">new</span> <span class="type">SQLContext</span>(sc)</span><br></pre></td></tr></table></figure>
</li>
<li><p>生成DataFrame</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> sqlContext.implicits._</span><br><span class="line"><span class="keyword">val</span> df = <span class="type">Seq</span>( (<span class="number">1</span>, <span class="string">&quot;Bill&quot;</span>, <span class="string">&quot;Computer Science&quot;</span>, <span class="number">100</span>),</span><br><span class="line">(<span class="number">2</span>, <span class="string">&quot;Bill&quot;</span>, <span class="string">&quot;Math&quot;</span>, <span class="number">85</span>),</span><br><span class="line">(<span class="number">3</span>, <span class="string">&quot;Bill&quot;</span>, <span class="string">&quot;English&quot;</span>, <span class="number">90</span>),</span><br><span class="line">(<span class="number">4</span>, <span class="string">&quot;Peter&quot;</span>, <span class="string">&quot;Math&quot;</span>, <span class="number">82</span>),</span><br><span class="line">(<span class="number">5</span>, <span class="string">&quot;Peter&quot;</span>, <span class="string">&quot;Music&quot;</span>, <span class="number">90</span>),</span><br><span class="line">(<span class="number">6</span>, <span class="string">&quot;Peter&quot;</span>, <span class="string">&quot;Chemistry&quot;</span>, <span class="number">85</span>)</span><br><span class="line">).toDF(<span class="string">&quot;id&quot;</span>, <span class="string">&quot;name&quot;</span>, <span class="string">&quot;course&quot;</span>, <span class="string">&quot;score&quot;</span>)</span><br></pre></td></tr></table></figure>
</li>
<li><p>使用算子执行业务逻辑</p>
<ul>
<li><p>使用标准SQL执行业务逻辑</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">df.registerTempTable(<span class="string">&quot;course_score&quot;</span>)</span><br><span class="line">sqlContext.sql(<span class="string">&quot;select name, avg(score) from course_score group by name&quot;</span>).show()</span><br></pre></td></tr></table></figure>
</li>
<li><p>使用DataFrame API执行业务逻辑</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">df.groupBy(<span class="string">&quot;name&quot;</span>)</span><br><span class="line">.avg(<span class="string">&quot;score&quot;</span>)</span><br><span class="line">.show()</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ol>
<h3 id="Spark-SQL-实战-2-0"><a href="#Spark-SQL-实战-2-0" class="headerlink" title="Spark SQL 实战 2.0"></a>Spark SQL 实战 2.0</h3><p>sqlContext、HiveContext不能使用，改为sparkSession，统一API、Hive、DataFrame、DataSet</p>
<ol>
<li><p>初始化SparkSession</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> spark = <span class="type">SparkSession</span></span><br><span class="line">.builder()</span><br><span class="line">.appName(<span class="string">&quot;Spark SQL Example&quot;</span>)</span><br><span class="line">.master(<span class="string">&quot;local[4]&quot;</span>)</span><br><span class="line">.config(<span class="string">&quot;spark.some.config.option&quot;</span>, <span class="string">&quot;some-value&quot;</span>)</span><br><span class="line">.getOrCreate()</span><br></pre></td></tr></table></figure>
</li>
<li><p>加载数据</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> dataSet = spark.range(<span class="number">5</span>, <span class="number">100</span>, <span class="number">5</span>)</span><br><span class="line"><span class="keyword">val</span> dataFrame = spark.read.json(<span class="string">&quot;examples/src/main/resources/people.json&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> resultsDF = spark.sql(<span class="string">&quot;SELECT city, pop, state, zip FROM zips_table&quot;</span>)</span><br></pre></td></tr></table></figure>
</li>
<li><p>Enjoy Computing</p>
</li>
</ol>
<h2 id="Spark-SQL-数据倾斜"><a href="#Spark-SQL-数据倾斜" class="headerlink" title="Spark SQL 数据倾斜"></a>Spark SQL 数据倾斜</h2><p>什么是MapJoin？<br>    join尽量不发生在reduce阶段，发生在map阶段。拿成绩数据来讲，每个人的id、name、age数据量比较少，是一个字典表，完全可以把这个数据广播到每一个节点，生成一个hashTable，直接通过查id得到name和age，这样可以在map阶段把name和age关联出来</p>
<ol>
<li><p>Spark 1.6 使用MapJoin</p>
<p><strong>CACHE</strong> TABLE xx AS select * from xx</p>
</li>
<li><p>Spark 2.2 使用MapJoin Hint</p>
<p>SELECT **&#x2F;<em>+ MAPJOIN(b) <em>&#x2F;</em></em> …</p>
</li>
<li><p>调整Spark SQL相关参数</p>
<ul>
<li><p>spark.sql.autoBroadcastJoinThreshold</p>
<p>被广播表的大小小于该值时启动BroadcastJoin，默认10MB。 </p>
<p>如果需要广播，服务器内存比较大可以调整该参数</p>
</li>
<li><p>spark.sql.broadcastTimeout</p>
<p>广播等待超时时间(秒)，默认300s。 </p>
</li>
<li><p>spark.sql.shuffle.partitions</p>
<p>join或者aggregation是使用多少分区，默认200。</p>
<p>数据量比较大可以调高该参数</p>
</li>
</ul>
<p>解决数据倾斜相关链接</p>
<ul>
<li>Spark性能优化指南——高级篇 - 美团技术团队 <a target="_blank" rel="noopener" href="https://tech.meituan.com/2016/05/12/spark-tuning-pro.html">https://tech.meituan.com/2016/05/12/spark-tuning-pro.html</a></li>
<li>Spark性能优化指南——基础篇 - 美团技术团队 <a target="_blank" rel="noopener" href="https://tech.meituan.com/2016/04/29/spark-tuning-basic.html">https://tech.meituan.com/2016/04/29/spark-tuning-basic.html</a></li>
</ul>
</li>
</ol>
<h1 id="Spark-MLlib-实战"><a href="#Spark-MLlib-实战" class="headerlink" title="Spark MLlib 实战"></a>Spark MLlib 实战</h1><p>MLlib是Spark机器学习库，包含了许多通用的机器学习算法和工具。</p>
<h2 id="什么是机器学习？"><a href="#什么是机器学习？" class="headerlink" title="什么是机器学习？"></a>什么是机器学习？</h2><p>让机器从数据中学习，进而得到一个更加符合现实规律的模型，通过对模型的使用使得机器比以往表现的更好，这就是机器学习。</p>
<h2 id="MLlib-究竟能干啥？"><a href="#MLlib-究竟能干啥？" class="headerlink" title="MLlib 究竟能干啥？"></a>MLlib 究竟能干啥？</h2><p>算法</p>
<ul>
<li>回归 Regression<br>预测未来的增长</li>
<li>分类 Classification</li>
<li>聚类 Clustering<br>对某一类人群进行投放广告，提高转化率</li>
<li>决策树 Decision trees</li>
<li>协同过滤 Collaborative filtering<br>推荐领域，A和B是相似的人，A喜欢看小说，B也有很大可能喜欢看小说</li>
</ul>
<p>特征工程</p>
<ul>
<li><p>特征抽取 Feature extraction</p>
</li>
<li><p>降维 Dimensionality reduction</p>
</li>
</ul>
<p>持久化</p>
<p>常用工具</p>
<ul>
<li><p>线性代数工具集</p>
</li>
<li><p>统计工具集</p>
</li>
</ul>
<p><strong>简单的决策树</strong></p>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20220306204539.png"></p>
<h2 id="逻辑回归预测乳腺癌"><a href="#逻辑回归预测乳腺癌" class="headerlink" title="逻辑回归预测乳腺癌"></a>逻辑回归预测乳腺癌</h2><table>
<thead>
<tr>
<th>特征X</th>
<th>解释</th>
</tr>
</thead>
<tbody><tr>
<td>Clump Thickness</td>
<td>肿块厚度</td>
</tr>
<tr>
<td>Uniformity of Cell Size</td>
<td>细胞大小均匀性</td>
</tr>
<tr>
<td>Uniformity of Cell Shape</td>
<td>细胞形状均匀性</td>
</tr>
<tr>
<td>Marginal Adhesion</td>
<td>边缘附着力</td>
</tr>
<tr>
<td>Single Epithelial Cell Size</td>
<td>单上皮细胞大小</td>
</tr>
<tr>
<td>Bare Nuclei</td>
<td>裸核</td>
</tr>
<tr>
<td>Bland Chromatin</td>
<td>染色质</td>
</tr>
<tr>
<td>Normal Nucleoli</td>
<td>正常核仁</td>
</tr>
<tr>
<td>Mitoses</td>
<td>有丝分裂</td>
</tr>
</tbody></table>
<p>预测Y：良性 or 恶性</p>
<h2 id="逻辑回归模型"><a href="#逻辑回归模型" class="headerlink" title="逻辑回归模型"></a>逻辑回归模型</h2><p>假设有个函数y &#x3D; h𝜃(𝑥)可以帮助我们预测癌症，且0 ≤ h𝜃(𝑥) ≤ 1</p>
<p>良性y &#x3D; 0</p>
<p>恶性y &#x3D; 1</p>
<h2 id="Sigmoid函数"><a href="#Sigmoid函数" class="headerlink" title="Sigmoid函数"></a>Sigmoid函数</h2><p>x的取值范围是负无穷大到正无穷大，y为该函数算出的值，以0作为分界点，把0代入x，e的负0次方为1，2分之1为0.5，我们可以把sigmoid函数用来去做一个概率的映射，小于0.5认为是良性，大于等于0.5是恶性</p>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20220306204833.png"><br>$$<br>𝑔(x) &#x3D; \dfrac{1} {1+𝑒^{-𝑥’}}，g(x)为sigmoid函数<br>$$</p>
<h2 id="LR分类Example"><a href="#LR分类Example" class="headerlink" title="LR分类Example"></a>LR分类Example</h2><p>逻辑回归，先在图上画出数据，假设我们只有2个维度x1、x2。横轴为x1，纵轴为x2。类别按不同的颜色来表示，蓝色和橘色。</p>
<p>去训练出一个平面，用一根线把俩类数据分开，这个叫线性可分。在线下和线上可以分为两类数据。当我们来了一个新的值x1、x2，我们只需要把他带入到这个方程里面，来看数据是在线的下面还是上面</p>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20220306205421.png"></p>
<h2 id="Spark-MLlib-训练模型流程"><a href="#Spark-MLlib-训练模型流程" class="headerlink" title="Spark MLlib 训练模型流程"></a>Spark MLlib 训练模型流程</h2><p>模型训练流程</p>
<ul>
<li>加载数据</li>
<li>数据抽取</li>
<li>训练出模型</li>
<li>模型评估</li>
</ul>
<p>模型测试流程</p>
<ul>
<li><p>加载数据，使用模型从未接触过的数据测试</p>
</li>
<li><p>数据抽取</p>
</li>
<li><p>预测结果</p>
</li>
<li><p>模型评估</p>
</li>
</ul>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20220306205450.png"></p>
<h2 id="乳腺癌数据源"><a href="#乳腺癌数据源" class="headerlink" title="乳腺癌数据源"></a>乳腺癌数据源</h2><p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20220306205524.png"></p>
<h3 id="Step-1-清洗数据"><a href="#Step-1-清洗数据" class="headerlink" title="Step 1 清洗数据"></a>Step 1 清洗数据</h3><p>String &#x3D;&gt;Object</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parseRDD</span></span>(rdd: <span class="type">RDD</span>[<span class="type">String</span>]): <span class="type">RDD</span>[<span class="type">Array</span>[<span class="type">Double</span>]] = &#123;</span><br><span class="line">rdd.map(_.split(<span class="string">&quot;,&quot;</span>)).filter(_(<span class="number">6</span>) != </span><br><span class="line"><span class="string">&quot;?&quot;</span>).map(_.drop(<span class="number">1</span>)).map(_.map(_.toDouble))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20220306205628.png"></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parseObs</span></span>(line: <span class="type">Array</span>[<span class="type">Double</span>]): <span class="type">Obs</span> = &#123;</span><br><span class="line"><span class="type">Obs</span>(</span><br><span class="line"><span class="keyword">if</span> (line(<span class="number">9</span>) == <span class="number">4.0</span>) <span class="number">1</span> <span class="keyword">else</span> <span class="number">0</span>, line(<span class="number">0</span>), line(<span class="number">1</span>), line(<span class="number">2</span>), line(<span class="number">3</span>), </span><br><span class="line">line(<span class="number">4</span>), line(<span class="number">5</span>), line(<span class="number">6</span>), line(<span class="number">7</span>), line(<span class="number">8</span>)</span><br><span class="line">) &#125;</span><br></pre></td></tr></table></figure>

<h3 id="Step-2-分割训练集和测试集"><a href="#Step-2-分割训练集和测试集" class="headerlink" title="Step 2 分割训练集和测试集"></a>Step 2 分割训练集和测试集</h3><p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20220306205524.png"></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> splitSeed = <span class="number">5043</span></span><br><span class="line"><span class="keyword">val</span> <span class="type">Array</span>(trainingData, testData) = </span><br><span class="line">df3.randomSplit(<span class="type">Array</span>(<span class="number">0.7</span>, <span class="number">0.3</span>), </span><br><span class="line">splitSeed)</span><br></pre></td></tr></table></figure>

<h3 id="Step-3-训练模型"><a href="#Step-3-训练模型" class="headerlink" title="Step 3 训练模型"></a>Step 3 训练模型</h3><p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20220306205728.png"></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> lr = <span class="keyword">new</span> <span class="type">LogisticRegression</span>()</span><br><span class="line"><span class="comment">// 设置最大迭代次数，防止过拟合</span></span><br><span class="line">.setMaxIter(<span class="number">10</span>)</span><br><span class="line"><span class="comment">// 正则化参数，尝试把一个模型把简单方向训练，防止过拟合</span></span><br><span class="line"><span class="comment">// 过拟合是对测试集拟合的过好，往往数据会有一些波动和异常值，当模型对于数据拟合过好之后，在实际应用中会发现结果会很差，和实际不相符。模型通常来讲越简单，提供的效果越好</span></span><br><span class="line"><span class="comment">// 所以这边会设置一些参数，尝试让x0...xn参数趋近于0，当特征越少，留下的特征都是一些重要的特征</span></span><br><span class="line">.setRegParam(<span class="number">0.3</span>)</span><br><span class="line">.setElasticNetParam(<span class="number">0.8</span>)</span><br><span class="line"><span class="keyword">val</span> model = lr.fit(trainingData)</span><br></pre></td></tr></table></figure>

<h3 id="Step-4-评估模型"><a href="#Step-4-评估模型" class="headerlink" title="Step 4 评估模型"></a>Step 4 评估模型</h3><p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20220306205809.png"></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> predictions = model.transform(testData)</span><br><span class="line">predictions.select(<span class="string">&quot;clas&quot;</span>, <span class="string">&quot;label&quot;</span>, <span class="string">&quot;prediction&quot;</span>)</span><br><span class="line">.show(<span class="number">50</span>)</span><br><span class="line"><span class="keyword">val</span> evaluator = <span class="keyword">new</span> <span class="type">BinaryClassificationEvaluator</span>()</span><br><span class="line">.setLabelCol(<span class="string">&quot;label&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> accuracy = evaluator.evaluate(predictions)</span><br></pre></td></tr></table></figure>




      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2023/09/10/13-%E5%A4%A7%E6%95%B0%E6%8D%AE/03-Spark/03-SparkShuffle&SQL&MLlib/" data-id="clmcxec7o0015u8waf1yqhp7j" data-title="" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-hello-world" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/09/09/hello-world/" class="article-date">
  <time class="dt-published" datetime="2023-09-09T15:48:19.822Z" itemprop="datePublished">2023-09-09</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/09/09/hello-world/">Hello World</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>Welcome to <a target="_blank" rel="noopener" href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a target="_blank" rel="noopener" href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a target="_blank" rel="noopener" href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a target="_blank" rel="noopener" href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>

<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/writing.html">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>

<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/server.html">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>

<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/generating.html">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>

<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2023/09/09/hello-world/" data-id="clmc82d4r0000dswa3948etis" data-title="Hello World" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-13-大数据/02-大数据离线处理与数仓/01-Hive" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2022/01/01/13-%E5%A4%A7%E6%95%B0%E6%8D%AE/02-%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%A6%BB%E7%BA%BF%E5%A4%84%E7%90%86%E4%B8%8E%E6%95%B0%E4%BB%93/01-Hive/" class="article-date">
  <time class="dt-published" datetime="2022-01-01T04:00:00.000Z" itemprop="datePublished">2022-01-01</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/">大数据</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2022/01/01/13-%E5%A4%A7%E6%95%B0%E6%8D%AE/02-%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%A6%BB%E7%BA%BF%E5%A4%84%E7%90%86%E4%B8%8E%E6%95%B0%E4%BB%93/01-Hive/">HIVE</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="什么是Hive？"><a href="#什么是Hive？" class="headerlink" title="什么是Hive？"></a>什么是Hive？</h1><h2 id="MapReduce的局限性"><a href="#MapReduce的局限性" class="headerlink" title="MapReduce的局限性"></a>MapReduce的局限性</h2><ol>
<li>学习曲线陡峭，不适合不懂Java的用户</li>
<li>开发效率不高</li>
<li>发布和更新比较麻烦，不适合业务快速变化的场景</li>
</ol>
<h2 id="开源数据仓库工具"><a href="#开源数据仓库工具" class="headerlink" title="开源数据仓库工具"></a>开源数据仓库工具</h2><p>Facebook开源，贡献给社区，Apache顶级项目</p>
<p><strong>开发目的</strong>是解决海量结构化的日志数据统计问题</p>
<p><strong>声明式编程</strong>：告诉“框架”你想要的是什么**(what)，让框架想出<em>如何</em>去做(how)**。~ SQL</p>
<p>Hive本质上就是一个Sql2MapReduce的框架</p>
<p>2009.04.29 — 第一个官方稳定版0.3.0</p>
<p>2017.07.17 — 最早的2.3版本 （支持Hadoop 2.x，目前主流）</p>
<p>2018.05 — 最早的3.0版本 （支持Hadoop 3.x)</p>
<p>目前经常使用的是CDH6打包后的版本2.1</p>
<h2 id="Hive的优点"><a href="#Hive的优点" class="headerlink" title="Hive的优点"></a>Hive的优点</h2><p><strong>快速</strong></p>
<p>Hive 所采用的数据可通过批处理快速处理 PB 级数据。</p>
<p><strong>熟悉</strong></p>
<p>Hive 提供非程序员可以使用的熟悉的类似于 SQL 的界面。语法类似于MySQL。</p>
<p><strong>可扩展</strong></p>
<p>Hive 可根据您的需求（数据量和效率）被轻松分发与扩展。</p>
<h2 id="Hive在Hadoop生态里所处的角色"><a href="#Hive在Hadoop生态里所处的角色" class="headerlink" title="Hive在Hadoop生态里所处的角色"></a>Hive在Hadoop生态里所处的角色</h2><p>Hadoop生态的组件大致分为4大类：计算、存储、资源调度、集成工具</p>
<p>红色是Hive</p>
<p>蓝色是依赖的组件: Hive依赖于HDFS存储数据，依赖MR处理数据</p>
<p>紫色是有交互的组件</p>
<p>橙色是有竞争的组件</p>
<p>Hive本质是一种计算引擎（组件），它可以和多种存储引擎（组件）交互</p>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211219184814.png"></p>
<h2 id="Hive环境选择"><a href="#Hive环境选择" class="headerlink" title="Hive环境选择"></a>Hive环境选择</h2><p>不同环境选择不同安装方法</p>
<ul>
<li>学习环境<ul>
<li>使用Cloudera的QuickstartVM<sup>[1]</sup>（使用VirtualBox打开），推荐16G内存</li>
<li>使用Docker初始化一个Hive环境<sup>[2]</sup><sup>[3]</sup>，推荐8G内存</li>
</ul>
</li>
<li>生产环境（自建集群）<ul>
<li>使用Cloudera Manager来安装CDH软件包</li>
<li>不建议自己安装开源版Hadoop和Hive</li>
</ul>
</li>
<li>生产环境（云托管集群）<ul>
<li>阿里云E-MapReduce<sup>[4]</sup></li>
<li>AWS EMR<sup>[5]</sup></li>
<li>Azure hdinsight<sup>[6]</sup></li>
<li>腾讯云EMR</li>
<li>华为云</li>
</ul>
</li>
</ul>
<p>[1] 下载链接 <a target="_blank" rel="noopener" href="https://www.cloudera.com/downloads/quickstart_vms/5-13.html">https://www.cloudera.com/downloads/quickstart_vms/5-13.html</a></p>
<p>[2] <a target="_blank" rel="noopener" href="https://github.com/big-data-europe/docker-hive">https://github.com/big-data-europe/docker-hive</a></p>
<p>[3] 如果会用Docker Compose软件，更推荐Docker方式</p>
<p>[4] <a target="_blank" rel="noopener" href="https://help.aliyun.com/document_detail/28129.html">https://help.aliyun.com/document_detail/28129.html</a></p>
<p>[5] <a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/zh_cn/emr/latest/ReleaseGuide/emr-hive.html">https://docs.aws.amazon.com/zh_cn/emr/latest/ReleaseGuide/emr-hive.html</a></p>
<p>[6] <a target="_blank" rel="noopener" href="https://docs.microsoft.com/zh-cn/azure/hdinsight/hadoop/hdinsight-use-hive">https://docs.microsoft.com/zh-cn/azure/hdinsight/hadoop/hdinsight-use-hive</a></p>
<h1 id="Hive可以干什么？"><a href="#Hive可以干什么？" class="headerlink" title="Hive可以干什么？"></a>Hive可以干什么？</h1><h2 id="Hive的主要用途：各种适合离线计算的场景"><a href="#Hive的主要用途：各种适合离线计算的场景" class="headerlink" title="Hive的主要用途：各种适合离线计算的场景"></a>Hive的主要用途：各种适合离线计算的场景</h2><h3 id="适合的场景"><a href="#适合的场景" class="headerlink" title="适合的场景"></a>适合的场景</h3><p><strong>离线ETL</strong></p>
<ul>
<li>支持SQL编程</li>
<li>可随数据量的大小来扩展</li>
<li>丰富的函数和支持自定义函数</li>
</ul>
<p><strong>数据湖架构</strong></p>
<ul>
<li>海量存储</li>
<li>支持结构化和半结构化数据</li>
<li>支持多种文件格式和数据格式</li>
</ul>
<p><strong>数据分析</strong></p>
<ul>
<li>支持多种数据访问接口</li>
<li>支持SQL查询</li>
</ul>
<h3 id="不适合的场景"><a href="#不适合的场景" class="headerlink" title="不适合的场景"></a>不适合的场景</h3><ul>
<li>流式数据处理</li>
<li>数据频繁刷新</li>
<li>低延迟查询</li>
</ul>
<p>Hive的次要用途：<strong>元数据管理</strong></p>
<h2 id="Hive数据生命周期"><a href="#Hive数据生命周期" class="headerlink" title="Hive数据生命周期"></a>Hive数据生命周期</h2><p><strong>数据导入</strong></p>
<ul>
<li>建表</li>
<li>从HDFS导入</li>
<li>从关系型DB导入</li>
</ul>
<p><strong>数据处理</strong></p>
<ul>
<li>数据清洗</li>
<li>ETL</li>
</ul>
<p><strong>数据分析</strong></p>
<ul>
<li>即席查询</li>
<li>报表工具查询</li>
<li>Dashboard工具查询</li>
</ul>
<p><strong>数据导出</strong></p>
<ul>
<li>导出到HDFS</li>
<li>导出到关系型DB</li>
<li>导出到NoSQL DB</li>
</ul>
<h2 id="Hive的两种常用场景"><a href="#Hive的两种常用场景" class="headerlink" title="Hive的两种常用场景"></a>Hive的两种常用场景</h2><p><strong>偏数据分析</strong></p>
<ul>
<li>检索Schema，查看数据</li>
<li>编写SQL语句生成结果</li>
<li>将结果导出到Excel</li>
</ul>
<p><strong>偏数据开发</strong></p>
<ul>
<li>从其他系统导入数据到HDFS</li>
<li>创建表来检索数据</li>
<li>编写ETL来进行数据清洗</li>
<li>数据导出到其他数据系统</li>
</ul>
<h1 id="用Hive做数据分析"><a href="#用Hive做数据分析" class="headerlink" title="用Hive做数据分析"></a>用Hive做数据分析</h1><h2 id="先了解访问Hive的几种方式"><a href="#先了解访问Hive的几种方式" class="headerlink" title="先了解访问Hive的几种方式"></a>先了解访问Hive的几种方式</h2><p>访问Hive一般分为：Web访问，CLI访问，工具访问三种方式</p>
<p><strong>Web访问</strong></p>
<ul>
<li>Hue (Web)</li>
</ul>
<p><strong>工具访问</strong></p>
<ul>
<li>各种支持JDBC或ODBC协议的工具，如<ul>
<li>squirrel-sql</li>
<li>Tableau Desktop</li>
</ul>
</li>
</ul>
<p><strong>CLI访问</strong></p>
<ul>
<li>Beeline CLI</li>
<li>Hive CLI（旧）</li>
</ul>
<p>CLI或工具访问的常见认证方式</p>
<ul>
<li>无认证</li>
<li>LDAP (eg. Active Directory) – 企业里，适合给个人用户做认证</li>
<li>Kerberos -企业里，适合给程序做认证，配置较复杂</li>
</ul>
<h2 id="重点说下Beeline-CLI"><a href="#重点说下Beeline-CLI" class="headerlink" title="重点说下Beeline CLI"></a>重点说下Beeline CLI</h2><ul>
<li><p>本质上是一个JDBC Client</p>
</li>
<li><p>一般工作在远程模式下,可以和Hive Server 2不在同一台机器上</p>
</li>
<li><p>启动方式: </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">beeline –u jdbc:hive2://&lt;hive-server2&gt;:10000/default –n &lt;user&gt; -p &lt;<span class="built_in">pwd</span>&gt;</span><br></pre></td></tr></table></figure>

<ul>
<li>前提是HiveServer2的认证方式是None(默认值)，user可以指定执行用户，不提供的话使用linux的登录用户</li>
</ul>
</li>
<li><p>更多启动参数: </p>
<ul>
<li>-f: 执行SQL文件: beeline … -f xxx.hql</li>
<li>-e: 执行SQL: beeline … -e ‘show tables’</li>
<li>–hiveconf: 设置hiveconf变量: beeline … –hiveconf property&#x3D;value –hiveconf …</li>
<li>–hivevar: 设置hivevar变量(可替换sql文件里的变量): beeline … –hivevar var1&#x3D;value1</li>
</ul>
</li>
<li><p>退出: !quit (或!q)</p>
</li>
</ul>
<h2 id="如何安装Beeline-CLI"><a href="#如何安装Beeline-CLI" class="headerlink" title="如何安装Beeline CLI"></a>如何安装Beeline CLI</h2><ul>
<li><p>先确定Hadoop的版本和Hive的版本，这里以Hadoop2.8 Hive 2.3为例, 事先安装好JDK，确定JAVA_HOME已设置</p>
</li>
<li><p>下载Hadoop包： wget <a target="_blank" rel="noopener" href="http://mirror.bit.edu.cn/apache/hadoop/common/hadoop-2.8.5/hadoop-2.8.5.tar.gz">http://mirror.bit.edu.cn/apache/hadoop/common/hadoop-2.8.5/hadoop-2.8.5.tar.gz</a></p>
</li>
<li><p>解压后进入目录：export HADOOP_HOME&#x3D;$(pwd)</p>
</li>
<li><p>下载Hive包：wget <a target="_blank" rel="noopener" href="http://mirror.bit.edu.cn/apache/hive/hive-2.3.4/apache-hive-2.3.4-bin.tar.gz">http://mirror.bit.edu.cn/apache/hive/hive-2.3.4/apache-hive-2.3.4-bin.tar.gz</a></p>
</li>
<li><p>解压后进入目录: export HIVE_HOME&#x3D;$(pwd)</p>
</li>
<li><p>把hive bin目录下的文件加入PATH： export PATH&#x3D;$HIVE_HOME&#x2F;bin:$PATH</p>
</li>
<li><p>使用下面命令连接到Hive Server</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">beeline -u jdbc:hive2://&lt;hive-server-ip&gt;:10000 -n hive -p hive</span><br><span class="line"></span><br><span class="line">show databases;</span><br><span class="line">use &lt;database&gt;;</span><br><span class="line">show tables;</span><br><span class="line"></span><br><span class="line">desc &lt;database&gt;;</span><br><span class="line">desc &lt;talbe&gt;;</span><br></pre></td></tr></table></figure></li>
</ul>
<h2 id="Beeline-CLI成功登录"><a href="#Beeline-CLI成功登录" class="headerlink" title="Beeline CLI成功登录"></a>Beeline CLI成功登录</h2><p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211219191215.png"></p>
<p>更多可以尝试的命令：!help, show tables</p>
<p>退出：!q</p>
<h2 id="Beeline-CLI常见问题"><a href="#Beeline-CLI常见问题" class="headerlink" title="Beeline CLI常见问题"></a>Beeline CLI常见问题</h2><ul>
<li><p>连不上HiveSer ver</p>
<p>测试端口通不通：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">telnet &lt;hive-ser ver-ip&gt; 10000</span><br></pre></td></tr></table></figure>
</li>
<li><p>Hive Server服务没启动</p>
<ul>
<li><p>如何查日志？</p>
<ul>
<li><p>从CM看</p>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211219191529.png"></p>
</li>
<li><p>从服务器上看</p>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211219191551.png"></p>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="DESC指令"><a href="#DESC指令" class="headerlink" title="DESC指令"></a>DESC指令</h2><p>DESCRIBE DATABASE default;</p>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211219192438.png"></p>
<p>DESCRIBE [EXTENDED|FORMATTED] table_name;</p>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211219192452.png"></p>
<h2 id="SHOW-CREATE-TABLE命令"><a href="#SHOW-CREATE-TABLE命令" class="headerlink" title="SHOW CREATE TABLE命令"></a>SHOW CREATE TABLE命令</h2><p>SHOW CREATE TABLE table_name; – 查看表结构、存储格式和存储路径</p>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211219192536.png"></p>
<h2 id="Hive-QL"><a href="#Hive-QL" class="headerlink" title="Hive QL"></a>Hive QL</h2><ul>
<li>查询<ul>
<li>SELECT</li>
</ul>
</li>
<li>过滤<ul>
<li>WHERE</li>
</ul>
</li>
<li>分组<ul>
<li>GROUP BY</li>
</ul>
</li>
<li>分组过滤<ul>
<li>HAVING</li>
</ul>
</li>
<li>TOP<ul>
<li>LIMIT</li>
</ul>
</li>
<li>排序<ul>
<li>ORDER BY, SORT BY</li>
</ul>
</li>
<li>聚合函数<ul>
<li>SUM(), MIN(), AVG(), MAX()</li>
</ul>
</li>
<li>条件语句<ul>
<li>CASE WHEN，IF</li>
</ul>
</li>
<li>分区函数<ul>
<li>Partition By</li>
</ul>
</li>
</ul>
<h2 id="Hive-JOIN"><a href="#Hive-JOIN" class="headerlink" title="Hive JOIN"></a>Hive JOIN</h2><ul>
<li>Inner join</li>
<li>Left Outer Join</li>
<li>Right Outer Join</li>
<li>Full Outer Join</li>
</ul>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> c.ID, c.NAME, o.AMOUNT, o.DATE</span><br><span class="line"><span class="keyword">FROM</span> CUSTOMERS c</span><br><span class="line"><span class="keyword">FULL</span> [<span class="keyword">OUTER</span>] <span class="keyword">JOIN</span> ORDERS o</span><br><span class="line"><span class="keyword">ON</span> (c.ID <span class="operator">=</span> o.CUSTOMER_ID);</span><br></pre></td></tr></table></figure>

<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211219192839.png"></p>
<h2 id="动手操作：Hive数据分析"><a href="#动手操作：Hive数据分析" class="headerlink" title="动手操作：Hive数据分析"></a>动手操作：Hive数据分析</h2><ul>
<li>统计access log里访问最多的5个IP<ul>
<li>注意观察Hive Job拆分成Map Reduce Job并执行</li>
<li>如何查看Hive MR Job执行的日志(通过JobID到Yarn的Resource Manager里查看）</li>
</ul>
</li>
</ul>
<h2 id="查看Hive-Job的状态"><a href="#查看Hive-Job的状态" class="headerlink" title="查看Hive Job的状态"></a>查看Hive Job的状态</h2><ol>
<li><p>获取Job的地址</p>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211219193024.png"></p>
</li>
<li><p>查看AM的日志</p>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211219193125.png"></p>
</li>
<li><p>查看MR的日志</p>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211219193155.png"></p>
</li>
<li><p>查看Job的计数器（可选）</p>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211219193232.png"></p>
</li>
</ol>
<h2 id="Hive操作符"><a href="#Hive操作符" class="headerlink" title="Hive操作符"></a>Hive操作符</h2><p>内置操作符（Operators）</p>
<p>&#x3D;, !&#x3D;, &lt;, &gt;, IS NULL, …关系型操作符</p>
<p>+, -, *, &#x2F;, … 算术型操作符</p>
<p>AND, OR, IN, … 逻辑型操作符</p>
<p>不支持NOT IN</p>
<h2 id="Hive函数分类"><a href="#Hive函数分类" class="headerlink" title="Hive函数分类"></a>Hive函数分类</h2><ul>
<li><p>内置标准函数</p>
<p>定义：org&#x2F;apache&#x2F;hadoop&#x2F;hive&#x2F;ql&#x2F;exec&#x2F;FunctionRegistry.java</p>
</li>
<li><p>数学函数</p>
</li>
<li><p>日期函数</p>
</li>
<li><p>类型转换函数</p>
</li>
<li><p>条件函数</p>
</li>
<li><p>字符函数</p>
</li>
<li><p>内置聚合函数</p>
</li>
<li><p>内置表生成函数</p>
</li>
<li><p>自定义函数</p>
</li>
</ul>
<h2 id="Hive函数"><a href="#Hive函数" class="headerlink" title="Hive函数"></a>Hive函数</h2><p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211219193435.png"></p>
<ul>
<li>内置函数（Functions）<ul>
<li><em>math</em>: round, floor, ceil, exp, log, … </li>
<li><em>date</em>: to_date, from_unixtimestamp, year, … </li>
<li><em>conditional</em>: if, isnull, case, coalesce, … </li>
<li><em>string</em>: char, concat, lower, trim, repeat, …</li>
</ul>
</li>
<li>内置聚合函数（Aggregate Functions）<ul>
<li>count, sum, min, max, corr, …</li>
</ul>
</li>
<li>内置表生成函数（Table-generating functions）<ul>
<li>explode, posexplode, parse_url_tuple, …</li>
</ul>
</li>
</ul>
<h2 id="各种函数的区别"><a href="#各种函数的区别" class="headerlink" title="各种函数的区别"></a>各种函数的区别</h2><table>
<thead>
<tr>
<th>几对几</th>
<th>函数</th>
<th>阶段</th>
</tr>
</thead>
<tbody><tr>
<td>1:1</td>
<td>标准函数Functions （UDFs &#x3D; User Defined Functions）</td>
<td>Map阶段</td>
</tr>
<tr>
<td>n:1</td>
<td>聚合函数Aggregate Functions （UDAFs）</td>
<td>Reduce阶段</td>
</tr>
<tr>
<td>1:1</td>
<td>表生成函数Table-generating Functions （UDTFs）</td>
<td>都可以</td>
</tr>
<tr>
<td>m:n</td>
<td>窗口分析函数Windowing and Analytics Functions</td>
<td></td>
</tr>
</tbody></table>
<h2 id="表生成函数UDTF例子"><a href="#表生成函数UDTF例子" class="headerlink" title="表生成函数UDTF例子"></a>表生成函数UDTF例子</h2><p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211219193923.png"></p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> Management ( </span><br><span class="line">    manager_name STRING, </span><br><span class="line">    direct_reports <span class="keyword">ARRAY</span><span class="operator">&lt;</span>STRING<span class="operator">&gt;</span>) </span><br><span class="line">STORED <span class="keyword">AS</span> PARQUET;</span><br><span class="line"></span><br><span class="line"><span class="keyword">INSERT</span> OVERWRITE <span class="keyword">TABLE</span> Management </span><br><span class="line"><span class="keyword">SELECT</span> </span><br><span class="line">	<span class="string">&#x27;Harry&#x27;</span> manager_name,</span><br><span class="line">	<span class="keyword">array</span>(<span class="string">&#x27;James&#x27;</span>, <span class="string">&#x27;Lorren&#x27;</span>) direct_reports</span><br><span class="line">;</span><br></pre></td></tr></table></figure>



<table>
<thead>
<tr>
<th>manager_name</th>
<th>direct_reports</th>
</tr>
</thead>
<tbody><tr>
<td>Harry</td>
<td>James, Lorren</td>
</tr>
</tbody></table>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211219193932.png"></p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> manager_name, employee</span><br><span class="line"><span class="keyword">from</span> Management</span><br><span class="line">	<span class="keyword">LATERAL</span> <span class="keyword">VIEW</span> explode(direct_reports) lateral_table <span class="keyword">as</span> employee;</span><br></pre></td></tr></table></figure>



<table>
<thead>
<tr>
<th>manager_name</th>
<th>direct_reports</th>
</tr>
</thead>
<tbody><tr>
<td>Harry</td>
<td>James</td>
</tr>
<tr>
<td>Harry</td>
<td>Lorren</td>
</tr>
</tbody></table>
<h2 id="窗口分析函数-Windowing-and-Analytics-Functions"><a href="#窗口分析函数-Windowing-and-Analytics-Functions" class="headerlink" title="窗口分析函数(Windowing and Analytics Functions)"></a>窗口分析函数(Windowing and Analytics Functions)</h2><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> column_A, <span class="built_in">ROW_NUMBER</span>() <span class="keyword">OVER</span> (<span class="keyword">PARTITION</span> <span class="keyword">BY</span> column_C SORT <span class="keyword">BY</span> xxx) <span class="keyword">as</span> rn</span><br><span class="line"><span class="keyword">FROM</span> table_name; </span><br><span class="line"><span class="keyword">SELECT</span> column_A, <span class="built_in">RANK</span>() <span class="keyword">OVER</span> (<span class="keyword">PARTITION</span> <span class="keyword">BY</span> column_C) <span class="keyword">FROM</span> table_name; </span><br><span class="line"><span class="keyword">SELECT</span> column_A, <span class="built_in">DENSE_RANK</span>() <span class="keyword">OVER</span> (<span class="keyword">PARTITION</span> <span class="keyword">BY</span> column_C) <span class="keyword">FROM</span> table_name;</span><br></pre></td></tr></table></figure>

<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211219194359.png"></p>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211219194409.png"></p>
<h2 id="窗口分析函数解析"><a href="#窗口分析函数解析" class="headerlink" title="窗口分析函数解析"></a>窗口分析函数解析</h2><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> column_A,</span><br><span class="line"><span class="built_in">ROW_NUMBER</span>() <span class="keyword">OVER</span> (<span class="keyword">PARTITION</span> <span class="keyword">BY</span> column_C), </span><br><span class="line"><span class="built_in">RANK</span>() <span class="keyword">OVER</span> (<span class="keyword">PARTITION</span> <span class="keyword">BY</span> column_D), </span><br><span class="line"><span class="built_in">DENSE_RANK</span>() <span class="keyword">OVER</span> (<span class="keyword">PARTITION</span> <span class="keyword">BY</span> column_E) </span><br><span class="line"><span class="keyword">FROM</span> table_name;</span><br></pre></td></tr></table></figure>

<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211219194628.png"></p>
<h2 id="窗口分析函数举例"><a href="#窗口分析函数举例" class="headerlink" title="窗口分析函数举例"></a>窗口分析函数举例</h2><p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211219194744.png"></p>
<h2 id="动手操作-函数使用"><a href="#动手操作-函数使用" class="headerlink" title="动手操作: 函数使用"></a>动手操作: 函数使用</h2><ul>
<li>普通函数应用：<ul>
<li>关联IP国家列表统计出访问最多的5个国家</li>
</ul>
</li>
<li>分区函数应用：<ul>
<li>会话分析</li>
<li>切分原则：同一个用户两次请求间隔时间超过20分钟</li>
</ul>
</li>
</ul>
<h2 id="最流行的区域"><a href="#最流行的区域" class="headerlink" title="最流行的区域"></a>最流行的区域</h2><p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211219194925.png"></p>
<p>CIDR, 国家</p>
<p>1.0.0.0&#x2F;24,美国</p>
<p>1.1.0.0&#x2F;24,美国</p>
<p>1.0.0.0 – 1.0.0.255,美国</p>
<p>1.1.0.0 – 1.1.0.255,美国</p>
<p>IPv4：112.65.12.136</p>
<p>地区：上海市徐汇区 联通漕河泾数据中心</p>
<p>城市：上海市</p>
<p>国家：中国</p>
<h2 id="会话分析"><a href="#会话分析" class="headerlink" title="会话分析"></a>会话分析</h2><p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/%E7%A6%BB%E7%BA%BF%E5%A4%84%E7%90%86Hive.png"></p>
<h2 id="会话分析逻辑"><a href="#会话分析逻辑" class="headerlink" title="会话分析逻辑"></a>会话分析逻辑</h2><table>
<thead>
<tr>
<th>IP+UA</th>
<th>timestamp</th>
<th>Lag（timestamp，1，0）</th>
<th>Session_boundary</th>
<th>Session Number</th>
</tr>
</thead>
<tbody><tr>
<td>UserA</td>
<td>1402798399</td>
<td>NULL</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>UserA</td>
<td>1402798409</td>
<td>1402798399</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>UserA</td>
<td>1402798502</td>
<td>1402798409</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>UserA</td>
<td>1402799060</td>
<td>1402798502</td>
<td>1402799060-1402798502 &gt; 300 &#x3D;&gt; 1</td>
<td>1</td>
</tr>
<tr>
<td>UserA</td>
<td>1402799230</td>
<td>1402799060</td>
<td>1402799230-1402799060 &lt; 300 &#x3D;&gt; 0</td>
<td>1</td>
</tr>
</tbody></table>
<h2 id="分析SQL创建中间表"><a href="#分析SQL创建中间表" class="headerlink" title="分析SQL创建中间表"></a>分析SQL创建中间表</h2><p>两种方式：</p>
<ol>
<li><p>使用CREATE TABLE语法，使用完需要DROP TABLE</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> <span class="operator">&lt;</span>NAME<span class="operator">&gt;</span> <span class="keyword">AS</span> 语法</span><br><span class="line"></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> tmp_a</span><br><span class="line"><span class="keyword">AS</span></span><br><span class="line"><span class="keyword">SELECT</span> …;</span><br></pre></td></tr></table></figure>
</li>
<li><p>使用WITH语法</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">WITH</span> <span class="operator">&lt;</span>NAME<span class="operator">&gt;</span> <span class="keyword">AS</span>语法</span><br><span class="line"></span><br><span class="line"><span class="keyword">WITH</span> tmp_a <span class="keyword">as</span> (</span><br><span class="line"><span class="keyword">SELECT</span> …</span><br><span class="line">), tmp_b <span class="keyword">as</span> (</span><br><span class="line"><span class="keyword">SELECT</span> … </span><br><span class="line">)</span><br><span class="line"><span class="keyword">SELECT</span> … <span class="keyword">FROM</span> tmp_a, tmp_b…</span><br></pre></td></tr></table></figure></li>
</ol>
<h2 id="导出分析结果数据"><a href="#导出分析结果数据" class="headerlink" title="导出分析结果数据"></a>导出分析结果数据</h2><p>两种方式：</p>
<ol>
<li>Beeline直接导出到文件 – 结果不太大的时候</li>
<li>先导出到Hive表，然后通过表所在HDFS路径获取文件</li>
</ol>
<p>先介绍第一种方式：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">beeline -u jdbc:hive2://centos7-2:10000/default -n hive -p hive --outputformat=csv2 \</span><br><span class="line">-e <span class="string">&quot;select ip, count(*) cnt from pq_access_logs group by ip order by cnt desc limit 10&quot;</span> &gt; result.csv</span><br></pre></td></tr></table></figure>

<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211219200741.png"></p>
<h1 id="用Hive做数据开发"><a href="#用Hive做数据开发" class="headerlink" title="用Hive做数据开发"></a>用Hive做数据开发</h1><h2 id="导入数据到Hive"><a href="#导入数据到Hive" class="headerlink" title="导入数据到Hive"></a>导入数据到Hive</h2><ul>
<li>数据的分类<ul>
<li>非结构化（文章，多媒体等）- 不支持</li>
<li><strong>半结构化(CSV, JSON LINE, XML, LOG等）</strong></li>
<li>结构化（数据库表）</li>
</ul>
</li>
<li>两种场景<ol>
<li>其他系统的半结构化文本数据文件导入到Hive</li>
<li>数据库数据导入Hive</li>
</ol>
</li>
<li>方案<ol>
<li>先将文件上传HDFS，再通过Hive<strong>外部表</strong>来访问</li>
<li>先通过Sqoop把表数据以TSV文件形式导入HDFS，再通过Hive<strong>外部表</strong>来访问</li>
</ol>
</li>
</ul>
<h2 id="使用Hive分析网站的访问日志"><a href="#使用Hive分析网站的访问日志" class="headerlink" title="使用Hive分析网站的访问日志"></a>使用Hive分析网站的访问日志</h2><p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211219201005.png"></p>
<ul>
<li>可以帮助我们完成：</li>
<li>访问最多的地区的分析</li>
<li>分析用户会话</li>
<li>各种关于用户属性的统计</li>
</ul>
<p><strong>数据源</strong></p>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211219201028.png"></p>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211219201038.png"></p>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211219201057.png"></p>
<h2 id="Apache-HTTP服务器的访问日志"><a href="#Apache-HTTP服务器的访问日志" class="headerlink" title="Apache HTTP服务器的访问日志"></a>Apache HTTP服务器的访问日志</h2><p>有两种：</p>
<p><strong>Common Log Format</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">格式</span><br><span class="line">&lt;IP&gt; - &lt;USER&gt; &lt;TIME&gt; &quot;&lt;VERB&gt; &lt;URL&gt; &lt;PR&gt;&quot; &lt;STATUS&gt; &lt;SIZE&gt;</span><br></pre></td></tr></table></figure>

<p><strong>Combined Log Format （主要使用）</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">格式</span><br><span class="line">Common Log Format + &lt;REFERER&gt; &lt;UA&gt; (额外两个属性) </span><br><span class="line">+ customized</span><br></pre></td></tr></table></figure>

<p>+<span style="color:#6f309f">cookies</span>：</p>
<p>83.0.11.22 - - [02&#x2F;Aug&#x2F;2009:12: 31: 30 +0200] “GET &#x2F;ct &#x2F; HTTP&#x2F;1.1” 200 - “-“ “Mozilla&#x2F;5.0 (Windows; U; Windows NT 5.1; pl; rv:1.9.1.1) Gecko&#x2F;2009071 5 Firefox&#x2F;3.5.1” “<span style="color:#6f309f">c1&#x3D;1; c2&#x3D;2; PHPSESSID&#x3D;6c4513f22852a235b8988da822f89d04</span>“</p>
<p>参考：<a target="_blank" rel="noopener" href="http://httpd.apache.org/docs/current/logs.html">http://httpd.apache.org/docs/current/logs.html</a></p>
<h2 id="访问日志示例"><a href="#访问日志示例" class="headerlink" title="访问日志示例"></a>访问日志示例</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">127.0.0.1 - frank [10/Oct/2000:13:55:36 -0700] &quot;GET /apache_pb.gif HTTP/1.0&quot; 200 2326 </span><br><span class="line">&quot;http://www.example.com/start.html&quot; &quot;Mozilla/4.08 [en] (Win98; I ;Nav)&quot; </span><br><span class="line">123.65.150.10 - - [23/Aug/2010:03:50:59 +0000] &quot;POST /wordpress3/wp-admin/admin-ajax.php</span><br><span class="line">HTTP/1.1&quot; 200 2 &quot;http://www.example.com/wordpress3/wp-admin/post-new.php&quot; </span><br><span class="line">&quot;Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10_6_4; en-US) AppleWebKit/534.3 </span><br><span class="line">(KHTML, like Gecko) Chrome/6.0.472.25 Safari/534.3“</span><br><span class="line">83.0.11.22 - - [02/Aug/2009:12:31:30 +0200] &quot;GET /ct/ HTTP/1.1&quot; 200 - &quot;-&quot; </span><br><span class="line">&quot;Mozilla/5.0 (Windows; U; Windows NT 5.1; pl; rv:1.9.1.1) Gecko/20090715 </span><br><span class="line">Firefox/3.5.1&quot; &quot;c1=1; c2=2; PHPSESSID=6c4513f22852a235b8988da822f89d04&quot;</span><br></pre></td></tr></table></figure>

<p><strong>IPv4</strong>：194.0.2.235 </p>
<p><strong>HTTP Verb</strong>：GET|POST</p>
<p><strong>URL</strong>: &#x2F;apache_pb.gif </p>
<p><strong>STATUS</strong>：200-500</p>
<p><strong>Timestamp</strong>： 10&#x2F;Oct&#x2F;2000:13:55:36 -0700 </p>
<p><strong>Cookie</strong>：c1&#x3D;1; c2&#x3D;2; PHPSESSID&#x3D;6c4513f22852a235b8988da822f89d04</p>
<h2 id="动手操作-日志表创建和数据导入"><a href="#动手操作-日志表创建和数据导入" class="headerlink" title="动手操作: 日志表创建和数据导入"></a>动手操作: 日志表创建和数据导入</h2><ul>
<li>将日志文件传到HDFS</li>
<li>建立Hive外部表对应于日志文件</li>
<li>将TEXT表转换为PARQUET表</li>
</ul>
<h2 id="Hive-DDL-创建表"><a href="#Hive-DDL-创建表" class="headerlink" title="Hive DDL: 创建表"></a>Hive DDL: 创建表</h2><p>创建表时指定Location路径</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">USE <span class="operator">&lt;</span>database_name<span class="operator">&gt;</span>; <span class="operator">/</span><span class="operator">/</span> 默认是<span class="keyword">default</span>库</span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> <span class="operator">&lt;</span>table_name<span class="operator">&gt;</span> (</span><br><span class="line">	<span class="operator">&lt;</span>column_name<span class="operator">&gt;</span><span class="operator">&lt;</span>column_type<span class="operator">&gt;</span>, ...</span><br><span class="line">)</span><br><span class="line">LOCATION &quot;path/to/hdfs/location&quot;; <span class="operator">/</span><span class="operator">/</span> LOCATION可选参数，默认是<span class="operator">/</span><span class="keyword">user</span><span class="operator">/</span>hive<span class="operator">/</span>warehouse<span class="operator">/</span>$db<span class="operator">/</span>$<span class="keyword">table</span></span><br></pre></td></tr></table></figure>

<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211219204557.png"></p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">USE gp;</span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> tab_dataset (</span><br><span class="line">rst_column STRING,</span><br><span class="line">second_column STRING,</span><br><span class="line"><span class="keyword">value</span> <span class="type">INT</span>);</span><br><span class="line"></span><br><span class="line">删除表：<span class="keyword">DROP</span> <span class="keyword">TABLE</span> <span class="operator">&lt;</span>TableName<span class="operator">&gt;</span>;</span><br></pre></td></tr></table></figure>

<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211219204631.png"></p>
<h2 id="记录行格式：DELIMITED"><a href="#记录行格式：DELIMITED" class="headerlink" title="记录行格式：DELIMITED"></a>记录行格式：DELIMITED</h2><ul>
<li><p>指定分隔符：字段和行</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> tab_dataset (</span><br><span class="line">rst_column STRING,</span><br><span class="line">second_column STRING,</span><br><span class="line">arr <span class="keyword">ARRAY</span><span class="operator">&lt;</span>STRING<span class="operator">&gt;</span>,</span><br><span class="line"><span class="keyword">value</span> <span class="type">INT</span>)</span><br><span class="line"><span class="type">ROW</span> FORMAT DELIMITED</span><br><span class="line">FIELDS TERMINATED <span class="keyword">BY</span> ‘\t’</span><br><span class="line">LINES TERMINATED <span class="keyword">BY</span> ‘\n’</span><br><span class="line">;</span><br><span class="line"></span><br><span class="line"><span class="type">ROW</span> FORMAT有两种值：DELIMITED和SERDE, 先看第一种</span><br></pre></td></tr></table></figure>
</li>
<li><p>以上方式创建的是内部表 – 删除表时会连数据一起删除, LOCATION默认是在HDFS的&#x2F;user&#x2F;hive&#x2F;warehouse的目录下，为了管理方便内部表创建时不要指定LOCATION（使用默认值）</p>
</li>
<li><p>还有一种叫外部表</p>
<ul>
<li><p>表删除后，数据还在HDFS上</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">EXTERNAL</span> <span class="keyword">TABLE</span> tab_dataset() LOCATION ‘…’;</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ul>
<h2 id="记录行格式：自定义的SERDE（序列化和反序列化）"><a href="#记录行格式：自定义的SERDE（序列化和反序列化）" class="headerlink" title="记录行格式：自定义的SERDE（序列化和反序列化）"></a>记录行格式：自定义的SERDE（序列化和反序列化）</h2><p>序列化是对象转换为字节序列的过程。</p>
<p>反序列化是字节序列恢复为对象的过程。</p>
<p>使用正则表达式反序列化日志文件</p>
<p>内置：Avro ORC RegEx Thrift Parquet CSV JsonSerDe</p>
<p>日志文件存放在这个目录下：&#x2F;user&#x2F;hive&#x2F;warehouse&#x2F;original_access_logs</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">EXTERNAL</span> <span class="keyword">TABLE</span> intermediate_access_logs (</span><br><span class="line">ip STRING,</span><br><span class="line"><span class="type">date</span> STRING,</span><br><span class="line"><span class="keyword">method</span> STRING,</span><br><span class="line">url STRING,</span><br><span class="line">http_version STRING,</span><br><span class="line">code1 STRING,</span><br><span class="line">code2 STRING,</span><br><span class="line">dash STRING,</span><br><span class="line">user_agent STRING)</span><br><span class="line"><span class="type">ROW</span> FORMAT SERDE <span class="string">&#x27;org.apache.hadoop.hive.contrib.serde2.RegexSerDe&#x27;</span></span><br><span class="line"><span class="keyword">WITH</span> SERDEPROPERTIES (</span><br><span class="line"><span class="string">&#x27;input.regex&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;([^ ]*) - - \\[([^\\]]*)\\] &quot;([^\ ]*) ([^\ ]*) ([^\ ]*)&quot; (\\d*) (\\d*) &quot;([^&quot;]*)&quot; &quot;([^&quot;]*)&quot;&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;output.format.string&#x27;</span> <span class="operator">=</span> &quot;%1$$s %2$$s %3$$s %4$$s %5$$s %6$$s %7$$s %8$$s %9$$s&quot;)</span><br><span class="line">LOCATION <span class="string">&#x27;/user/hive/warehouse/original_access_logs&#x27;</span>;</span><br></pre></td></tr></table></figure>

<p>RegexSerDe可以处理有规则的LOG类文本，还有个常用的JSON序列化类</p>
<p>是org.openx.data.jsonserde.JsonSerDe，可以处理一行一个JSON串的文件</p>
<h2 id="字段类型（基本）"><a href="#字段类型（基本）" class="headerlink" title="字段类型（基本）"></a>字段类型（基本）</h2><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">EXTERNAL</span> <span class="keyword">TABLE</span> tab_dataset (</span><br><span class="line">rst_column STRING, second_column STRING, <span class="keyword">value</span> <span class="type">INT</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<p>特别注意：Hive表的Schema on read特性：</p>
<ol>
<li>文件和表结构不一致时，只会在读数据时报错</li>
<li>改了字段数据类型，数据要重写入</li>
</ol>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211219205113.png"></p>
<h2 id="字段类型（复杂）"><a href="#字段类型（复杂）" class="headerlink" title="字段类型（复杂）"></a>字段类型（复杂）</h2><p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211219205142.png"></p>
<h2 id="分区表"><a href="#分区表" class="headerlink" title="分区表"></a>分区表</h2><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> partitioned_access_log ( ip STRING, </span><br><span class="line">) ... </span><br><span class="line">PARTITIONED <span class="keyword">BY</span> (request_date STRING) ...;</span><br><span class="line"></span><br><span class="line">hdfs:<span class="operator">/</span><span class="operator">/</span><span class="operator">/</span>access_logs<span class="operator">/</span> ... </span><br><span class="line"></span><br><span class="line">— <span class="number">2017</span>_01_20 </span><br><span class="line"></span><br><span class="line">— <span class="number">2017</span>_01_21 </span><br><span class="line"></span><br><span class="line">— <span class="number">2017</span>_01_22 ... </span><br><span class="line"></span><br><span class="line">— &quot;today&quot; </span><br><span class="line"></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> partitioned_access_log ( ip STRING, </span><br><span class="line">) ... </span><br><span class="line">PARTITIONED <span class="keyword">BY</span> (<span class="keyword">year</span> STRING, <span class="keyword">month</span> STRING, <span class="keyword">day</span> STRING) ...;</span><br><span class="line"></span><br><span class="line">hdfs:<span class="operator">/</span><span class="operator">/</span><span class="operator">/</span>access_logs<span class="operator">/</span> ...</span><br><span class="line">— <span class="number">2017</span></span><br><span class="line">—— <span class="number">01</span> </span><br><span class="line">——— <span class="number">20</span> </span><br><span class="line">——— <span class="number">21</span> </span><br><span class="line">——— <span class="number">22</span> </span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">查看分区表的分区: <span class="keyword">SHOW</span> PARTITIONS <span class="operator">&lt;</span>TableName<span class="operator">&gt;</span>;</span><br></pre></td></tr></table></figure>

<h2 id="分区表作用"><a href="#分区表作用" class="headerlink" title="分区表作用"></a>分区表作用</h2><ul>
<li>避免全表扫描</li>
<li>提升查询性能</li>
<li>方便数据按分区写入（覆盖）</li>
<li>方便管理</li>
</ul>
<h2 id="数据写入分区表"><a href="#数据写入分区表" class="headerlink" title="数据写入分区表"></a>数据写入分区表</h2><p><strong>静态分区</strong></p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">FROM</span> raw_access_log</span><br><span class="line"><span class="keyword">INSERT</span> OVERWRITE <span class="keyword">TABLE</span> partitioned_access_log</span><br><span class="line"><span class="keyword">PARTITION</span> (<span class="keyword">year</span><span class="operator">=</span>&quot;2017&quot;, <span class="keyword">month</span><span class="operator">=</span>&quot;03&quot;, <span class="keyword">day</span><span class="operator">=</span>&quot;25&quot;) </span><br><span class="line"><span class="keyword">SELECT</span> ip, ...</span><br></pre></td></tr></table></figure>

<p><strong>动态分区</strong>，分区字段要写在最后，字段的顺序很重要</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">FROM</span> raw_access_log</span><br><span class="line"><span class="keyword">INSERT</span> OVERWRITE <span class="keyword">TABLE</span> </span><br><span class="line">partitioned_access_log</span><br><span class="line"><span class="keyword">PARTITION</span> (<span class="keyword">year</span>, <span class="keyword">month</span>, <span class="keyword">day</span>)</span><br><span class="line"><span class="keyword">SELECT</span> ip, ..., <span class="keyword">year</span>, <span class="keyword">month</span>, <span class="keyword">day</span> </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p><strong>混合模式</strong></p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">FROM</span> raw_access_log</span><br><span class="line"><span class="keyword">INSERT</span> OVERWRITE <span class="keyword">TABLE</span> partitioned_access_log</span><br><span class="line"><span class="keyword">PARTITION</span> (<span class="keyword">year</span><span class="operator">=</span>&quot;2017&quot;, <span class="keyword">month</span>, <span class="keyword">day</span>)</span><br><span class="line"><span class="keyword">SELECT</span> ip, ..., <span class="keyword">month</span>, <span class="keyword">day</span></span><br></pre></td></tr></table></figure>

<h2 id="写分区表设置参数"><a href="#写分区表设置参数" class="headerlink" title="写分区表设置参数"></a>写分区表设置参数</h2><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SET</span> hive.exec.max.dynamic.partitions<span class="operator">=</span><span class="number">2048</span>; </span><br><span class="line"><span class="keyword">SET</span> hive.exec.max.dynamic.partitions.pernode<span class="operator">=</span><span class="number">256</span>; </span><br><span class="line"><span class="keyword">SET</span> hive.exec.max.created.files<span class="operator">=</span><span class="number">10000</span>; </span><br><span class="line"><span class="comment">-- 防止空的分区产生</span></span><br><span class="line"><span class="keyword">SET</span> hive.error.on.empty.partition<span class="operator">=</span><span class="literal">true</span>; </span><br><span class="line"></span><br><span class="line"><span class="comment">-- 允许动态分区</span></span><br><span class="line"><span class="keyword">SET</span> hive.exec.dynamic.partition.mode<span class="operator">=</span>nonstrict;</span><br></pre></td></tr></table></figure>

<h2 id="动手操作-分区表"><a href="#动手操作-分区表" class="headerlink" title="动手操作: 分区表"></a>动手操作: 分区表</h2><ol>
<li>创建分区表</li>
<li>往分区表写入数据</li>
<li>观察分区表物理存储结构</li>
</ol>
<h2 id="分桶表"><a href="#分桶表" class="headerlink" title="分桶表"></a>分桶表</h2><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> granular_access_log ( ip STRING, </span><br><span class="line">) ... </span><br><span class="line">PARTITIONED <span class="keyword">BY</span> (request_date STRING)</span><br><span class="line">CLUSTERED <span class="keyword">BY</span> (column_name, ...) </span><br><span class="line"><span class="keyword">INTO</span> <span class="number">200</span> BUCKETS ...;</span><br></pre></td></tr></table></figure>

<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211219212814.png"></p>
<p><strong>CLUSTERED BY</strong> (<strong>user_id</strong>) </p>
<p>分桶函数底层是hash函数：INT字段是hash后取模，STRING字段hash后能保证平均分布</p>
<p>整数字段的hash函数返回原值，string字段hash函数和Java String.hashCode类似，多个字段组合的hash函数和Java List.hashCode类似</p>
<p>分桶可以让数据分布更加均匀</p>
<h2 id="桶内Sort的用处"><a href="#桶内Sort的用处" class="headerlink" title="桶内Sort的用处"></a>桶内Sort的用处</h2><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> granular_access_log ( ip STRING, </span><br><span class="line">) ... </span><br><span class="line">PARTITIONED <span class="keyword">BY</span> (request_date STRING) </span><br><span class="line">CLUSTERED <span class="keyword">BY</span> (user_id) </span><br><span class="line">SORTED <span class="keyword">BY</span> (user_id) </span><br><span class="line"><span class="keyword">INTO</span> <span class="number">200</span> BUCKETS ...;</span><br></pre></td></tr></table></figure>

<p><strong>Insert时需设置SET</strong> hive.enforce.sorting&#x3D;true; # 2.0后默认为true</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> user_id, <span class="built_in">COUNT</span>(<span class="number">1</span>) </span><br><span class="line"><span class="keyword">FROM</span> granular_access_log</span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> user_id;</span><br></pre></td></tr></table></figure>

<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211219212959.png"></p>
<p>使执行计划更简单</p>
<h2 id="分桶表写入需要注意的地方"><a href="#分桶表写入需要注意的地方" class="headerlink" title="分桶表写入需要注意的地方"></a>分桶表写入需要注意的地方</h2><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SET</span> mapred.reduce.tasks <span class="operator">=</span> <span class="number">200</span>; </span><br><span class="line"><span class="keyword">FROM</span> raw_access_log</span><br><span class="line"><span class="keyword">INSERT</span> OVERWRITE <span class="keyword">TABLE</span> granular_access_log <span class="keyword">PARTITION</span> <span class="keyword">BY</span> (request_date)</span><br><span class="line"><span class="keyword">SELECT</span> ..., request_date</span><br><span class="line"><span class="keyword">WHERE</span> ... DISTRIBUTE <span class="keyword">BY</span> user_id</span><br><span class="line">[SORT <span class="keyword">BY</span> user_id]</span><br><span class="line">;</span><br><span class="line"></span><br><span class="line"><span class="keyword">SET</span> hive.enforce.bucketing<span class="operator">=</span><span class="literal">true</span>; # <span class="number">2.0</span>后默认为<span class="literal">true</span></span><br><span class="line"><span class="keyword">FROM</span> raw_access_log</span><br><span class="line"><span class="keyword">INSERT</span> OVERWRITE <span class="keyword">TABLE</span> granular_access_log <span class="keyword">PARTITION</span> <span class="keyword">BY</span> (request_date)</span><br><span class="line"><span class="keyword">SELECT</span> ..., request_date</span><br><span class="line"><span class="keyword">WHERE</span> ...;</span><br></pre></td></tr></table></figure>

<h2 id="动手操作-分桶表"><a href="#动手操作-分桶表" class="headerlink" title="动手操作: 分桶表"></a>动手操作: 分桶表</h2><ul>
<li>创建日志分桶表<ul>
<li>按IP地址的第一段分桶</li>
<li>桶内按request_time排序</li>
</ul>
</li>
<li>观察分桶表的物理存储结构</li>
</ul>
<h2 id="表的文件储存格式-File-Format"><a href="#表的文件储存格式-File-Format" class="headerlink" title="表的文件储存格式(File Format)"></a>表的文件储存格式(File Format)</h2><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> tab_dataset (</span><br><span class="line">rst_column STRING,</span><br><span class="line">second_column STRING,</span><br><span class="line"><span class="keyword">value</span> <span class="type">INT</span></span><br><span class="line">)</span><br><span class="line"><span class="type">ROW</span> FORMAT DELIMITED</span><br><span class="line">FIELDS TERMINATED <span class="keyword">BY</span> <span class="string">&#x27;\t&#x27;</span></span><br><span class="line">LINES TERMINATED <span class="keyword">BY</span> <span class="string">&#x27;\n&#x27;</span></span><br><span class="line">STORED <span class="keyword">AS</span> file_format</span><br><span class="line">;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">CSV, TSV等格式：FIELDS TERMINATED <span class="keyword">BY</span> <span class="string">&#x27;\t&#x27;</span></span><br><span class="line">默认：Text File (文本)：LINES TERMINATED <span class="keyword">BY</span> <span class="string">&#x27;\n&#x27;</span></span><br></pre></td></tr></table></figure>

<p>常用的有：Parquet(列式)，Avro，ORC(列式)，Sequence File，INPUT FORMAT &amp; OUTPUT FORMAT (二进制)</p>
<h2 id="Record-Columnar-File-RCFile-列式储存"><a href="#Record-Columnar-File-RCFile-列式储存" class="headerlink" title="Record Columnar File (RCFile, 列式储存)"></a>Record Columnar File (RCFile, 列式储存)</h2><ul>
<li>设计目标<ul>
<li>更快地加载, 文件体积更小</li>
<li>更快速的查询</li>
<li>更高效的储存空间利用</li>
</ul>
</li>
</ul>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211219213357.png"><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211219213405.png"></p>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211219213441.png"></p>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211219213456.png"></p>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211219213515.png"></p>
<h2 id="列式存储之压缩"><a href="#列式存储之压缩" class="headerlink" title="列式存储之压缩"></a>列式存储之压缩</h2><p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211219213829.png"></p>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211219213859.png"></p>
<p>时间字段压缩</p>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211219213920.png"></p>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211219214026.png"></p>
<p>字符和数字类型的字段的压缩算法</p>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211219214047.png"></p>
<h2 id="两种最常用列式存储格式"><a href="#两种最常用列式存储格式" class="headerlink" title="两种最常用列式存储格式"></a>两种最常用列式存储格式</h2><h3 id="orc"><a href="#orc" class="headerlink" title="orc"></a>orc</h3><p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211219214113.png"></p>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211219214125.png"></p>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211219214139.png"></p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> my_orc_table ( </span><br><span class="line">)... </span><br><span class="line">STORED <span class="keyword">AS</span> orc </span><br><span class="line">TBLPROPERTIES (&quot;orc.compress&quot;<span class="operator">=</span>&quot;NONE&quot;) ...;</span><br></pre></td></tr></table></figure>

<h3 id="parquet"><a href="#parquet" class="headerlink" title="parquet"></a>parquet</h3><p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211219214218.png"></p>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211219214226.png"></p>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211219214237.png"></p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> my_parquet_table ( </span><br><span class="line">)... </span><br><span class="line">STORED <span class="keyword">AS</span> parquet;</span><br></pre></td></tr></table></figure>

<h2 id="动手操作-ORC表"><a href="#动手操作-ORC表" class="headerlink" title="动手操作: ORC表"></a>动手操作: ORC表</h2><ul>
<li>创建ORC表<ul>
<li>启用压缩</li>
</ul>
</li>
<li>经验<ul>
<li>时间和空间的权衡</li>
<li>依据实际测试</li>
</ul>
</li>
</ul>
<h2 id="Hive-DDL知识总结"><a href="#Hive-DDL知识总结" class="headerlink" title="Hive DDL知识总结"></a>Hive DDL知识总结</h2><p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211219214335.png"></p>
<h2 id="Hive-DML"><a href="#Hive-DML" class="headerlink" title="Hive DML"></a>Hive DML</h2><ul>
<li>数据导入进表<ul>
<li>手工从HDFS导入</li>
<li>Flume可以导入文件</li>
<li>用工具从Relational DB导入<ul>
<li>Sqoop Import</li>
<li>StreamSets Data Collector</li>
<li>Kettle</li>
</ul>
</li>
</ul>
</li>
<li>数据从表里导出<ul>
<li>导出到HDFS</li>
<li>用工具导出到其他数据库<ul>
<li>Sqoop Export</li>
<li>Kettle</li>
<li>第三方Storage Handler, 如ES, HBase</li>
</ul>
</li>
</ul>
</li>
<li>数据导入策略<ul>
<li>全量<ul>
<li>覆盖</li>
<li>追加分区(比如给表做快照)</li>
</ul>
</li>
<li>增量<ul>
<li>通过更新时间来同步数据</li>
<li>需要使用数据合并逻辑来更新表的数据,需要注意的是Hive一般不支持Update和Delete语句<ul>
<li>除非使用ACID和Transactions功能</li>
</ul>
</li>
<li>或者使用Apache Kudu</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="动手操作：Sqoop数据导入Hive"><a href="#动手操作：Sqoop数据导入Hive" class="headerlink" title="动手操作：Sqoop数据导入Hive"></a>动手操作：Sqoop数据导入Hive</h2><ol>
<li>MySQL数据表的数据导入HDFS</li>
<li>创建外部分区表访问该数据</li>
</ol>
<h2 id="常用DML语法"><a href="#常用DML语法" class="headerlink" title="常用DML语法"></a>常用DML语法</h2><ol>
<li><p><strong>写数据到HDFS</strong></p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">INSERT</span> OVERWRITE DIRECTORY </span><br><span class="line">‘hdfs:<span class="operator">/</span><span class="operator">/</span><span class="operator">/</span>tmp<span class="operator">/</span>employees<span class="string">&#x27;</span></span><br><span class="line"><span class="string">SELECT name, salary, address</span></span><br><span class="line"><span class="string">FROM employees</span></span><br><span class="line"><span class="string">WHERE …;</span></span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>写数据到表</strong></p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">INSERT</span> OVERWRITE <span class="keyword">TABLE</span> t_emp</span><br><span class="line"><span class="keyword">SELECT</span> name, salary, address</span><br><span class="line"><span class="keyword">FROM</span> employees</span><br><span class="line"><span class="keyword">WHERE</span> …;</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>multiple-insert statement</strong></p>
</li>
</ol>
   <figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">	<span class="keyword">FROM</span> raw_table</span><br><span class="line"><span class="keyword">INSERT</span> OVERWRITE <span class="keyword">TABLE</span> us_employees</span><br><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span></span><br><span class="line"><span class="keyword">WHERE</span> country <span class="operator">=</span> <span class="string">&#x27;US&#x27;</span></span><br><span class="line"><span class="keyword">INSERT</span> OVERWRITE <span class="keyword">TABLE</span> uk_employees</span><br><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span></span><br><span class="line"><span class="keyword">WHERE</span> country <span class="operator">=</span> <span class="string">&#x27;UK&#x27;</span></span><br><span class="line">…;</span><br></pre></td></tr></table></figure>

<ol start="4">
<li><p><strong>导出到表Create Table AS</strong></p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> us_employees</span><br><span class="line"><span class="keyword">AS</span></span><br><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> raw_table</span><br><span class="line"><span class="keyword">WHERE</span> country <span class="operator">=</span> ‘US’</span><br></pre></td></tr></table></figure></li>
</ol>
<h1 id="手动操作"><a href="#手动操作" class="headerlink" title="手动操作"></a>手动操作</h1><ul>
<li>将网站的访问日志导入Hive，然后</li>
</ul>
<ol>
<li>统计访问量最多的10个URL</li>
<li>统计网站两天内每天的PV和UV（IP+UserAgent的组合可以确定一个唯一的用户）</li>
<li>统计网站访问最多的5个落地页（每个会话的第一次访问称为落地页）</li>
</ol>
<h1 id="操作-Hive-Sql"><a href="#操作-Hive-Sql" class="headerlink" title="操作 Hive Sql"></a>操作 Hive Sql</h1><ol>
<li><p>统计 access log ⾥访问最多的 5 个 IP</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> ip, <span class="built_in">count</span>(<span class="operator">*</span>) cnt </span><br><span class="line"><span class="keyword">from</span> pq_access_logs </span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> ip </span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span> cnt <span class="keyword">desc</span> </span><br><span class="line">limit <span class="number">5</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>关联 IP 国家列表统计出访问最多的 5 个国家</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span></span><br><span class="line">	F.country_name, </span><br><span class="line">	<span class="built_in">count</span>(<span class="operator">*</span>) ip_cnt, </span><br><span class="line">	<span class="built_in">sum</span>(F.cnt) visits </span><br><span class="line"><span class="keyword">from</span> (</span><br><span class="line">    <span class="keyword">select</span></span><br><span class="line">    	A.ip, A.cnt, A.ip_to_int, C.country_name </span><br><span class="line">    <span class="keyword">from</span> (</span><br><span class="line">        <span class="keyword">select</span> </span><br><span class="line">        	ip, </span><br><span class="line">        	cnt, </span><br><span class="line">        	<span class="built_in">cast</span>(regexp_extract(ip,&quot;(\\d+)\\.(\\d+)\\.(\\d+)\\.(\\d+)&quot;,<span class="number">1</span>) <span class="keyword">as</span> <span class="type">bigint</span>) <span class="operator">*</span> <span class="number">16777216</span> <span class="operator">+</span> </span><br><span class="line">        	<span class="built_in">cast</span>(regexp_extract(ip,&quot;(\\d+)\\.(\\d+)\\.(\\d+)\\.(\\d+)&quot;,<span class="number">2</span>) <span class="keyword">as</span> <span class="type">bigint</span>) <span class="operator">*</span> <span class="number">65536</span> <span class="operator">+</span> </span><br><span class="line">        	<span class="built_in">cast</span>(regexp_extract(ip,&quot;(\\d+)\\.(\\d+)\\.(\\d+)\\.(\\d+)&quot;,<span class="number">3</span>) <span class="keyword">as</span> <span class="type">bigint</span>) <span class="operator">*</span> <span class="number">256</span> <span class="operator">+</span> </span><br><span class="line">        	<span class="built_in">cast</span>(regexp_extract(ip,&quot;(\\d+)\\.(\\d+)\\.(\\d+)\\.(\\d+)&quot;,<span class="number">4</span>) <span class="keyword">as</span> <span class="type">bigint</span>) <span class="keyword">as</span> ip_to_int, </span><br><span class="line">        	regexp_extract(ip,&quot;(\\d+)\\.(\\d+)\\.(\\d+)\\.(\\d+)&quot;,<span class="number">1</span>) first_no </span><br><span class="line">        <span class="keyword">from</span> (</span><br><span class="line">            <span class="keyword">select</span> ip, <span class="built_in">count</span>(<span class="operator">*</span>) cnt </span><br><span class="line">			<span class="keyword">from</span> pq_access_logs </span><br><span class="line">            <span class="keyword">group</span> <span class="keyword">by</span> ip </span><br><span class="line">        ) T </span><br><span class="line">    ) A </span><br><span class="line">    <span class="keyword">left</span> <span class="keyword">join</span> ip_country_block B </span><br><span class="line">    	<span class="keyword">on</span> (split(B.network, <span class="string">&#x27;\\.&#x27;</span>)[<span class="number">0</span>] <span class="operator">=</span> first_no) </span><br><span class="line">    <span class="keyword">left</span> <span class="keyword">join</span> ip_country_location C </span><br><span class="line">    	<span class="keyword">on</span> (B.geoname_id <span class="operator">=</span> C.geoname_id) </span><br><span class="line">    <span class="keyword">where</span> A.ip_to_int <span class="keyword">between</span> B.first_ip_no <span class="keyword">and</span> B.last_ip_no </span><br><span class="line">) F </span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> F.country_name </span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span> visits <span class="keyword">desc</span> </span><br><span class="line">limit <span class="number">5</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>会话分析</p>
<p>建⽴会话表</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">DROP</span> <span class="keyword">TABLE</span> IF <span class="keyword">EXISTS</span> mw_sessions; </span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> mw_sessions ( </span><br><span class="line">    session_id STRING, </span><br><span class="line">    session_number <span class="type">int</span>, </span><br><span class="line">    session_boundary <span class="type">int</span>, </span><br><span class="line">    request_time STRING, </span><br><span class="line">    <span class="type">timestamp</span> <span class="type">int</span>, </span><br><span class="line">    ip STRING, </span><br><span class="line">    user_agent STRING, </span><br><span class="line">    url STRING) </span><br><span class="line">STORED <span class="keyword">AS</span> PARQUET;</span><br></pre></td></tr></table></figure>

<p>会话分析 SQL</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 找到会话的边界</span></span><br><span class="line"><span class="comment">-- 通过比较当前记录和前一条记录的timestamp字段, 如果间隔时间超过5分钟（假设），则赋值为1，否则为0 </span></span><br><span class="line"><span class="keyword">with</span> session_start <span class="keyword">as</span> ( </span><br><span class="line">	<span class="keyword">SELECT</span> </span><br><span class="line">		<span class="keyword">CASE</span></span><br><span class="line">			<span class="keyword">WHEN</span> `<span class="type">timestamp</span>` <span class="operator">-</span> <span class="built_in">lag</span>(`<span class="type">timestamp</span>`, <span class="number">1</span>, <span class="number">0</span>) <span class="keyword">OVER</span> (<span class="keyword">PARTITION</span> <span class="keyword">BY</span> ip, user_agent <span class="keyword">ORDER</span> <span class="keyword">BY</span> `<span class="type">timestamp</span>`) <span class="operator">&gt;</span> <span class="number">300</span> </span><br><span class="line">				<span class="keyword">THEN</span> <span class="number">1</span> </span><br><span class="line">			<span class="keyword">ELSE</span> <span class="number">0</span> </span><br><span class="line">			<span class="keyword">END</span> <span class="keyword">as</span> session_boundary, </span><br><span class="line">		request_time, </span><br><span class="line">		`<span class="type">timestamp</span>`, </span><br><span class="line">		ip, </span><br><span class="line">		user_agent, </span><br><span class="line">		url </span><br><span class="line">	<span class="keyword">FROM</span> pq_access_logs </span><br><span class="line">),</span><br><span class="line"><span class="comment">-- 填充会话编号</span></span><br><span class="line"><span class="comment">-- 通过对会话边界的累加，每个会话会有一个自增长的会话编号</span></span><br><span class="line">sn <span class="keyword">as</span> ( </span><br><span class="line">	<span class="keyword">SELECT</span> </span><br><span class="line">		<span class="built_in">SUM</span>(session_boundary) <span class="keyword">OVER</span> (<span class="keyword">PARTITION</span> <span class="keyword">BY</span> ip, user_agent <span class="keyword">ORDER</span> <span class="keyword">BY</span> `<span class="type">timestamp</span>`) <span class="keyword">as</span> session_number, </span><br><span class="line">    <span class="operator">*</span> </span><br><span class="line">    <span class="keyword">FROM</span> session_start </span><br><span class="line">),</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 为每一个会话生成一个唯一的会话ID </span></span><br><span class="line">t_session_id <span class="keyword">as</span> ( </span><br><span class="line">    <span class="keyword">SELECT</span> </span><br><span class="line">    	reflect(<span class="string">&#x27;org.apache.commons.codec.digest.DigestUtils&#x27;</span>, <span class="string">&#x27;shaHex&#x27;</span>, </span><br><span class="line">			CONCAT( </span><br><span class="line">                <span class="built_in">FIRST_VALUE</span>(`<span class="type">timestamp</span>`) </span><br><span class="line">                	<span class="keyword">OVER</span> (<span class="keyword">PARTITION</span> <span class="keyword">BY</span> ip, user_agent, session_number <span class="keyword">ORDER</span> <span class="keyword">BY</span> `<span class="type">timestamp</span>`), </span><br><span class="line">                ip, user_agent) </span><br><span class="line">		)</span><br><span class="line">    	<span class="keyword">as</span> session_id, </span><br><span class="line">    	<span class="operator">*</span> </span><br><span class="line">    	<span class="keyword">FROM</span> sn </span><br><span class="line">)</span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> mw_sessions <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> t_session_id;</span><br></pre></td></tr></table></figure>

<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> session_id, ip, user_agent, request_time, url <span class="keyword">from</span> mw_sessions limit <span class="number">10</span>;</span><br></pre></td></tr></table></figure>
</li>
<li><p>⽇志表创建和数据导⼊</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># 将日志文件传到HDFS </span><br><span class="line">hdfs dfs <span class="operator">-</span>mkdir <span class="operator">/</span><span class="keyword">user</span><span class="operator">/</span>hive<span class="operator">/</span>warehouse<span class="operator">/</span>original_access_logs</span><br><span class="line">hdfs dfs <span class="operator">-</span>put access.log <span class="operator">/</span><span class="keyword">user</span><span class="operator">/</span>hive<span class="operator">/</span>warehouse<span class="operator">/</span>original_access_logs</span><br></pre></td></tr></table></figure>

<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">EXTERNAL</span> <span class="keyword">TABLE</span> original_access_logs ( </span><br><span class="line">    ip STRING, </span><br><span class="line">    request_time STRING, </span><br><span class="line">    <span class="keyword">method</span> STRING, </span><br><span class="line">    url STRING, </span><br><span class="line">    http_version STRING, </span><br><span class="line">    code1 STRING, </span><br><span class="line">    code2 STRING, </span><br><span class="line">    dash STRING, </span><br><span class="line">    user_agent STRING) </span><br><span class="line"><span class="type">ROW</span> FORMAT SERDE <span class="string">&#x27;org.apache.hadoop.hive.contrib.serde2.RegexSerDe&#x27;</span> </span><br><span class="line"><span class="keyword">WITH</span> SERDEPROPERTIES ( </span><br><span class="line">    <span class="string">&#x27;input.regex&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;([^ ]*) - - \\[([^\\]]*)\\] &quot;([^\ ]*) ([^\ ]*) ([^\ ]*)&quot; (\\d*) (\\d*) &quot;([^&quot;]*)&quot; &quot;([^&quot;]*)&quot;&#x27;</span>, </span><br><span class="line">    <span class="string">&#x27;output.format.string&#x27;</span> <span class="operator">=</span> &quot;%1$$s %2$$s %3$$s %4$$s %5$$s %6$$s %7$$s %8$$s %9$$s&quot;) </span><br><span class="line">LOCATION <span class="string">&#x27;/user/hive/warehouse/original_access_logs&#x27;</span>; </span><br><span class="line"></span><br><span class="line"><span class="comment">-- SELECT遇到ClassNotFound的错误, 说明需要将contrib包引入，根据自己服务器路径的不同做修改</span></span><br><span class="line"><span class="comment">-- 注意：添加后执行sql还报错，执行：!q 退出重新连接，再执行 ADD JAR</span></span><br><span class="line"><span class="keyword">ADD</span> JAR <span class="operator">/</span>opt<span class="operator">/</span>cloudera<span class="operator">/</span>parcels<span class="operator">/</span>CDH<span class="operator">/</span>jars<span class="operator">/</span>hive<span class="operator">-</span>contrib<span class="number">-2.1</span><span class="number">.1</span><span class="operator">-</span>cdh6<span class="number">.2</span><span class="number">.0</span>.jar;</span><br></pre></td></tr></table></figure>

<p>（可选）将 TEXT 表转换为 PARQUET 表 </p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> pq_access_logs ( </span><br><span class="line">    ip STRING, </span><br><span class="line">    request_time STRING,</span><br><span class="line">    <span class="keyword">method</span> STRING, </span><br><span class="line">    url STRING,</span><br><span class="line">    http_version STRING, </span><br><span class="line">    code1 STRING, </span><br><span class="line">    code2 STRING, </span><br><span class="line">    dash STRING,</span><br><span class="line">    user_agent STRING, </span><br><span class="line">    `<span class="type">timestamp</span>` <span class="type">int</span>) </span><br><span class="line">STORED <span class="keyword">AS</span> PARQUET; </span><br><span class="line"></span><br><span class="line"><span class="keyword">INSERT</span> OVERWRITE <span class="keyword">TABLE</span> pq_access_logs </span><br><span class="line"><span class="keyword">SELECT</span> </span><br><span class="line">	ip, </span><br><span class="line">	from_unixtime(unix_timestamp(request_time, <span class="string">&#x27;dd/MMM/yyyy:HH:mm:ss z&#x27;</span>), <span class="string">&#x27;yyyy-MM-dd HH:mm:ss z&#x27;</span>), </span><br><span class="line">	<span class="keyword">method</span>, </span><br><span class="line">	url, </span><br><span class="line">	http_version, </span><br><span class="line">	code1, </span><br><span class="line">	code2, </span><br><span class="line">	dash, </span><br><span class="line">	user_agent, </span><br><span class="line">	unix_timestamp(request_time, <span class="string">&#x27;dd/MMM/yyyy:HH:mm:ss z&#x27;</span>) </span><br><span class="line"><span class="keyword">FROM</span> original_access_logs ;</span><br></pre></td></tr></table></figure>
</li>
<li><p>分区表</p>
<p>创建分区表</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">DROP</span> <span class="keyword">TABLE</span> IF <span class="keyword">EXISTS</span> partitioned_access_logs;</span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> partitioned_access_logs ( </span><br><span class="line">    ip STRING, </span><br><span class="line">    request_time STRING, </span><br><span class="line">    <span class="keyword">method</span> STRING, url STRING, </span><br><span class="line">    http_version STRING, </span><br><span class="line">    code1 STRING, </span><br><span class="line">    code2 STRING, </span><br><span class="line">    dash STRING, </span><br><span class="line">    user_agent STRING, </span><br><span class="line">    `<span class="type">timestamp</span>` <span class="type">int</span>) </span><br><span class="line">PARTITIONED <span class="keyword">BY</span> (request_date STRING) </span><br><span class="line">STORED <span class="keyword">AS</span> PARQUET</span><br><span class="line">;</span><br></pre></td></tr></table></figure>

<p>将日志表写⼊分区表，使⽤动态分区插⼊</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.exec.dynamic.partition.mode<span class="operator">=</span>nonstrict; </span><br><span class="line"></span><br><span class="line"><span class="keyword">INSERT</span> OVERWRITE <span class="keyword">TABLE</span> partitioned_access_logs </span><br><span class="line"><span class="keyword">PARTITION</span> (request_date) </span><br><span class="line"><span class="keyword">SELECT</span> ip, request_time, <span class="keyword">method</span>, url, http_version, code1, code2, dash, user_agent, `<span class="type">timestamp</span>`, to_date(request_time) a </span><br><span class="line"><span class="keyword">FROM</span> pq_access_logs</span><br><span class="line">;</span><br></pre></td></tr></table></figure>

<p>观察分区表目录结构</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfs <span class="operator">-</span>ls <span class="operator">/</span><span class="keyword">user</span><span class="operator">/</span>hive<span class="operator">/</span>warehouse<span class="operator">/</span>partitioned_access_logs</span><br></pre></td></tr></table></figure>
</li>
<li><p>使用 Sqoop 导入 Mysql 数据到 Hive</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"># 模板</span><br><span class="line">sqoop import <span class="comment">--connect jdbc:mysql://&lt;dbhost&gt;:3306/&lt;dbname&gt; \ </span></span><br><span class="line"><span class="comment">--fields-terminated-by \\t \ </span></span><br><span class="line"><span class="comment">--username &lt;dbuser&gt; --password &lt;dbpwd&gt; \ </span></span><br><span class="line"><span class="comment">--query &#x27;SELECT &lt;columns&gt; FROM &lt;table&gt; WHERE 1=1 AND $CONDITIONS&#x27; \ </span></span><br><span class="line"><span class="comment">--target-dir hdfs:///user/hive/warehouse/stg_&lt;table&gt;/ds=&lt;current_date&gt; \ </span></span><br><span class="line"><span class="comment">--verbose -m 1 </span></span><br><span class="line"></span><br><span class="line"># Example </span><br><span class="line">hdfs dfs <span class="operator">-</span>rm <span class="operator">-</span>r <span class="operator">/</span><span class="keyword">user</span><span class="operator">/</span>hive<span class="operator">/</span>warehouse<span class="operator">/</span>stg_drift_table<span class="operator">/</span>ds<span class="operator">=</span><span class="number">20210508</span> </span><br><span class="line"></span><br><span class="line">sqoop import <span class="comment">--connect jdbc:mysql://192.168.1.109:3306/dev \ </span></span><br><span class="line"><span class="comment">--fields-terminated-by \\t \ </span></span><br><span class="line"><span class="comment">--username dev --password dev \ </span></span><br><span class="line"><span class="comment">--query &#x27;SELECT id,c1,c2,c3,updated_at FROM drift_table WHERE 1=1 AND $CONDITIONS&#x27; \ </span></span><br><span class="line"><span class="comment">--target-dir hdfs:///user/hive/warehouse/stg_drift_table/ds=20210508 \ </span></span><br><span class="line"><span class="comment">--verbose -m 1 </span></span><br><span class="line"></span><br><span class="line">hdfs dfs <span class="operator">-</span>cat <span class="operator">/</span><span class="keyword">user</span><span class="operator">/</span>hive<span class="operator">/</span>warehouse<span class="operator">/</span>stg_drift_table<span class="operator">/</span>ds<span class="operator">=</span><span class="number">20210508</span><span class="operator">/</span>part<span class="operator">-</span>m<span class="number">-00000</span></span><br></pre></td></tr></table></figure>

<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 创建hive表 </span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">EXTERNAL</span> <span class="keyword">TABLE</span> stg_drift_table ( </span><br><span class="line">    id <span class="type">int</span>, </span><br><span class="line">    c1 string, </span><br><span class="line">    c2 <span class="type">int</span>, </span><br><span class="line">    c3 string, </span><br><span class="line">    updated_at string </span><br><span class="line">)</span><br><span class="line">PARTITIONED <span class="keyword">BY</span> (ds string) </span><br><span class="line"><span class="type">ROW</span> FORMAT DELIMITED </span><br><span class="line">FIELDS TERMINATED <span class="keyword">BY</span> <span class="string">&#x27;\t&#x27;</span> </span><br><span class="line">LOCATION <span class="string">&#x27;/user/hive/warehouse/stg_drift_table/&#x27;</span>; </span><br><span class="line"></span><br><span class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> stg_drift_table <span class="keyword">ADD</span> <span class="keyword">PARTITION</span>(ds<span class="operator">=</span><span class="string">&#x27;20210508&#x27;</span>);</span><br></pre></td></tr></table></figure></li>
</ol>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2022/01/01/13-%E5%A4%A7%E6%95%B0%E6%8D%AE/02-%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%A6%BB%E7%BA%BF%E5%A4%84%E7%90%86%E4%B8%8E%E6%95%B0%E4%BB%93/01-Hive/" data-id="clmcxec6z000vu8waai68agq2" data-title="HIVE" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/YARN/" rel="tag">YARN</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-13-大数据/03-Spark/01-Scala和Spark基础" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2022/01/01/13-%E5%A4%A7%E6%95%B0%E6%8D%AE/03-Spark/01-Scala%E5%92%8CSpark%E5%9F%BA%E7%A1%80/" class="article-date">
  <time class="dt-published" datetime="2022-01-01T04:00:00.000Z" itemprop="datePublished">2022-01-01</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/">大数据</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2022/01/01/13-%E5%A4%A7%E6%95%B0%E6%8D%AE/03-Spark/01-Scala%E5%92%8CSpark%E5%9F%BA%E7%A1%80/">Spark</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="Spark简介"><a href="#Spark简介" class="headerlink" title="Spark简介"></a>Spark简介</h1><h2 id="系统架构"><a href="#系统架构" class="headerlink" title="系统架构"></a>系统架构</h2><p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20220220135938.png"></p>
<h2 id="Spark简介-1"><a href="#Spark简介-1" class="headerlink" title="Spark简介"></a>Spark简介</h2><p>什么是Spark？<br>    Spark是基于<strong>内存计算</strong>的通用大规模数据处理框架</p>
<p>Spark已经融入了Hadoop生态系统，可支持的作业类型和应用场景比MapReduce更为广泛，并且具备了MapReduce所有的高容错性和高伸缩性特点。</p>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20220220141243.png"></p>
<h2 id="为何会诞生Spark？"><a href="#为何会诞生Spark？" class="headerlink" title="为何会诞生Spark？"></a>为何会诞生Spark？</h2><p>回顾MapReduce<br>并不是所有的问题都可以简单的分解成Map和Reduce两步模型处理</p>
<p>MapReduce缺点</p>
<ul>
<li>延时高<br>Example：不适合交互式SQL分析</li>
<li>迭代计算力不从心<br>Example：斐波那契数列</li>
<li>流式数据处理<br>Example：统计网站PV、UV数据</li>
</ul>
<p>Spark 一站式解决</p>
<ul>
<li>离线批处理</li>
<li>流式计算</li>
<li>在线实时分析</li>
</ul>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20220220141828.png"></p>
<h2 id="Spark为何快？"><a href="#Spark为何快？" class="headerlink" title="Spark为何快？"></a>Spark为何快？</h2><p>MapReduce会将中间结果输出到本地磁盘，例如Shuffle时Map输出的中间结果</p>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20220220142341.png"></p>
<p>有多个MapReduce任务串联时，依赖HDFS存储中间结果的输出，例如执行Hive查询</p>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20220220142424.png"></p>
<p>MapReduce在处理复杂DAG时会带来大量的数据copy、序列化和磁盘I&#x2F;O开销</p>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20220220142528.png"></p>
<ul>
<li>Spark尽可能减少中间结果写入磁盘</li>
<li>尽可能减少不必要的Sort&#x2F;Shuffle</li>
<li>反复用到的数据进行Cache</li>
<li>对于DAG进行高度优化<ul>
<li>划分不同的Stage</li>
<li>使用延迟计算技术</li>
</ul>
</li>
</ul>
<p>逻辑回归耗时</p>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20220220142628.png"></p>
<h2 id="Spark特点"><a href="#Spark特点" class="headerlink" title="Spark特点"></a>Spark特点</h2><ul>
<li><p>内存计算</p>
<ul>
<li>支持复杂查询、流式计算、机器学习、图计算</li>
</ul>
</li>
<li><p>融入Hadoop生态圈</p>
<ul>
<li>兼容HDFS</li>
<li>兼容Yarn</li>
</ul>
</li>
<li><p>核心代码由Scala编写</p>
</li>
<li><p>发展速度快</p>
<ul>
<li>社区活跃</li>
<li>最新版本3.1.2 (截止2021年06月01日)</li>
</ul>
</li>
</ul>
<h2 id="Spark多语言支持"><a href="#Spark多语言支持" class="headerlink" title="Spark多语言支持"></a>Spark多语言支持</h2><p>Python和Scala支持交互式分析</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">lines = sc.textFile(…)</span><br><span class="line">lines.<span class="built_in">filter</span>(<span class="keyword">lambda</span> s: <span class="string">&quot;Error&quot;</span> <span class="keyword">in</span> s).count()</span><br></pre></td></tr></table></figure>

<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> lines = sc.textFile(…)</span><br><span class="line">lines.filter(x =&gt; x.contains(<span class="string">&quot;Error&quot;</span>)).count()</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">JavaRDD&lt;String&gt; lines = sc.textFile(…);</span><br><span class="line">lines.filter(<span class="keyword">new</span> <span class="title class_">Function</span>&lt;String, Boolean&gt;)() &#123;</span><br><span class="line">    Boolean <span class="title function_">call</span><span class="params">(String s)</span> &#123;</span><br><span class="line">    <span class="keyword">return</span> s.contains(<span class="string">&quot;Error&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;).count()</span><br></pre></td></tr></table></figure>

<h1 id="Scala编程基础"><a href="#Scala编程基础" class="headerlink" title="Scala编程基础"></a>Scala编程基础</h1><p><strong>版本：scala-2.11.12</strong></p>
<h2 id="Scala是一门怎样的语言，具有哪些优点？"><a href="#Scala是一门怎样的语言，具有哪些优点？" class="headerlink" title="Scala是一门怎样的语言，具有哪些优点？"></a>Scala是一门怎样的语言，具有哪些优点？</h2><ul>
<li>快速实验<ul>
<li>快速尝试各种语法和代码</li>
</ul>
</li>
<li>一致性<ul>
<li>静态类型系统+面向对象+函数式编程</li>
</ul>
</li>
<li>面向对象<ul>
<li>所有的变量和方法都封装在对象中</li>
</ul>
</li>
<li>函数式编程<ul>
<li>函数可以独立存在，可以定义一个函数作为另外一个函数的返回值，也可以接受函数作为函数的参数</li>
</ul>
</li>
<li>异步编程<ul>
<li>函数式编程提倡变量不可变，使得异步编程变得十分容易</li>
</ul>
</li>
<li>基于JVM<ul>
<li>Scala会被编译成为Bytecode，所以Scala能无缝集成已有的Java类库</li>
</ul>
</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">products</span> </span>= orders.flatMap(o =&gt; o.products)</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> List&lt;Product&gt; <span class="title function_">getProducts</span><span class="params">()</span> &#123;</span><br><span class="line">    List&lt;Product&gt; products = <span class="keyword">new</span> </span><br><span class="line">    <span class="title class_">ArrayList</span>&lt;Product&gt;();</span><br><span class="line">        <span class="keyword">for</span> (Order order : orders) &#123;</span><br><span class="line">            products.addAll(order.getProducts());</span><br><span class="line">        &#125;</span><br><span class="line">    <span class="keyword">return</span> products;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="Scala语法基础-从Hello-World说起"><a href="#Scala语法基础-从Hello-World说起" class="headerlink" title="Scala语法基础-从Hello World说起"></a>Scala语法基础-从Hello World说起</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">HelloWorld</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">    	System.out.println(<span class="string">&quot;Hello, World!&quot;</span>);</span><br><span class="line">    &#125; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>object 是一个关键词，表示一个单例对象</p>
<p>方法返回为空用 Unit 表示</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">HelloWorld</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        println(<span class="string">&quot;Hello, World!&quot;</span>) </span><br><span class="line">    &#125; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="Scala值和变量声明"><a href="#Scala值和变量声明" class="headerlink" title="Scala值和变量声明"></a>Scala值和变量声明</h2><ul>
<li>val变量和var变量<ul>
<li>val声明的变量不可变，相当于java中的final<ul>
<li>val a &#x3D; 1</li>
<li>a &#x3D; 2 &#x2F;&#x2F; 错误</li>
</ul>
</li>
<li>var声明的变量可变<ul>
<li>var a &#x3D; 1</li>
<li>a &#x3D; 2 &#x2F;&#x2F; OK</li>
</ul>
</li>
</ul>
</li>
<li>在scala的类中，val会自动带有getter方法，var会自动带有getter和setter方法</li>
</ul>
<p>举个例子：</p>
<p>在 VariableTest.scala 文件定义了俩个变量</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">VariableTest</span> </span>&#123; </span><br><span class="line">	<span class="keyword">val</span> a = <span class="number">1</span></span><br><span class="line">	<span class="keyword">var</span> b = <span class="number">2</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>使用 scalac VariableTest.scala 命令编译为 VariableTest.class 文件，再使用 jad VariableTest 反编译 class 文件结果如下</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">VariableTest</span> &#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="type">int</span> <span class="variable">a</span> <span class="operator">=</span> <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">private</span> <span class="type">int</span> b;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="type">int</span> <span class="title function_">a</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> a;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="type">int</span> <span class="title function_">b</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> b;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> b_$eq(<span class="type">int</span> x$<span class="number">1</span>) &#123;</span><br><span class="line">        b = x$<span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="title function_">VariableTest</span><span class="params">()</span> &#123;</span><br><span class="line">        b = <span class="number">2</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="Scala常用类型"><a href="#Scala常用类型" class="headerlink" title="Scala常用类型"></a>Scala常用类型</h2><p>Scala没有区分基本类型和包装类型，统一定义为class类。 </p>
<ul>
<li>1.toString() &#x2F;&#x2F; 生成字符串1</li>
</ul>
<p>7种数值类型+1种Boolean类型</p>
<ul>
<li>Byte -&gt; RichByte</li>
<li>Char -&gt; RichChar</li>
<li>Short -&gt; RichShort</li>
<li>Int -&gt; RichInt</li>
<li>Long -&gt; RichLong</li>
<li>Float -&gt; RichFloat</li>
<li>Double -&gt; RichDouble</li>
</ul>
<p>在基本数据类型上使用那些没有提供的方法时，scala会尝试“隐式转换”转换成增强类型</p>
<ul>
<li>1.to(10) &#x2F;&#x2F; 生成出Range(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)</li>
</ul>
<h2 id="方法的定义和使用"><a href="#方法的定义和使用" class="headerlink" title="方法的定义和使用"></a>方法的定义和使用</h2><p>方法定义</p>
<ul>
<li><p>格式<br><strong>def</strong> 方法名(参数名: 参数类型): 返回值类型 &#x3D; {<br>return xxx &#x2F;&#x2F; return可省略<br>}</p>
</li>
<li><p>当返回值为 unit 时</p>
<p><strong>def</strong> 方法名(参数名: 参数类型) {</p>
<p>&#x2F;&#x2F; 方法体</p>
<p>}</p>
</li>
<li><p>无参数函数定义</p>
<p><strong>def</strong> 方法名 {</p>
<p>&#x2F;&#x2F; 方法体</p>
<p>}</p>
</li>
</ul>
<p>例子</p>
<p><strong>def</strong> m1(a: Int, b: Int): Int &#x3D; {</p>
<p>a + b</p>
<p>}</p>
<p><strong>def</strong> m2() &#x3D; 100</p>
<p><strong>def</strong> m3 &#x3D; 100</p>
<h2 id="函数的定义和使用"><a href="#函数的定义和使用" class="headerlink" title="函数的定义和使用"></a>函数的定义和使用</h2><p>函数定义</p>
<ul>
<li><p>函数在scala中是一等公民</p>
</li>
<li><p><strong>val</strong> 函数名:(参数类型1, … , 参数类型n)&#x3D;&gt;返回值类型 &#x3D; (T1,…, Tn) &#x3D;&gt; 函数体</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> f0 : (<span class="type">Int</span>) =&gt; <span class="type">Boolean</span> = i =&gt; &#123; i % <span class="number">2</span> == <span class="number">0</span> &#125;</span><br><span class="line"><span class="keyword">val</span> f1 : <span class="type">Int</span> =&gt; <span class="type">Boolean</span> = i =&gt; &#123; i % <span class="number">2</span> == <span class="number">0</span> &#125;</span><br><span class="line"><span class="keyword">val</span> f2 : <span class="type">Int</span> =&gt; <span class="type">Boolean</span> = i =&gt; i % <span class="number">2</span> == <span class="number">0</span></span><br><span class="line"><span class="keyword">val</span> f3 : <span class="type">Int</span> =&gt; <span class="type">Boolean</span> = _ % <span class="number">2</span> == <span class="number">0</span></span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>val</strong> 函数名 &#x3D; (参数名1: 参数类型1, … , 参数名n: 参数类型n) &#x3D;&gt;函数体</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// implicit approach</span></span><br><span class="line"><span class="keyword">val</span> add0 = (x: <span class="type">Int</span>, y: <span class="type">Int</span>) =&gt; &#123; x + y &#125;</span><br><span class="line"><span class="keyword">val</span> add1 = (x: <span class="type">Int</span>, y: <span class="type">Int</span>) =&gt; x + y</span><br><span class="line"><span class="comment">// explicit approach</span></span><br><span class="line"><span class="keyword">val</span> add2 : (<span class="type">Int</span>, <span class="type">Int</span>) =&gt; <span class="type">Int</span> = (x,y) =&gt; &#123; x + y &#125;</span><br><span class="line"><span class="keyword">val</span> add3 : (<span class="type">Int</span>, <span class="type">Int</span>) =&gt; <span class="type">Int</span> = (x,y) =&gt; x + y</span><br></pre></td></tr></table></figure>
</li>
<li><p>函数必须有参数列表，否则报错</p>
<ul>
<li><strong>val</strong> f1 &#x3D; &#x3D;&gt; 100 &#x2F;&#x2F; 错误</li>
<li><strong>val</strong> f2 &#x3D; () &#x3D;&gt; 100 &#x2F;&#x2F; 正确</li>
</ul>
</li>
</ul>
<h2 id="函数与方法区别"><a href="#函数与方法区别" class="headerlink" title="函数与方法区别"></a>函数与方法区别</h2><p>方法不可以赋值给变量但是函数可以</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">m1</span></span>(a: <span class="type">Int</span>, b: <span class="type">Int</span>): <span class="type">Int</span> = &#123;</span><br><span class="line">a + b</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">val</span> m = m1 <span class="comment">// 错误：方法不可以赋值给一个变量</span></span><br><span class="line"><span class="keyword">val</span> m = m1 _ <span class="comment">// 正确：方法自动转化为函数</span></span><br><span class="line">m: (<span class="type">Int</span>, <span class="type">Int</span>) =&gt; <span class="type">Int</span> = &lt;function2&gt;</span><br></pre></td></tr></table></figure>

<p>对于一个无参数的方法是没有参数列表的，而对于函数是有一个空参数列表。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">x</span> </span>= println(<span class="string">&quot;Hi scala&quot;</span>)</span><br><span class="line">x: <span class="type">Unit</span> <span class="comment">// 没有参数列表</span></span><br><span class="line"><span class="keyword">val</span> y = x _</span><br><span class="line">y: () =&gt; <span class="type">Unit</span> = &lt;function0&gt; <span class="comment">// 空的参数列表()</span></span><br></pre></td></tr></table></figure>

<p>函数名后必须加括号才代表函数调用，否则为该函数本身，而方法名后不加括号为方法调用</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">y <span class="comment">// 仅代表函数本身</span></span><br><span class="line">res0: () =&gt; <span class="type">Unit</span> = &lt;function0&gt;</span><br><span class="line">y() <span class="comment">// 函数调用</span></span><br></pre></td></tr></table></figure>

<h2 id="Example-方法计时器"><a href="#Example-方法计时器" class="headerlink" title="Example - 方法计时器"></a>Example - 方法计时器</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">TimeTest</span> </span>&#123;</span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">time</span></span>[<span class="type">R</span>](block: =&gt; <span class="type">R</span>): <span class="type">R</span> = &#123; <span class="comment">// 这里使用了call-by-name参数</span></span><br><span class="line">    <span class="keyword">val</span> t0 = <span class="type">System</span>.currentTimeMillis()</span><br><span class="line">    <span class="keyword">val</span> result = block</span><br><span class="line">    <span class="keyword">val</span> t1 = <span class="type">System</span>.currentTimeMillis()</span><br><span class="line">    println(<span class="string">&quot;Elapsed time = &quot;</span> + (t1 - t0) + <span class="string">&quot;ms&quot;</span>)</span><br><span class="line">    result</span><br><span class="line">	&#125;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">method</span></span>(x: <span class="type">Int</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">        println(<span class="string">&quot;print test = &quot;</span> + x)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        time(method(<span class="number">100</span>))</span><br><span class="line">    &#125; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="循环和高级for循环"><a href="#循环和高级for循环" class="headerlink" title="循环和高级for循环"></a>循环和高级for循环</h2><p>拥有与Java相同的while和do循环</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">while</span> (x &gt; <span class="number">0</span>) &#123;</span><br><span class="line"><span class="comment">// do while</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>for循环与Java不同</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> (x &lt;- 表达式) &#123;</span><br><span class="line">	<span class="comment">// do for</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">for</span> (x &lt;- <span class="number">1</span> to n) &#123;</span><br><span class="line">	<span class="comment">// do for</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">for</span> (x &lt;- <span class="string">&quot;Hello, World!&quot;</span>) &#123;</span><br><span class="line">	<span class="comment">// do for</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">val</span> s = <span class="string">&quot;Hello, World!&quot;</span></span><br><span class="line"><span class="keyword">for</span> (x &lt;- <span class="number">0</span> until s.length) &#123; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>for循环中不支持continue和break</p>
<p>支持守卫</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span>(i &lt;- <span class="number">1</span> until <span class="number">10</span> reverse; <span class="keyword">if</span> i % <span class="number">2</span> == <span class="number">0</span>) &#123;</span><br><span class="line">	println(i) </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>支持多个生成器</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span>(i &lt;- <span class="number">1</span> to <span class="number">3</span>; j &lt;- <span class="number">1</span> to <span class="number">3</span>) &#123; </span><br><span class="line">	print((<span class="number">10</span> * i + j) + <span class="string">&quot; &quot;</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>支持守卫yield关键词</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> seq = <span class="keyword">for</span> (i &lt;- <span class="number">1</span> to <span class="number">10</span> <span class="keyword">if</span> i % <span class="number">2</span> == <span class="number">0</span>) <span class="keyword">yield</span> i</span><br><span class="line">print(seq)</span><br></pre></td></tr></table></figure>

<h2 id="常见集合使用"><a href="#常见集合使用" class="headerlink" title="常见集合使用"></a>常见集合使用</h2><p>Scala有三大类集合</p>
<ul>
<li><p>序列Seq</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> v1 = <span class="type">Vector</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line"><span class="keyword">val</span> list1 = <span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line"><span class="keyword">val</span> que1 = <span class="type">Queue</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>)</span><br></pre></td></tr></table></figure>
</li>
<li><p>集 Set</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">Set</span>(<span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>)</span><br></pre></td></tr></table></figure>
</li>
<li><p>映射 Map</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">Map</span>(<span class="number">1</span> -&gt; <span class="number">2</span>)</span><br><span class="line"><span class="type">Map</span>(<span class="string">&quot;foo&quot;</span> -&gt; <span class="string">&quot;bar&quot;</span>)</span><br><span class="line"><span class="type">Map</span>(<span class="number">1</span> -&gt; <span class="string">&quot;one&quot;</span>, <span class="number">2</span> -&gt; <span class="string">&quot;two&quot;</span>)</span><br></pre></td></tr></table></figure>
</li>
<li><p>几乎对所有集合都提供了可变和不可变的版本</p>
<ul>
<li>Scala.collection.immutable.Map &#x2F;&#x2F; 不可变集合</li>
<li>Scala.collection.mutable.Map &#x2F;&#x2F; 可变集合</li>
</ul>
</li>
<li><p>可以对集合进行丰富的操作</p>
<ul>
<li>map, flatMap, reduce, distinct, zip</li>
</ul>
</li>
</ul>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20220220211756.png"></p>
<ul>
<li>C 操作时间为常数</li>
<li>eC 操作时间在满足某些假设的前提下为常数</li>
<li>aC该操作的均摊运行时间为常数。某些调用的可能耗时较长，但多次调用之下，每次调用的平均耗时是常数。</li>
<li>L 操作是线性的，耗时与容器的大小成正比。</li>
</ul>
<table>
<thead>
<tr>
<th>时间复杂度</th>
<th>索引</th>
<th>更新</th>
<th>头部追加</th>
<th>尾部追加</th>
<th>插入</th>
</tr>
</thead>
<tbody><tr>
<td>Immutable</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>List</td>
<td>L</td>
<td>L</td>
<td>C</td>
<td>L</td>
<td>-</td>
</tr>
<tr>
<td>Stream</td>
<td>L</td>
<td>L</td>
<td>C</td>
<td>L</td>
<td>-</td>
</tr>
<tr>
<td>Vector</td>
<td>eC</td>
<td>eC</td>
<td>eC</td>
<td>eC</td>
<td>-</td>
</tr>
<tr>
<td>Stack</td>
<td>L</td>
<td>L</td>
<td>C</td>
<td>L</td>
<td>L</td>
</tr>
<tr>
<td>Queue</td>
<td>L</td>
<td>L</td>
<td>C</td>
<td>C</td>
<td>-</td>
</tr>
<tr>
<td>Range</td>
<td>C</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>String</td>
<td>C</td>
<td>L</td>
<td>L</td>
<td>L</td>
<td>-</td>
</tr>
<tr>
<td>mutable</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>ArrayBuffer</td>
<td>C</td>
<td>C</td>
<td>L</td>
<td>aC</td>
<td>L</td>
</tr>
<tr>
<td>ListBuffer</td>
<td>L</td>
<td>L</td>
<td>C</td>
<td>C</td>
<td>L</td>
</tr>
<tr>
<td>StringBuilder</td>
<td>C</td>
<td>C</td>
<td>L</td>
<td>aC</td>
<td>L</td>
</tr>
<tr>
<td>MutableList</td>
<td>L</td>
<td>L</td>
<td>C</td>
<td>C</td>
<td>L</td>
</tr>
<tr>
<td>Queue</td>
<td>L</td>
<td>L</td>
<td>C</td>
<td>C</td>
<td>L</td>
</tr>
<tr>
<td>ArraySeq</td>
<td>C</td>
<td>C</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>Stack</td>
<td>L</td>
<td>L</td>
<td>C</td>
<td>L</td>
<td>L</td>
</tr>
<tr>
<td>ArrayStack</td>
<td>C</td>
<td>C</td>
<td>aC</td>
<td>L</td>
<td>L</td>
</tr>
<tr>
<td>Array</td>
<td>C</td>
<td>C</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
</tbody></table>
<h2 id="常见集合操作"><a href="#常见集合操作" class="headerlink" title="常见集合操作"></a>常见集合操作</h2><p>map</p>
<ul>
<li><p>在列表中的每个元素上计算一个函数，并且返回一个包含相同数目元素的列表。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> nums = <span class="type">List</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line"><span class="keyword">val</span> squareNums1 = nums.map(num =&gt; num * num)</span><br></pre></td></tr></table></figure></li>
</ul>
<p>flatMap</p>
<ul>
<li><p>结合了map和flatten的功能。接收一个可以处理嵌套列表的函数，把返回结果连接起来。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> nestedNumbers = <span class="type">List</span>(<span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>), <span class="type">List</span>(<span class="number">3</span>, <span class="number">4</span>))</span><br><span class="line">nestedNumbers.flatMap(x =&gt; x.map(<span class="type">Math</span>.pow(_, <span class="number">2</span>)))</span><br></pre></td></tr></table></figure></li>
</ul>
<p>filter</p>
<ul>
<li><p>移除任何使得传入的函数返回false的元素。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> nums = <span class="type">List</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line">nums.filter((i: <span class="type">Int</span>) =&gt; i % <span class="number">2</span> == <span class="number">0</span>)</span><br></pre></td></tr></table></figure></li>
</ul>
<p>zip</p>
<ul>
<li><p>两个列表的元素合成一个由元组对组成的列表里</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>).zip(<span class="type">List</span>(<span class="string">&quot;a&quot;</span>, <span class="string">&quot;b&quot;</span>, <span class="string">&quot;c&quot;</span>))</span><br></pre></td></tr></table></figure></li>
</ul>
<p>partition</p>
<ul>
<li><p>断言函数的返回值对列表进行拆分</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">10</span>).partition(_ % <span class="number">2</span> == <span class="number">0</span>)</span><br></pre></td></tr></table></figure></li>
</ul>
<h2 id="异常处理"><a href="#异常处理" class="headerlink" title="异常处理"></a>异常处理</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">	methodThatThrowsIOException();</span><br><span class="line">&#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">	<span class="comment">// do IOException</span></span><br><span class="line">&#125; <span class="keyword">catch</span> (Throwable e) &#123;</span><br><span class="line">	<span class="comment">// do Throwable</span></span><br><span class="line">&#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">	<span class="comment">// do finally</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">try</span> &#123; </span><br><span class="line">	methodThatMayThrowAnException()</span><br><span class="line">&#125; <span class="keyword">catch</span> &#123; </span><br><span class="line">	<span class="keyword">case</span> e: <span class="type">IOException</span> =&gt; <span class="comment">// do IOException</span></span><br><span class="line">	<span class="keyword">case</span> _: <span class="type">Throwable</span> =&gt; <span class="comment">// do Throwable</span></span><br><span class="line">&#125; <span class="keyword">finally</span> &#123; </span><br><span class="line">	<span class="comment">// do finally</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>如果在.map, .flatMap中遇到异常如何处理？</p>
<p>Scala提供了scala.util.Try 类型更加优雅的处理异常</p>
<ul>
<li>如果成功返回Success</li>
<li>如果抛出异常返回Failure并携带异常信息</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scala.util.&#123;<span class="type">Try</span>, <span class="type">Success</span>, <span class="type">Failure</span>&#125;</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">divideBy</span></span>(x: <span class="type">Int</span>, y: <span class="type">Int</span>): <span class="type">Try</span>[<span class="type">Int</span>] = &#123; </span><br><span class="line">	<span class="type">Try</span>(x / y)</span><br><span class="line">&#125;</span><br><span class="line">divideBy(<span class="number">1</span>, <span class="number">0</span>) <span class="keyword">match</span> &#123; </span><br><span class="line">	<span class="keyword">case</span> <span class="type">Success</span>(i) =&gt; println(<span class="string">s&quot;Success, value is: <span class="subst">$i</span>&quot;</span>) </span><br><span class="line">	<span class="keyword">case</span> <span class="type">Failure</span>(s) =&gt; println(<span class="string">s&quot;Failed, message is: <span class="subst">$s</span>&quot;</span>) </span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// Failed, message is: java.lang.ArithmeticException: / by zero</span></span><br><span class="line">println(divideBy(<span class="number">1</span>, <span class="number">1</span>).getOrElse(<span class="number">0</span>)) <span class="comment">// 1</span></span><br><span class="line">println(divideBy(<span class="number">1</span>, <span class="number">0</span>).getOrElse(<span class="number">0</span>)) <span class="comment">// 0</span></span><br><span class="line"></span><br><span class="line"><span class="number">50.</span>to(<span class="number">55</span>).zip(<span class="number">0.</span>to(<span class="number">5</span>)).map(pair =&gt; divideBy(pair._1, pair._2).getOrElse(<span class="number">0</span>))</span><br><span class="line"><span class="comment">// scala.collection.immutable.IndexedSeq[Int] = Vector(0, 51, 26, 17, 13, 11)</span></span><br></pre></td></tr></table></figure>

<h2 id="类定义"><a href="#类定义" class="headerlink" title="类定义"></a>类定义</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Person</span>(<span class="params">name: <span class="type">String</span>, age: <span class="type">Int</span></span>) </span>&#123;</span><br><span class="line">    <span class="keyword">var</span> work: <span class="type">String</span> = _</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">val</span> defaultSalary: <span class="type">Int</span> = <span class="number">10000</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">getPersonInfo</span></span>(): <span class="type">String</span> = &#123;</span><br><span class="line">        <span class="string">&quot;Name = [&quot;</span> + name + <span class="string">&quot;], Age = [&quot;</span> + age + <span class="string">&quot;] &quot;</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li>scala类中方法默认都是public</li>
<li>对var字段会生成getter和setter方法</li>
<li>对val字段只会生成getter方法</li>
<li>主构造器中的参数必须要被初始化</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">Person</span> &#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> String name;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="type">int</span> age;</span><br><span class="line">    <span class="keyword">private</span> String work;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="type">int</span> <span class="variable">defaultSalary</span> <span class="operator">=</span> <span class="number">10000</span>;</span><br><span class="line">    <span class="keyword">public</span> <span class="title function_">Person</span><span class="params">(String name, <span class="type">int</span> age)</span>&#123;</span><br><span class="line">        <span class="built_in">super</span>();</span><br><span class="line">        <span class="built_in">this</span>.name = name;</span><br><span class="line">        <span class="built_in">this</span>.age = age;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">public</span> String <span class="title function_">getPersonInfo</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> (<span class="keyword">new</span> <span class="title class_">StringBuilder</span>())</span><br><span class="line">            .append(<span class="string">&quot;Name = [&quot;</span>)</span><br><span class="line">            .append(name).append(<span class="string">&quot;], Age = [&quot;</span>)</span><br><span class="line">            .append(BoxesRunTime.boxToInteger(age))</span><br><span class="line">            .append(<span class="string">&quot;]&quot;</span>).toString();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">public</span> String <span class="title function_">work</span><span class="params">()</span> &#123;</span><br><span class="line">    	<span class="keyword">return</span> work;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> work_$eq(String x$<span class="number">1</span>) &#123;</span><br><span class="line">    	work = x$<span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">private</span> <span class="type">int</span> <span class="title function_">defaultSalary</span><span class="params">()</span> &#123;</span><br><span class="line">    	<span class="keyword">return</span> defaultSalary; </span><br><span class="line">    &#125; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="类构造函数"><a href="#类构造函数" class="headerlink" title="类构造函数"></a>类构造函数</h2><ul>
<li>辅助构造器使用def this定义</li>
<li>每一个辅助构造器第一行必须调用其它辅助构造器或者一个主构造器</li>
<li>println语句是主构造器的一部分</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Person</span>(<span class="params">name: <span class="type">String</span>, age: <span class="type">Int</span></span>) </span>&#123;</span><br><span class="line">	println(<span class="string">&quot;Name = [&quot;</span> + name + <span class="string">&quot;], Age = [&quot;</span> + age 	+ <span class="string">&quot;] &quot;</span>)</span><br><span class="line">	<span class="keyword">var</span> work: <span class="type">String</span> = _</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">this</span></span>(name: <span class="type">String</span>, age: <span class="type">Int</span>, work: <span class="type">String</span>) &#123;</span><br><span class="line">    	<span class="keyword">this</span>(name, age)</span><br><span class="line">    	<span class="keyword">this</span>.work = work</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">getPersonInfo</span></span>(): <span class="type">String</span> = &#123;</span><br><span class="line">    	<span class="string">&quot;Name = [&quot;</span> + name + <span class="string">&quot;], Age = [&quot;</span> + age + <span class="string">&quot;] &quot;</span></span><br><span class="line">    &#125; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<table>
<thead>
<tr>
<th>主构造器参数</th>
<th>生成的字段&#x2F;方法</th>
</tr>
</thead>
<tbody><tr>
<td>name: String</td>
<td>对象私有字段</td>
</tr>
<tr>
<td>val&#x2F;var name: String</td>
<td>私有字段，公有的getter&#x2F;setter方法</td>
</tr>
<tr>
<td>private val&#x2F;var name: String</td>
<td>私有字段，私有的getter&#x2F;setter方法</td>
</tr>
<tr>
<td>@BeanProperty val&#x2F;var name: String</td>
<td>私有字段，公有Scala版和JavaBeans版的getter和setter方法</td>
</tr>
</tbody></table>
<h2 id="类继承"><a href="#类继承" class="headerlink" title="类继承"></a>类继承</h2><ul>
<li>Scala继承类和java一样使用extends关键字</li>
<li>可以将类、字段或者方法声明为final，确保它们不能被重写</li>
<li>重写一个非抽象方法必须使用override关键词</li>
<li>可以将类定义为abstract作为抽象类，子类中重写超类的抽象方法时不需要使用override关键词</li>
<li>调用超类与Java一致使用super关键词</li>
<li>只有主构造器才能调用超类的构造器</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Student</span>(<span class="params">name: <span class="type">String</span>, age: <span class="type">Int</span></span>) <span class="keyword">extends</span> <span class="title">Person</span>(<span class="params">name, age</span>) </span>&#123;</span><br><span class="line">	<span class="keyword">var</span> score : <span class="type">Int</span> = _</span><br><span class="line">	<span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getPersonInfo</span></span>: <span class="type">String</span> = &#123;</span><br><span class="line">		<span class="string">s&quot;Name = [<span class="subst">$&#123;name&#125;</span>], Age = [<span class="subst">$&#123;age&#125;</span>], Score = [<span class="subst">$&#123;score&#125;</span>]“</span></span><br><span class="line"><span class="string">	&#125; </span></span><br><span class="line"><span class="string">&#125;</span></span><br></pre></td></tr></table></figure>

<h2 id="单例对象"><a href="#单例对象" class="headerlink" title="单例对象"></a>单例对象</h2><p>Scala中没有静态方法和静态字段，但是可以使用object达到同样的效果</p>
<p>Account是伴生对象所有的字段和方法都是静态的</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Account</span> </span>&#123;</span><br><span class="line">    <span class="keyword">val</span> id = <span class="type">Account</span>.newUniqueId()</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">var</span> balance = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">deposit</span></span>(amount: <span class="type">Double</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    	balance += amount</span><br><span class="line">    &#125; </span><br><span class="line">&#125;</span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Account</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">var</span> lastId = <span class="number">0</span></span><br><span class="line">    <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">newUniqueId</span></span>() = &#123;</span><br><span class="line">        lastId += <span class="number">1</span></span><br><span class="line">        lastId</span><br><span class="line">    &#125; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="函数式编程思想"><a href="#函数式编程思想" class="headerlink" title="函数式编程思想"></a>函数式编程思想</h2><p>函数式编程关心的是<strong>数据的映射</strong>而命令式编程关心的是解决问题的步骤</p>
<p>函数式编程提倡</p>
<ul>
<li>没有可变的变量<ul>
<li>例如无论sqrt(x)，这个函数的值只取决于函数的输入的值</li>
</ul>
</li>
<li>没有类似于命令式编程中循环元素</li>
</ul>
<p>好处</p>
<ul>
<li>不依赖于外部的状态，也不修改外部的状态，使得代码容易推理，单元测试和调试变得十分容易</li>
<li>由于多个线程之前不共享状态，因此不会造成资源的竞争，可以更好的支持并发</li>
</ul>
<h2 id="函数式编程思想Example"><a href="#函数式编程思想Example" class="headerlink" title="函数式编程思想Example"></a>函数式编程思想Example</h2><h3 id="命令式编程"><a href="#命令式编程" class="headerlink" title="命令式编程"></a>命令式编程</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Node</span>(<span class="params">var value: <span class="type">Int</span>, var left: <span class="type">Option</span>[<span class="type">Node</span>], var right: <span class="type">Option</span>[<span class="type">Node</span>]</span>)</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Node</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">invertTree</span></span>(root: <span class="type">Option</span>[<span class="type">Node</span>]): <span class="type">Option</span>[<span class="type">Node</span>] = &#123;</span><br><span class="line">        <span class="keyword">if</span> (root isDefined) &#123;</span><br><span class="line">            <span class="keyword">val</span> left = root.get.left</span><br><span class="line">            <span class="keyword">val</span> right = root.get.right</span><br><span class="line">            root.get.left = invertTree(right)</span><br><span class="line">            root.get.right = invertTree(left)</span><br><span class="line">        &#125;</span><br><span class="line">        root</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> node4 = <span class="keyword">new</span> <span class="type">Node</span>(<span class="number">4</span>, <span class="type">None</span>, <span class="type">None</span>)</span><br><span class="line">        <span class="keyword">val</span> node5 = <span class="keyword">new</span> <span class="type">Node</span>(<span class="number">5</span>, <span class="type">None</span>, <span class="type">None</span>)</span><br><span class="line">        <span class="keyword">val</span> node6 = <span class="keyword">new</span> <span class="type">Node</span>(<span class="number">6</span>, <span class="type">None</span>, <span class="type">None</span>)</span><br><span class="line">        <span class="keyword">val</span> node7 = <span class="keyword">new</span> <span class="type">Node</span>(<span class="number">7</span>, <span class="type">None</span>, <span class="type">None</span>)</span><br><span class="line">        <span class="keyword">val</span> node2 = <span class="keyword">new</span> <span class="type">Node</span>(<span class="number">2</span>, <span class="type">Some</span>(node4), <span class="type">Some</span>(node5))</span><br><span class="line">        <span class="keyword">val</span> node3 = <span class="keyword">new</span> <span class="type">Node</span>(<span class="number">3</span>, <span class="type">Some</span>(node6), <span class="type">Some</span>(node7))</span><br><span class="line">        <span class="keyword">val</span> node1 = <span class="keyword">new</span> <span class="type">Node</span>(<span class="number">1</span>, <span class="type">Some</span>(node2), <span class="type">Some</span>(node3))</span><br><span class="line">        invertTree(<span class="type">Some</span>(node1))</span><br><span class="line">        println(node1)</span><br><span class="line">    &#125; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="函数式编程"><a href="#函数式编程" class="headerlink" title="函数式编程"></a>函数式编程</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Node</span>(<span class="params">val value: <span class="type">Int</span>, val left: <span class="type">Option</span>[<span class="type">Node</span>], val right: <span class="type">Option</span>[<span class="type">Node</span>]</span>)</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Node</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">invertTree</span></span>(root: <span class="type">Option</span>[<span class="type">Node</span>]): <span class="type">Option</span>[<span class="type">Node</span>] = &#123;</span><br><span class="line">        <span class="keyword">if</span> (root isDefined) &#123;</span><br><span class="line">        	<span class="type">Some</span>(<span class="keyword">new</span> <span class="type">Node</span>(root.get.value, </span><br><span class="line">        	invertTree(root.get.right), invertTree(root.get.left)))</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        	<span class="type">None</span></span><br><span class="line">        &#125; </span><br><span class="line">	&#125;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> node4 = <span class="keyword">new</span> <span class="type">Node</span>(<span class="number">4</span>, <span class="type">None</span>, <span class="type">None</span>)</span><br><span class="line">        <span class="keyword">val</span> node5 = <span class="keyword">new</span> <span class="type">Node</span>(<span class="number">5</span>, <span class="type">None</span>, <span class="type">None</span>)</span><br><span class="line">        <span class="keyword">val</span> node6 = <span class="keyword">new</span> <span class="type">Node</span>(<span class="number">6</span>, <span class="type">None</span>, <span class="type">None</span>)</span><br><span class="line">        <span class="keyword">val</span> node7 = <span class="keyword">new</span> <span class="type">Node</span>(<span class="number">7</span>, <span class="type">None</span>, <span class="type">None</span>)</span><br><span class="line">        <span class="keyword">val</span> node2 = <span class="keyword">new</span> <span class="type">Node</span>(<span class="number">2</span>, <span class="type">Some</span>(node4), <span class="type">Some</span>(node5))</span><br><span class="line">        <span class="keyword">val</span> node3 = <span class="keyword">new</span> <span class="type">Node</span>(<span class="number">3</span>, <span class="type">Some</span>(node6), <span class="type">Some</span>(node7))</span><br><span class="line">        <span class="keyword">val</span> node1 = <span class="keyword">new</span> <span class="type">Node</span>(<span class="number">1</span>, <span class="type">Some</span>(node2), <span class="type">Some</span>(node3))</span><br><span class="line">        <span class="keyword">val</span> newTree = invertTree(<span class="type">Some</span>(node1)).get</span><br><span class="line">    &#125; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h1 id="Spark体系结构和源代码解析"><a href="#Spark体系结构和源代码解析" class="headerlink" title="Spark体系结构和源代码解析"></a>Spark体系结构和源代码解析</h1><h2 id="弹性分布式数据集RDD"><a href="#弹性分布式数据集RDD" class="headerlink" title="弹性分布式数据集RDD"></a>弹性分布式数据集RDD</h2><p>Spark将数据缓存在<strong>分布式内存</strong>中</p>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20220220221158.png"></p>
<p>如何实现？RDD</p>
<ul>
<li>Spark的核心</li>
<li>分布式内存抽象</li>
<li>提供了一个高度受限的共享内存模型</li>
<li>逻辑上集中但是物理上是存储在集群的多台机器上</li>
</ul>
<h2 id="RDD-属性和特点"><a href="#RDD-属性和特点" class="headerlink" title="RDD 属性和特点"></a>RDD 属性和特点</h2><p>只读</p>
<ul>
<li>通过HDFS或者其它持久化系统创建RDD</li>
<li>通过transformation将父RDD转化得到新的RDD</li>
<li>RDD上保存着前后之间依赖关系</li>
</ul>
<p>Partition</p>
<ul>
<li><p>基本组成单位，RDD在逻辑上按照Partition分块</p>
</li>
<li><p>分布在各个节点上</p>
</li>
<li><p>分片数量决定并行计算的粒度</p>
</li>
<li><p>RDD中保存如何计算每一个分区的函数</p>
</li>
</ul>
<p>容错</p>
<ul>
<li>失败自动重建</li>
<li>如果发生部分分区数据丢失，可以通过依赖关系重新计算</li>
</ul>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20220220221341.png"></p>
<h2 id="RDD-scala-解析"><a href="#RDD-scala-解析" class="headerlink" title="RDD.scala 解析"></a>RDD.scala 解析</h2><p>RDD.scala是所有RDD的总得抽象</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">* Implemented by subclasses to return the set of partitions in this RDD. This method will only</span></span><br><span class="line"><span class="comment">* be called once, so it is safe to implement a time-consuming computation in it.</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">getPartitions</span></span>: <span class="type">Array</span>[<span class="type">Partition</span>]</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">* Implemented by subclasses to return how this RDD depends on parent RDDs. This method will only</span></span><br><span class="line"><span class="comment">* be called once, so it is safe to implement a time-consuming computation in it.</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">getDependencies</span></span>: <span class="type">Seq</span>[<span class="type">Dependency</span>[_]] = deps</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">* :: DeveloperApi ::</span></span><br><span class="line"><span class="comment">* Implemented by subclasses to compute a given partition.</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="meta">@DeveloperApi</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute</span></span>(split: <span class="type">Partition</span>, context: <span class="type">TaskContext</span>): <span class="type">Iterator</span>[<span class="type">T</span>]</span><br></pre></td></tr></table></figure>

<h2 id="RDD-Example"><a href="#RDD-Example" class="headerlink" title="RDD Example"></a>RDD Example</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> lines = sc.textFile(…)</span><br><span class="line">lines.filter(x =&gt; x.contains(<span class="string">&quot;Error&quot;</span>)).count()</span><br></pre></td></tr></table></figure>

<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20220220221614.png"></p>
<h2 id="宽依赖和窄依赖"><a href="#宽依赖和窄依赖" class="headerlink" title="宽依赖和窄依赖"></a>宽依赖和窄依赖</h2><ul>
<li>窄依赖<ul>
<li>没有数据shuffling</li>
<li>所有父RDD中的Partition均会和子RDD的Partition关系是一对一</li>
</ul>
</li>
</ul>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20220220221711.png"></p>
<ul>
<li>宽依赖<ul>
<li>有数据shuffling</li>
<li>所有父RDD中的Partition会被切分，根据key的不同划分到子RDD的Partition中</li>
</ul>
</li>
</ul>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20220220221827.png"></p>
<h2 id="Stage"><a href="#Stage" class="headerlink" title="Stage"></a>Stage</h2><p>什么是Stage</p>
<ul>
<li>一个Job会被拆分为多组Task，每组Task被称为一个Stage</li>
</ul>
<p>划分依据</p>
<ul>
<li>以shuffle操作作为边界，遇到一个宽依赖就分一个stage</li>
</ul>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20220220222050.png"></p>
<h2 id="Stage执行优化"><a href="#Stage执行优化" class="headerlink" title="Stage执行优化"></a>Stage执行优化</h2><ul>
<li><p>对窄依赖可以进行流水线(pipeline)优化</p>
</li>
<li><p>不互相依赖的Stage可以并行执行</p>
</li>
<li><p>存在依赖的Stage必须在依赖的Stage执行完之后才能执行</p>
</li>
<li><p>Stage并行执行程度取决于资源数</p>
</li>
</ul>
<h2 id="Spark执行流程"><a href="#Spark执行流程" class="headerlink" title="Spark执行流程"></a>Spark执行流程</h2><ul>
<li><p>用户创建Spark程序并提交</p>
</li>
<li><p>每个Action会生成一个Job</p>
<ul>
<li>包含了一系列RDD以及如何对其进行转换transformation</li>
</ul>
</li>
<li><p>对每个Job生成DAG</p>
<ul>
<li>Directed Acyclic Graph</li>
</ul>
</li>
<li><p>对根据宽窄依赖对DAG进行划分Stage</p>
</li>
<li><p>对每一个Stage生成一组Task</p>
<ul>
<li>一个Partition对应一个Task</li>
</ul>
</li>
<li><p>Spark会以一组Task为单位进行执行计算</p>
</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> lines = sc.textFile(…)</span><br><span class="line">lines.filter(x =&gt; x.contains(<span class="string">&quot;Error&quot;</span>)).count()</span><br></pre></td></tr></table></figure>

<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20220220222407.png"></p>
<h2 id="Yarn资源调度过程"><a href="#Yarn资源调度过程" class="headerlink" title="Yarn资源调度过程"></a>Yarn资源调度过程</h2><p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/yarn%E8%B5%84%E6%BA%90%E8%B0%83%E5%BA%A6%E8%BF%87%E7%A8%8B-Yarn%E8%B5%84%E6%BA%90%E8%B0%83%E5%BA%A6%E8%BF%87%E7%A8%8B-10.png"></p>
<h2 id="Spark-on-Yarn"><a href="#Spark-on-Yarn" class="headerlink" title="Spark on Yarn"></a>Spark on Yarn</h2><p>Yarn</p>
<ul>
<li><p>ResourceManager：负责整个集群资源管理和分配</p>
</li>
<li><p>ApplicationMaster：Yarn中每个Application对应一个AM，负责与ResrouceManager协商获取资源，并告知NodeManager分配启动Container</p>
</li>
<li><p>NodeManager：每个节点的资源和任务管理器，负责启动Container，并监视资源使用情况</p>
</li>
<li><p>Container：资源抽象</p>
</li>
</ul>
<p>Spark</p>
<ul>
<li><p>Application：用户自己编写的Spark程序</p>
</li>
<li><p>Driver：运行Application的main函数并创建SparkContext，和ClusterManager通信申请资源，任务分配并监控运行情况</p>
</li>
<li><p>ClusterManager：指的是Yarn</p>
</li>
<li><p>DAGScheduler：对DAG图划分Stage</p>
</li>
<li><p>TaskScheduler：把TaskSet分配给具体的Executor</p>
</li>
</ul>
<p>Spark支持三种运行模式</p>
<ul>
<li>standalon, yarn-cluster, yarn-client</li>
</ul>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20220220223145.png"></p>
<h1 id="动手操作"><a href="#动手操作" class="headerlink" title="动手操作"></a>动手操作</h1><ol>
<li><p>使用Scala对数组进行排序，并优雅的给出计算耗时</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">TimeTest</span> </span>&#123;</span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">time</span></span>[<span class="type">R</span>](block: =&gt; <span class="type">R</span>): <span class="type">R</span> = &#123; <span class="comment">// 这里使用了call-by-name参数</span></span><br><span class="line">    <span class="keyword">val</span> t0 = <span class="type">System</span>.currentTimeMillis()</span><br><span class="line">    <span class="keyword">val</span> result = block</span><br><span class="line">    <span class="keyword">val</span> t1 = <span class="type">System</span>.currentTimeMillis()</span><br><span class="line">    println(<span class="string">&quot;Elapsed time = &quot;</span> + (t1 - t0) + <span class="string">&quot;ms&quot;</span>)</span><br><span class="line">    result</span><br><span class="line">	&#125;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">method</span></span>(x: <span class="type">Int</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">        println(<span class="string">&quot;print test = &quot;</span> + x)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        time(method(<span class="number">100</span>))</span><br><span class="line">    &#125; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>


</li>
<li><p>在CDH集群上进入spark-shell，运行如下代码：</p>
<ul>
<li>val lines &#x3D; sc.textFile(hdfs路径xxx)</li>
<li>lines.filter(x &#x3D;&gt; x.contains(“Error”)).count()</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> lines = sc.textFile(<span class="string">&quot;hdfs:///user/cdh/topK/input&quot;</span>)</span><br><span class="line">lines: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">String</span>] = hdfs:<span class="comment">///user/cdh/topK/input MapPartitionsRDD[1] at textFile at &lt;console&gt;:24</span></span><br><span class="line"></span><br><span class="line">scala&gt; lines.filter(x=&gt;x.contains(<span class="string">&quot;品&quot;</span>)).count()</span><br><span class="line">res2: <span class="type">Long</span> = <span class="number">30</span> </span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">男士用品,A商品,zhangsan</span><br><span class="line">男士用品,B商品,zhangsan</span><br><span class="line">男士用品,C商品,zhangsan</span><br><span class="line">男士用品,D商品,zhangsan</span><br><span class="line">男士用品,B商品,lisi</span><br><span class="line">男士用品,C商品,lisi</span><br><span class="line">男士用品,D商品,lisi</span><br><span class="line">男士用品,C商品,wangwu</span><br><span class="line">男士用品,D商品,wangwu</span><br><span class="line">男士用品,D商品,zhaoliu</span><br><span class="line">女士用品,Aa商品,zhangsan</span><br><span class="line">女士用品,Ba商品,zhangsan</span><br><span class="line">女士用品,Ca商品,zhangsan</span><br><span class="line">女士用品,Da商品,zhangsan</span><br><span class="line">女士用品,Ba商品,lisi</span><br><span class="line">女士用品,Ca商品,lisi</span><br><span class="line">女士用品,Da商品,lisi</span><br><span class="line">女士用品,Ca商品,wangwu</span><br><span class="line">女士用品,Da商品,wangwu</span><br><span class="line">女士用品,Da商品,zhaoliu</span><br><span class="line">老人用品,Aa商品,zhangsan</span><br><span class="line">老人用品,Ba商品,zhangsan</span><br><span class="line">老人用品,Ca商品,zhangsan</span><br><span class="line">老人用品,Da商品,zhangsan</span><br><span class="line">老人用品,Ba商品,lisi</span><br><span class="line">老人用品,Ca商品,lisi</span><br><span class="line">老人用品,Da商品,lisi</span><br><span class="line">老人用品,Ca商品,wangwu</span><br><span class="line">老人用品,Da商品,wangwu</span><br><span class="line">老人用品,Da商品,zhaoliu</span><br></pre></td></tr></table></figure></li>
</ol>
<h1 id="常见面试问题"><a href="#常见面试问题" class="headerlink" title="常见面试问题"></a>常见面试问题</h1><ol>
<li><p>解释一下什么是RDD？</p>
<p>RDD是Spark的核心，是一个分布式内存抽象，提供了一个高度受限的共享内存模型，逻辑上集中但是物理上是存储在集群的多台机器上。</p>
<p>RDD的特点是只读、容错。</p>
<p>只读</p>
<ul>
<li>通过HDFS或者其它持久化系统创建RDD</li>
<li>通过transformation将父RDD转化得到新的RDD</li>
<li>RDD上保存着前后之间依赖关系</li>
</ul>
<p>容错</p>
<ul>
<li>失败自动重建</li>
<li>如果发生部分分区数据丢失，可以通过依赖关系重新计算</li>
</ul>
<p>RDD的属性有Partition</p>
<p>Partition</p>
<ul>
<li><p>基本组成单位，RDD在逻辑上按照Partition分块</p>
</li>
<li><p>分布在各个节点上</p>
</li>
<li><p>分片数量决定并行计算的粒度</p>
</li>
<li><p>RDD中保存如何计算每一个分区的函数</p>
</li>
</ul>
</li>
<li><p>什么是宽依赖什么是窄依赖？如何划分？</p>
<p>窄依赖</p>
<ul>
<li>没有数据shuffling</li>
<li>所有父RDD中的Partition均会和子RDD的Partition关系是一对一</li>
</ul>
<p>宽依赖</p>
<ul>
<li>有数据shuffling</li>
<li>所有父RDD中的Partition会被切分，根据key的不同划分到子RDD的Partition中</li>
</ul>
<p>以shuffle操作作为边界，遇到一个宽依赖就分一个stage。一个Job会被拆分为多组Task，每组Task被称为一个Stage</p>
</li>
<li><p>Spark和MapReduce分别适合哪些场景？各自的优缺点分别是什么？</p>
<p><strong>MapReduce计算场景</strong></p>
<p><strong>数据查找</strong></p>
<ul>
<li>分布式Grep</li>
</ul>
<p><strong>Web访问日志分析</strong></p>
<ul>
<li>词频统计</li>
<li>网站PV UV统计</li>
<li>Top K问题</li>
</ul>
<p><strong>倒排索引</strong></p>
<ul>
<li>建立搜索引擎索引</li>
</ul>
<p><strong>分布式排序</strong></p>
<p><strong>MapReduce优点和缺点</strong></p>
<p><strong>优点</strong></p>
<ol>
<li>模型简单，Map + Reduce</li>
<li>高伸缩性，支持横向扩展</li>
<li>灵活，结构化和非结构化数据</li>
<li>速度快，高吞吐离线处理数据</li>
<li>并行处理，编程模型天然支持并行处理</li>
<li>容错能力强</li>
</ol>
<p><strong>缺点</strong></p>
<ol>
<li>流式数据-MapReduce处理模型就决定了需要静态数据</li>
<li>实时计算-不适合低延迟数据处理，需要毫秒级别响应</li>
<li>复杂算法-例如SVM支持向量机</li>
<li>迭代计算-例如斐波那契数列</li>
</ol>
<p><strong>Spark计算场景</strong></p>
<ul>
<li>离线批处理</li>
<li>流式计算</li>
<li>在线实时分析</li>
</ul>
<p><strong>Spark优点和缺点</strong><br><strong>优点</strong></p>
<ul>
<li>内存计算<ul>
<li>支持复杂查询、流式计算、机器学习、图计算</li>
</ul>
</li>
<li>融入Hadoop生态圈<ul>
<li>兼容HDFS</li>
<li>兼容Yarn</li>
</ul>
</li>
<li>核心代码由Scala编写</li>
<li>发展速度快<ul>
<li>社区活跃</li>
<li>最新版本3.1.2 (截止2021年06月01日)</li>
</ul>
</li>
</ul>
<p><strong>缺点</strong></p>
<ul>
<li>JVM的内存开销太大，1G的数据通常需要消耗5G的内存</li>
<li>不同的spark app之间缺乏有效的共享内存机制</li>
</ul>
</li>
<li><p>使用sc.textFile方法读取文件时HDFS Block 和 RDD上Partition之间的对应关系是什么？</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> path = <span class="string">&quot;Current.txt&quot;</span></span><br><span class="line"><span class="keyword">val</span> rdd1 = sc.textFile(path,<span class="number">2</span>)</span><br><span class="line"><span class="keyword">val</span> rdd2=rdd1.filter(line=&gt;line.contains(<span class="string">&quot;error&quot;</span>))</span><br></pre></td></tr></table></figure>

<p>首先rdd1的生成是调用了sc.textFile(path,2)，后面的参数2是生成的RDD的最小分区数minPartitions</p>
<p>textFile会构造一个HadoopRDD，HadoopRDD的分区信息与Hadoop InputFormat有关，HadoopRDD会调用InputFormat的getSplits方法来获取InputFormat的分片信息，一个分片对应一个Spark Partition，因此rdd1的分区数为多少取决于对应的InputFormat 的getSplits返回的分片信息。</p>
<p>对于textFile来说使用的是TextInputFormat，该Format继承于FileInputFormat，然后具体来看getSplits，getSplits的定义为:public InputSplit[] getSplits(JobConf job, int numSplits),后面的numSplits就是你开始设置的minPartitions，HadoopRDD调用的时候会传进来，计算splits之前会先计算几个size</p>
<ol>
<li>goalSize:他的计算逻辑为totalSize &#x2F; (numSplits &#x3D;&#x3D; 0 ? 1 : numSplits)，totalSize为文件总大小，这里就是512M，计算goalSize就应该是256M</li>
<li>minSize:逻辑为 Math.<em>max</em>(job.getLong(org.apache.hadoop.mapreduce.lib.input.FileInputFormat.SPLIT_MINSIZE, 1), minSplitSize)这就是1</li>
<li>splitSize:该值为一个分片的大致大小,Math.<em>max</em>(minSize, Math.<em>min</em>(goalSize, blockSize))，在这里blocksize为128M(该size会在遍历文件时获取每个文件的blocksize，根据题主说的这里就认为128),因此splitSize为128M</li>
</ol>
<p>计算split：会遍历path里的所有的文件，当文件长度为空时生成一个空的split，当文件不为空时分两种情况，根据isSplitable来区别，意思是该文件是否可以被拆分开(主要是看编码格式)，txt文件是可以拆分的(如果不能拆分则是一个文件一个split），在可拆分的情况下会进入一个while，主要是按照splitSize来截取文件，因此一个文件可能是产生多个分片的，前面的分片是按照splitSize来均切分，但是可能后面有个尾巴不足以splitSize的情况是单独作为一个split的。</p>
<p>这样split就计算完成了，中间省略了一些计算块元信息等的其余细节，这里不细说。</p>
<p>再回到问题中来,按照上面的计算，只有一个文件大小512M，splitSize为128M，刚好拆分4次，split数应该是刚好4个split了。并且应该是0+134217728，134217728+134217728，268435456+134217728，402653184+134217728等4个split，因此rdd1分区数为4.</p>
<p>第二个问题，rdd2由rdd1过滤转换而来，并不会改变分区策略，因此分区数也是4.</p>
<p>第三个问题，这三句代码不涉及其余的shuffle操作，task个数由HadoopRDD分区多少来决定，也是4.</p>
</li>
</ol>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2022/01/01/13-%E5%A4%A7%E6%95%B0%E6%8D%AE/03-Spark/01-Scala%E5%92%8CSpark%E5%9F%BA%E7%A1%80/" data-id="clmcxec74000yu8wa4jzr5474" data-title="Spark" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Spark/" rel="tag">Spark</a></li></ul>

    </footer>
  </div>
  
</article>



  


  <nav id="page-nav">
    
    <a class="extend prev" rel="prev" href="/page/6/">&laquo; Prev</a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/5/">5</a><a class="page-number" href="/page/6/">6</a><span class="page-number current">7</span><a class="page-number" href="/page/8/">8</a><a class="page-number" href="/page/9/">9</a><span class="space">&hellip;</span><a class="page-number" href="/page/19/">19</a><a class="extend next" rel="next" href="/page/8/">Next &raquo;</a>
  </nav>

</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/ActiveMQ/">ActiveMQ</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Docker/">Docker</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Dubbo/">Dubbo</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Golang/">Golang</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Java%E5%9F%BA%E7%A1%80/">Java基础</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/K8s/">K8s</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Kafka/">Kafka</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/MQ/">MQ</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/MySQL/">MySQL</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Mybatis/">Mybatis</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Netty/">Netty</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/RPC/">RPC</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/RabbitMQ/">RabbitMQ</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Redis/">Redis</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Socker/">Socker</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Spring/">Spring</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/SpringBoot/">SpringBoot</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/SpringCloud/">SpringCloud</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Tomcat/">Tomcat</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/ZooKeeper/">ZooKeeper</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Zookeeper/">Zookeeper</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/">大数据</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/">数据结构</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E9%9D%A2%E8%AF%95%E9%A2%98/">面试题</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/ActiveMQ/" rel="tag">ActiveMQ</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/CDH/" rel="tag">CDH</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Docker/" rel="tag">Docker</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Dubbo/" rel="tag">Dubbo</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Golang/" rel="tag">Golang</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/HDFS/" rel="tag">HDFS</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/K8s/" rel="tag">K8s</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/KDTree/" rel="tag">KDTree</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Kafka/" rel="tag">Kafka</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/MQ/" rel="tag">MQ</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/MapReduce/" rel="tag">MapReduce</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/MySQL/" rel="tag">MySQL</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Mybatis/" rel="tag">Mybatis</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Netty/" rel="tag">Netty</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/RPC/" rel="tag">RPC</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/RabbitMQ/" rel="tag">RabbitMQ</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Redis/" rel="tag">Redis</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Socker/" rel="tag">Socker</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Spark/" rel="tag">Spark</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Spring/" rel="tag">Spring</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/SpringBoot/" rel="tag">SpringBoot</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/SpringCloud/" rel="tag">SpringCloud</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Tomcat/" rel="tag">Tomcat</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/YARN/" rel="tag">YARN</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ZooKeeper/" rel="tag">ZooKeeper</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%A4%9A%E7%BA%BF%E7%A8%8B/" rel="tag">多线程</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%B3%A8%E8%A7%A3/" rel="tag">注解</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/" rel="tag">设计模式</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E9%9B%86%E5%90%88%E6%BA%90%E7%A0%81/" rel="tag">集合源码</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E9%9D%A2%E8%AF%95%E9%A2%98/" rel="tag">面试题</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/ActiveMQ/" style="font-size: 10px;">ActiveMQ</a> <a href="/tags/CDH/" style="font-size: 10px;">CDH</a> <a href="/tags/Docker/" style="font-size: 18px;">Docker</a> <a href="/tags/Dubbo/" style="font-size: 13px;">Dubbo</a> <a href="/tags/Golang/" style="font-size: 19px;">Golang</a> <a href="/tags/HDFS/" style="font-size: 10px;">HDFS</a> <a href="/tags/K8s/" style="font-size: 10px;">K8s</a> <a href="/tags/KDTree/" style="font-size: 10px;">KDTree</a> <a href="/tags/Kafka/" style="font-size: 10px;">Kafka</a> <a href="/tags/MQ/" style="font-size: 10px;">MQ</a> <a href="/tags/MapReduce/" style="font-size: 10px;">MapReduce</a> <a href="/tags/MySQL/" style="font-size: 10px;">MySQL</a> <a href="/tags/Mybatis/" style="font-size: 14px;">Mybatis</a> <a href="/tags/Netty/" style="font-size: 14px;">Netty</a> <a href="/tags/RPC/" style="font-size: 11px;">RPC</a> <a href="/tags/RabbitMQ/" style="font-size: 15px;">RabbitMQ</a> <a href="/tags/Redis/" style="font-size: 17px;">Redis</a> <a href="/tags/Socker/" style="font-size: 10px;">Socker</a> <a href="/tags/Spark/" style="font-size: 11px;">Spark</a> <a href="/tags/Spring/" style="font-size: 20px;">Spring</a> <a href="/tags/SpringBoot/" style="font-size: 14px;">SpringBoot</a> <a href="/tags/SpringCloud/" style="font-size: 10px;">SpringCloud</a> <a href="/tags/Tomcat/" style="font-size: 13px;">Tomcat</a> <a href="/tags/YARN/" style="font-size: 11px;">YARN</a> <a href="/tags/ZooKeeper/" style="font-size: 12px;">ZooKeeper</a> <a href="/tags/%E5%A4%9A%E7%BA%BF%E7%A8%8B/" style="font-size: 16px;">多线程</a> <a href="/tags/%E6%B3%A8%E8%A7%A3/" style="font-size: 10px;">注解</a> <a href="/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/" style="font-size: 11px;">设计模式</a> <a href="/tags/%E9%9B%86%E5%90%88%E6%BA%90%E7%A0%81/" style="font-size: 13px;">集合源码</a> <a href="/tags/%E9%9D%A2%E8%AF%95%E9%A2%98/" style="font-size: 10px;">面试题</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/01/">January 2022</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/12/">December 2021</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/05/">May 2021</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/04/">April 2021</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/03/">March 2021</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/02/">February 2021</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/01/">January 2021</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/12/">December 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/11/">November 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/10/">October 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/08/">August 2020</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2023/09/10/11-MySQL/04-MySQL%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%8A%E8%AF%BB%E5%86%99%E5%88%86%E7%A6%BB/05-%E9%AB%98%E5%8F%AF%E7%94%A8%E6%95%B0%E6%8D%AE%E5%BA%93%E9%9B%86%E7%BE%A4%E5%AE%89%E8%A3%85%E6%90%AD%E5%BB%BA%E6%89%8B%E5%86%8C/">(no title)</a>
          </li>
        
          <li>
            <a href="/2023/09/10/11-MySQL/04-MySQL%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%8A%E8%AF%BB%E5%86%99%E5%88%86%E7%A6%BB/04-Sharding-JDBC%E8%AF%BB%E5%86%99%E5%88%86%E7%A6%BB/">(no title)</a>
          </li>
        
          <li>
            <a href="/2023/09/10/11-MySQL/04-MySQL%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%8A%E8%AF%BB%E5%86%99%E5%88%86%E7%A6%BB/03-MyCat%E8%AF%BB%E5%86%99%E5%88%86%E7%A6%BB/">(no title)</a>
          </li>
        
          <li>
            <a href="/2023/09/10/11-MySQL/04-MySQL%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%8A%E8%AF%BB%E5%86%99%E5%88%86%E7%A6%BB/02-MySQL%E4%B8%BB%E4%BB%8E%E5%A4%8D%E5%88%B6/">(no title)</a>
          </li>
        
          <li>
            <a href="/2023/09/10/11-MySQL/04-MySQL%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%8A%E8%AF%BB%E5%86%99%E5%88%86%E7%A6%BB/01-MySQL%E4%B8%BB%E4%BB%8E%E5%A4%8D%E5%88%B6%E5%92%8C%E9%AB%98%E5%8F%AF%E7%94%A8/">(no title)</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2023 John Doe<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.6.4.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
</body>
</html>