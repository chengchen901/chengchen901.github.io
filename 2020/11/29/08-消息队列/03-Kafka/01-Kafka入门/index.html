<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>Kafka入门 | Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="使用版本：2.3 学习目标 掌握kafka是什么，主要用途是什么，了解kafka的特性 掌握kafka集群安装 掌握kafka核心概念、工作原理 掌握kafka的使用  简介官网：http:&#x2F;&#x2F;kafka.apache.org&#x2F; 中文文档介绍：https:&#x2F;&#x2F;kafka.apachecn.org&#x2F; Kafka是什么？ 一个分布式的流式数据处理平台。 可以用它来发布和订阅流式的记录。这一方面与消息队">
<meta property="og:type" content="article">
<meta property="og:title" content="Kafka入门">
<meta property="og:url" content="http://example.com/2020/11/29/08-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/03-Kafka/01-Kafka%E5%85%A5%E9%97%A8/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="使用版本：2.3 学习目标 掌握kafka是什么，主要用途是什么，了解kafka的特性 掌握kafka集群安装 掌握kafka核心概念、工作原理 掌握kafka的使用  简介官网：http:&#x2F;&#x2F;kafka.apache.org&#x2F; 中文文档介绍：https:&#x2F;&#x2F;kafka.apachecn.org&#x2F; Kafka是什么？ 一个分布式的流式数据处理平台。 可以用它来发布和订阅流式的记录。这一方面与消息队">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211030144959.png">
<meta property="og:image" content="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211030150619.png">
<meta property="og:image" content="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211030151536.png">
<meta property="og:image" content="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211030151744.png">
<meta property="og:image" content="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211030152359.png">
<meta property="og:image" content="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211031094613.png">
<meta property="og:image" content="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211031094838.png">
<meta property="og:image" content="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211031095659.png">
<meta property="og:image" content="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211031095829.png">
<meta property="og:image" content="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211031100807.png">
<meta property="og:image" content="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211031100901.png">
<meta property="og:image" content="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211031101841.png">
<meta property="og:image" content="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211031101948.png">
<meta property="og:image" content="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211031102057.png">
<meta property="og:image" content="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211113172943.png">
<meta property="og:image" content="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211114114620.png">
<meta property="og:image" content="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211114115059.png">
<meta property="og:image" content="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211119200722.png">
<meta property="og:image" content="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211121115638.png">
<meta property="og:image" content="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211121121020.png">
<meta property="og:image" content="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211121123058.png">
<meta property="og:image" content="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211121125000.png">
<meta property="article:published_time" content="2020-11-29T07:40:00.000Z">
<meta property="article:modified_time" content="2023-02-28T13:42:19.005Z">
<meta property="article:author" content="John Doe">
<meta property="article:tag" content="Kafka">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211030144959.png">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/typeface-source-code-pro@0.0.71/index.min.css">

  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
  
<meta name="generator" content="Hexo 6.3.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hexo</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"><span class="fa fa-bars"></span></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
        
          <a class="nav-icon" href="/atom.xml" title="RSS Feed"><span class="fa fa-rss"></span></a>
        
        <a class="nav-icon nav-search-btn" title="Search"><span class="fa fa-search"></span></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://example.com"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main"><article id="post-08-消息队列/03-Kafka/01-Kafka入门" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2020/11/29/08-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/03-Kafka/01-Kafka%E5%85%A5%E9%97%A8/" class="article-date">
  <time class="dt-published" datetime="2020-11-29T07:40:00.000Z" itemprop="datePublished">2020-11-29</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Kafka/">Kafka</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="p-name article-title" itemprop="headline name">
      Kafka入门
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>使用版本：2.3</p>
<h1 id="学习目标"><a href="#学习目标" class="headerlink" title="学习目标"></a>学习目标</h1><ol>
<li>掌握kafka是什么，主要用途是什么，了解kafka的特性</li>
<li>掌握kafka集群安装</li>
<li>掌握kafka核心概念、工作原理</li>
<li>掌握kafka的使用</li>
</ol>
<h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><p>官网：<a target="_blank" rel="noopener" href="http://kafka.apache.org/">http://kafka.apache.org/</a></p>
<p>中文文档介绍：<a target="_blank" rel="noopener" href="https://kafka.apachecn.org/">https://kafka.apachecn.org/</a></p>
<h2 id="Kafka是什么？"><a href="#Kafka是什么？" class="headerlink" title="Kafka是什么？"></a>Kafka是什么？</h2><ul>
<li>一个分布式的流式数据处理平台。</li>
<li>可以用它来发布和订阅流式的记录。这一方面与消息队列或者企业消息系统类似</li>
<li>它将流式的数据安全地存储在分布式、有副本备份、容错的集群上</li>
<li>可以用来做流式计算</li>
</ul>
<h2 id="Kafka适用于什么场景？"><a href="#Kafka适用于什么场景？" class="headerlink" title="Kafka适用于什么场景？"></a>Kafka适用于什么场景？</h2><p>它可以用于两大类别的应用: </p>
<ol>
<li>构造实时流数据管道，它可以在系统或应用之间可靠地获取数据。 (相当于message queue)</li>
<li>构建实时流式应用程序，对这些流数据进行转换或者影响。 (就是流处理，通过kafka stream topic和topic之间内部进行变化)</li>
</ol>
<h2 id="Kafka的架构体系"><a href="#Kafka的架构体系" class="headerlink" title="Kafka的架构体系"></a>Kafka的架构体系</h2><p><strong>分布式集群</strong></p>
<ul>
<li>Kafka作为一个集群，运行在一台或者多台服务器上.</li>
<li>Kafka 通过 topic 对存储的流数据进行分类。</li>
</ul>
<p><strong>Kafka有四个核心的API：</strong></p>
<ul>
<li>The <a target="_blank" rel="noopener" href="https://kafka.apachecn.org/documentation.html#producerapi">Producer API</a> 允许一个应用程序发布一串流式的数据到一个或者多个Kafka topic。</li>
<li>The <a target="_blank" rel="noopener" href="https://kafka.apachecn.org/documentation.html#consumerapi">Consumer API</a> 允许一个应用程序订阅一个或多个 topic ，并且对发布给他们的流式数据进行处理。</li>
<li>The <a target="_blank" rel="noopener" href="https://kafka.apachecn.org/documentation/streams">Streams API</a> 允许一个应用程序作为一个<em>流处理器</em>，消费一个或者多个topic产生的输入流，然后生产一个输出流到一个或多个topic中去，在输入输出流中进行有效的转换。</li>
<li>The <a target="_blank" rel="noopener" href="https://kafka.apachecn.org/documentation.html#connect">Connector API</a> 允许构建并运行可重用的生产者或者消费者，将Kafka topics连接到已存在的应用程序或者数据系统。比如，连接到一个关系型数据库，捕捉表（table）的所有变更内容。</li>
</ul>
<p>在Kafka中，客户端和服务器使用一个简单、高性能、支持多语言的 <a target="_blank" rel="noopener" href="https://kafka.apache.org/protocol.html">TCP 协议</a>.此协议版本化并且向下兼容老版本， 我们为Kafka提供了Java客户端，也支持许多<a target="_blank" rel="noopener" href="https://cwiki.apache.org/confluence/display/KAFKA/Clients">其他语言的客户端</a>。</p>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211030144959.png"></p>
<h1 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h1><h2 id="环境要求"><a href="#环境要求" class="headerlink" title="环境要求"></a>环境要求</h2><ul>
<li>生产环境，强烈要求 linux ，学习可以windows</li>
<li>java1.8 或以上</li>
</ul>
<h2 id="安装-1"><a href="#安装-1" class="headerlink" title="安装"></a>安装</h2><ol>
<li><p>下载安装包：<a target="_blank" rel="noopener" href="https://www.apache.org/dyn/closer.cgi?path=/kafka/2.3.0/kafka_2.12-2.3.0.tgz">https://www.apache.org/dyn/closer.cgi?path=/kafka/2.3.0/kafka_2.12-2.3.0.tgz</a></p>
<p>windows 和 linux都是同一个安装包，window命令在 bin&#x2F;windows&#x2F; 下。</p>
</li>
<li><p>安装</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mkdir /usr/kafka</span><br><span class="line">tar -xzf kafka_2.12-2.3.0.tgz -C /usr/kafka/</span><br><span class="line">ln -s /usr/kafka/kafka_2.12-2.3.0 /usr/kafka/latest</span><br></pre></td></tr></table></figure>
</li>
<li><p>了解目录结构</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">ll /usr/kafka/latest</span><br><span class="line"></span><br><span class="line">drwxr-xr-x. 3 root root  4096 6月  20 2019 bin</span><br><span class="line">drwxr-xr-x. 2 root root  4096 12月  6 2020 config</span><br><span class="line">drwxr-xr-x. 2 root root  4096 12月  6 2020 libs</span><br><span class="line">-rw-r--r--. 1 root root 32216 6月  20 2019 LICENSE</span><br><span class="line">drwxr-xr-x. 2 root root  4096 12月  6 2020 logs</span><br><span class="line">-rw-r--r--. 1 root root   337 6月  20 2019 NOTICE</span><br><span class="line">drwxr-xr-x. 2 root root    44 6月  20 2019 site-docs</span><br></pre></td></tr></table></figure>

<p>了解 bin 、config 、libs 下都有些什么。</p>
</li>
<li><p>开启zookeeper服务</p>
<p>Kafka集群使用zookeeper来存储元信息，在生产环境下你应该独立安装一个zookeeper集群。学习可以使用Kafka安装包中内置的zookeeper，启动一个单实例的zookeeper</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /usr/kafka/latest/</span><br><span class="line">bin/zookeeper-server-start.sh config/zookeeper.properties &amp;</span><br></pre></td></tr></table></figure>
</li>
<li><p>配置Kafka，kafka的配置文件 config&#x2F;server.properties</p>
<p>要掌握的基本配置项：</p>
<figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Licensed to the Apache Software Foundation (ASF) under one or more</span></span><br><span class="line"><span class="comment"># contributor license agreements.  See the NOTICE file distributed with</span></span><br><span class="line"><span class="comment"># this work for additional information regarding copyright ownership.</span></span><br><span class="line"><span class="comment"># The ASF licenses this file to You under the Apache License, Version 2.0</span></span><br><span class="line"><span class="comment"># (the &quot;License&quot;); you may not use this file except in compliance with</span></span><br><span class="line"><span class="comment"># the License.  You may obtain a copy of the License at</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment">#    http://www.apache.org/licenses/LICENSE-2.0</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># Unless required by applicable law or agreed to in writing, software</span></span><br><span class="line"><span class="comment"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span></span><br><span class="line"><span class="comment"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span></span><br><span class="line"><span class="comment"># See the License for the specific language governing permissions and</span></span><br><span class="line"><span class="comment"># limitations under the License.</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment"># see kafka.server.KafkaConfig for additional details and defaults</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">############################# Server Basics #############################</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment"># The id of the broker. This must be set to a unique integer for each broker.</span></span><br><span class="line"><span class="attr">broker.id</span>=<span class="string">0	# 集群中每个broker的唯一整数 id</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">############################# Socket Server Settings #############################</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment"># The address the socket server listens on. It will get the value returned from </span></span><br><span class="line"><span class="comment"># java.net.InetAddress.getCanonicalHostName() if not configured.</span></span><br><span class="line"><span class="comment">#   FORMAT:</span></span><br><span class="line"><span class="comment">#     listeners = listener_name://host_name:port</span></span><br><span class="line"><span class="comment">#   EXAMPLE:</span></span><br><span class="line"><span class="comment">#     listeners = PLAINTEXT://your.host.name:9092</span></span><br><span class="line"><span class="comment"># 服务端口，默认9092</span></span><br><span class="line"><span class="comment">#listeners==PLAINTEXT://:9092</span></span><br><span class="line"><span class="comment"># 这里ip修改成自己的ip</span></span><br><span class="line"><span class="attr">listeners</span>=<span class="string">PLAINTEXT://192.168.254.159:9092</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment"># Hostname and port the broker will advertise to producers and consumers. If not set, </span></span><br><span class="line"><span class="comment"># it uses the value for &quot;listeners&quot; if configured.  Otherwise, it will use the value</span></span><br><span class="line"><span class="comment"># returned from java.net.InetAddress.getCanonicalHostName().</span></span><br><span class="line"><span class="comment"># broker发布自己的地址给生产者和消费用，不配做则使用配置的listeners值，如果没有配置listeners则取主机名。这里建议配置IP，否则客户端通过主机名连接可能连不通。</span></span><br><span class="line"><span class="comment">#advertised.listeners=PLAINTEXT://your.host.name:9092</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment"># Maps listener names to security protocols, the default is for them to be the same. See the config documentation for more details</span></span><br><span class="line"><span class="comment">#listener.security.protocol.map=PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment"># The number of threads that the server uses for receiving requests from the network and sending responses to the network</span></span><br><span class="line"><span class="attr">num.network.threads</span>=<span class="string">3</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment"># The number of threads that the server uses for processing requests, which may include disk I/O</span></span><br><span class="line"><span class="attr">num.io.threads</span>=<span class="string">8</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment"># The send buffer (SO_SNDBUF) used by the socket server</span></span><br><span class="line"><span class="attr">socket.send.buffer.bytes</span>=<span class="string">102400</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment"># The receive buffer (SO_RCVBUF) used by the socket server</span></span><br><span class="line"><span class="attr">socket.receive.buffer.bytes</span>=<span class="string">102400</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment"># The maximum size of a request that the socket server will accept (protection against OOM)</span></span><br><span class="line"><span class="attr">socket.request.max.bytes</span>=<span class="string">104857600</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">############################# Log Basics #############################</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment"># A comma separated list of directories under which to store log files</span></span><br><span class="line"><span class="attr">log.dirs</span>=<span class="string">/usr/kafka/kafka-logs	# 数据存储目录，逗号间隔的多个目录，一定要修改为你想要存 放的目录。</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment"># The default number of log partitions per topic. More partitions allow greater</span></span><br><span class="line"><span class="comment"># parallelism for consumption, but this will also result in more files across</span></span><br><span class="line"><span class="comment"># the brokers.</span></span><br><span class="line"><span class="comment"># 当创建Topic(主题)未指定分区数时的默认分区数，</span></span><br><span class="line"><span class="attr">num.partitions</span>=<span class="string">1</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment"># The number of threads per data directory to be used for log recovery at startup and flushing at shutdown.</span></span><br><span class="line"><span class="comment"># This value is recommended to be increased for installations with data dirs located in RAID array.</span></span><br><span class="line"><span class="attr">num.recovery.threads.per.data.dir</span>=<span class="string">1</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">############################# Internal Topic Settings  #############################</span></span><br><span class="line"><span class="comment"># The replication factor for the group metadata internal topics &quot;__consumer_offsets&quot; and &quot;__transaction_state&quot;</span></span><br><span class="line"><span class="comment"># For anything other than development testing, a value greater than 1 is recommended for to ensure availability such as 3.</span></span><br><span class="line"><span class="attr">offsets.topic.replication.factor</span>=<span class="string">1</span></span><br><span class="line"><span class="attr">transaction.state.log.replication.factor</span>=<span class="string">1</span></span><br><span class="line"><span class="attr">transaction.state.log.min.isr</span>=<span class="string">1</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">############################# Log Flush Policy #############################</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment"># Messages are immediately written to the filesystem but by default we only fsync() to sync</span></span><br><span class="line"><span class="comment"># the OS cache lazily. The following configurations control the flush of data to disk.</span></span><br><span class="line"><span class="comment"># There are a few important trade-offs here:</span></span><br><span class="line"><span class="comment">#    1. Durability: Unflushed data may be lost if you are not using replication.</span></span><br><span class="line"><span class="comment">#    2. Latency: Very large flush intervals may lead to latency spikes when the flush does occur as there will be a lot of data to flush.</span></span><br><span class="line"><span class="comment">#    3. Throughput: The flush is generally the most expensive operation, and a small flush interval may lead to excessive seeks.</span></span><br><span class="line"><span class="comment"># The settings below allow one to configure the flush policy to flush data after a period of time or</span></span><br><span class="line"><span class="comment"># every N messages (or both). This can be done globally and overridden on a per-topic basis.</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment"># The number of messages to accept before forcing a flush of data to disk</span></span><br><span class="line"><span class="comment">#log.flush.interval.messages=10000</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment"># The maximum amount of time a message can sit in a log before we force a flush</span></span><br><span class="line"><span class="comment">#log.flush.interval.ms=1000</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">############################# Log Retention Policy #############################</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment"># The following configurations control the disposal of log segments. The policy can</span></span><br><span class="line"><span class="comment"># be set to delete segments after a period of time, or after a given size has accumulated.</span></span><br><span class="line"><span class="comment"># A segment will be deleted whenever *either* of these criteria are met. Deletion always happens</span></span><br><span class="line"><span class="comment"># from the end of the log.</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment"># The minimum age of a log file to be eligible for deletion due to age</span></span><br><span class="line"><span class="attr">log.retention.hours</span>=<span class="string">168</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment"># A size-based retention policy for logs. Segments are pruned from the log unless the remaining</span></span><br><span class="line"><span class="comment"># segments drop below log.retention.bytes. Functions independently of log.retention.hours.</span></span><br><span class="line"><span class="comment">#log.retention.bytes=1073741824</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment"># The maximum size of a log segment file. When this size is reached a new log segment will be created.</span></span><br><span class="line"><span class="attr">log.segment.bytes</span>=<span class="string">1073741824</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment"># The interval at which log segments are checked to see if they can be deleted according</span></span><br><span class="line"><span class="comment"># to the retention policies</span></span><br><span class="line"><span class="attr">log.retention.check.interval.ms</span>=<span class="string">300000</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">############################# Zookeeper #############################</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment"># Zookeeper connection string (see zookeeper docs for details).</span></span><br><span class="line"><span class="comment"># This is a comma separated host:port pairs, each corresponding to a zk</span></span><br><span class="line"><span class="comment"># server. e.g. &quot;127.0.0.1:3000,127.0.0.1:3001,127.0.0.1:3002&quot;.</span></span><br><span class="line"><span class="comment"># You can also append an optional chroot string to the urls to specify the</span></span><br><span class="line"><span class="comment"># root directory for all kafka znodes.</span></span><br><span class="line"><span class="attr">zookeeper.connect</span>=<span class="string">localhost:2181</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment"># Timeout in ms for connecting to zookeeper</span></span><br><span class="line"><span class="attr">zookeeper.connection.timeout.ms</span>=<span class="string">6000</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">############################# Group Coordinator Settings #############################</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment"># The following configuration specifies the time, in milliseconds, that the GroupCoordinator will delay the initial consumer rebalance.</span></span><br><span class="line"><span class="comment"># The rebalance will be further delayed by the value of group.initial.rebalance.delay.ms as new members join the group, up to a maximum of max.poll.interval.ms.</span></span><br><span class="line"><span class="comment"># The default value for this is 3 seconds.</span></span><br><span class="line"><span class="comment"># We override this to 0 here as it makes for a better out-of-the-box experience for development and testing.</span></span><br><span class="line"><span class="comment"># However, in production environments the default value of 3 seconds is more suitable as this will help to avoid unnecessary, and potentially expensive, rebalances during application startup.</span></span><br><span class="line"><span class="comment"># 消费组的重平衡延时（单位毫秒），【注意】生产环境请恢复为默认值3秒，或根据实际需要增大</span></span><br><span class="line"><span class="attr">group.initial.rebalance.delay.ms</span>=<span class="string">0</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>启动Kafka broker 实例</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-server-start.sh config/server.properties &amp;</span><br></pre></td></tr></table></figure>

<p>接下来我们可以通过zookeeper客户端去查看kafka集群信息</p>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211030150619.png"></p>
</li>
</ol>
<h2 id="集群搭建"><a href="#集群搭建" class="headerlink" title="集群搭建"></a>集群搭建</h2><p>【生产集群】在其他机器上同样安装kafka，配置它们连接到同一个zookeeper集群、它们的唯一id，数据目录，启动Broker实例即加入集群。</p>
<p>【学习用集群】在同一台机器上启动多个broker实例，按如下步骤操作来搭建一个3节点的集群：</p>
<h3 id="学习用集群搭建"><a href="#学习用集群搭建" class="headerlink" title="学习用集群搭建"></a>学习用集群搭建</h3><p>拷贝配置文件</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cp config/server.properties config/server-1.properties</span><br><span class="line">cp config/server.properties config/server-2.properties</span><br></pre></td></tr></table></figure>

<p>修改配置文件：</p>
<p>config&#x2F;server-1.properties</p>
<figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">broker.id</span>=<span class="string">1 </span></span><br><span class="line"><span class="comment"># 这里ip修改成自己的ip</span></span><br><span class="line"><span class="attr">listeners</span>=<span class="string">PLAINTEXT://192.168.254.159:9093</span></span><br><span class="line"><span class="attr">log.dirs</span>=<span class="string">/usr/kafka/kafka-logs-1</span></span><br></pre></td></tr></table></figure>

<p>config&#x2F;server-2.properties</p>
<figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">broker.id</span>=<span class="string">2 </span></span><br><span class="line"><span class="comment"># 这里ip修改成自己的ip</span></span><br><span class="line"><span class="attr">listeners</span>=<span class="string">PLAINTEXT://192.168.254.159:9094</span></span><br><span class="line"><span class="attr">log.dirs</span>=<span class="string">/usr/kafka/kafka-logs-2</span></span><br></pre></td></tr></table></figure>

<p>启动这两个broker实例</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-server-start.sh config/server-1.properties &amp;</span><br><span class="line">bin/kafka-server-start.sh config/server-2.properties &amp;</span><br></pre></td></tr></table></figure>

<p><strong>【启动失败说明】</strong> 如果启动第二个或第三个broker时提示内存不够用，可以做如下调整：</p>
<p>1、调大你的虚拟机的内存（1G 或更多）</p>
<p>2、调小<strong>Kafka</strong>的堆大小，默认是<strong>1G</strong>，生产用时可以调大。这里学习用可以调为256M（不能太小</p>
<p>了，启动时会heap OOM)</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vi bin/kafka-server-start.sh</span><br></pre></td></tr></table></figure>

<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211030151536.png"></p>
<p>启动好后，可以查看启动的java进程，将看到3个Kafka，一个zookeeper</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">jps</span><br><span class="line"></span><br><span class="line">2324 QuorumPeerMain</span><br><span class="line">3335 Kafka</span><br><span class="line">2633 Kafka</span><br><span class="line">2985 Kafka</span><br><span class="line">3710 Jps</span><br></pre></td></tr></table></figure>

<p>再看看zookeeper上的kafka集群信息：<br><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211030151744.png"></p>
<p>可以玩了，创建一个只有1个分片，3个备份的Topic</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-topics.sh --create --bootstrap-server 192.168.254.159:9092 --replication-factor 3 --partitions 1 --topic my-replicated-topic</span><br></pre></td></tr></table></figure>

<p>查看主题信息：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost latest]# bin/kafka-topics.sh --describe --bootstrap-server 192.168.254.159:9092 --topic my-replicated-topic</span><br><span class="line">Topic:my-replicated-topic	PartitionCount:1	ReplicationFactor:3	Configs:segment.bytes=1073741824</span><br><span class="line">	Topic: my-replicated-topic	Partition: 0	Leader: 1	Replicas: 1,2,0	Isr: 1,2,0</span><br></pre></td></tr></table></figure>

<p>也可在zookeeper客户端上看：</p>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211030152359.png"></p>
<p><strong>【掌握】状态信息的含义</strong></p>
<ul>
<li><p>“leader” is the node responsible for all reads and writes for the given partition. Each node will be the leader for a randomly selected portion of the partitions.</p>
</li>
<li><p>“replicas” is the list of nodes that replicate the log for this partition regardless of whether they are the leader or even if they are currently alive.</p>
</li>
<li><p>“isr” is the set of “in-sync” replicas. This is the subset of the replicas list that is currently alive and caught-up to the leader.</p>
</li>
<li><p>leader是负责指定分区所有读写的节点。每个节点将是随机选择的分区部分的领导者。</p>
</li>
<li><p>“replicas”是复制该分区日志的节点列表，无论它们是leader还是当前处于活动状态。</p>
</li>
<li><p>“isr”是“同步”副本的集合。这是当前活动的副本列表的子集，并赶上了leader。</p>
</li>
</ul>
<ol>
<li><p>现在让我们用Kafka安装包中提供的客户端程序来发布消息：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost latest]# bin/kafka-console-producer.sh --broker-list 192.168.254.159:9092 --topic my-replicated-topic</span><br><span class="line">...</span><br><span class="line"><span class="meta prompt_">&gt;</span><span class="language-bash">my <span class="built_in">test</span> message 1</span> </span><br><span class="line"><span class="meta prompt_">&gt;</span><span class="language-bash">my <span class="built_in">test</span> message 2</span> </span><br><span class="line"><span class="meta prompt_">&gt;</span><span class="language-bash">^C</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>来消费消息</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost latest]# bin/kafka-console-consumer.sh --bootstrap-server 192.168.254.159:9092 --from-beginning --topic my-replicated-topic</span><br><span class="line">...</span><br><span class="line">my test message 1 </span><br><span class="line">my test message 2</span><br><span class="line">^C</span><br></pre></td></tr></table></figure>
</li>
<li><p>怎么停止Broker</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost latest]# bin/kafka-server-stop.sh</span><br></pre></td></tr></table></figure>

<p>这会把我们刚才启动的三个broker都停掉。</p>
</li>
<li><p>让我们来测试一下集群容错，现在主题 my-replicated-topic 的唯一分片的leader备份是1号节点，我们把1号broker停掉看看，找到它的进程号，直接kill。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost latest]# ps aux | grep server-1.properties</span><br><span class="line"> </span><br><span class="line">2633  1.4 18.9 2800140 352408 pts/0  Sl   15:16   0:15 /usr/jdk/latest//bin/java -Xmx256M -Xms256M -server...</span><br><span class="line"></span><br><span class="line">[root@localhost latest]# kill 2633</span><br></pre></td></tr></table></figure>

<p>再看看 主题 my-replicated-topic 的信息：【注意 不能连0号节点了】</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost latest]# bin/kafka-topics.sh --describe --bootstrap-server 192.168.254.159:9092 --topic my-replicated-topic</span><br><span class="line">Topic:my-replicated-topic	PartitionCount:1	ReplicationFactor:3	Configs:segment.bytes=1073741824</span><br><span class="line">	Topic: my-replicated-topic	Partition: 0	Leader: 2	Replicas: 1,2,0	Isr: 2,0</span><br></pre></td></tr></table></figure>

<p>集群搭建完成。</p>
</li>
</ol>
<h1 id="监控管理"><a href="#监控管理" class="headerlink" title="监控管理"></a><strong>监控管理</strong></h1><p>Kakfa自身未提供图形化的监控管理工具，市面上有很多开源的监控管理工具，但都不怎么成熟可靠。这里给介绍一款稍可靠的工具。</p>
<h2 id="Kafka-Offset-Monitor"><a href="#Kafka-Offset-Monitor" class="headerlink" title="Kafka Offset Monitor"></a>Kafka Offset Monitor</h2><p><a target="_blank" rel="noopener" href="https://github.com/sylar88/kafkaOffsetMonitor-0.3.0/blob/master/KafkaOffsetMonitor-assembly-0.3.0-SNAPSHOT.jar">https://github.com/sylar88/kafkaOffsetMonitor-0.3.0/blob/master/KafkaOffsetMonitor-assembly-0.3.0-SNAPSHOT.jar</a></p>
<p>可以实时监控：</p>
<ul>
<li>Kafka集群状态</li>
<li>Topic、Consumer Group列表</li>
<li>图形化展示topic和consumer之间的关系</li>
<li>图形化展示consumer的Offset、Lag等信息</li>
</ul>
<p>它是一个jar 包，使用很简单</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">java -cp KafkaOffsetMonitor-assembly-0.3.0-SNAPSHOT.jar \</span><br><span class="line">    com.quantifind.kafka.offsetapp.OffsetGetterWeb \</span><br><span class="line">    --offsetStorage kafka \</span><br><span class="line">    --zk 192.168.254.159:2181 \</span><br><span class="line">    --port 8080 \</span><br><span class="line">    --refresh 10.seconds \</span><br><span class="line">    --retain 2.days</span><br></pre></td></tr></table></figure>

<h2 id="0-2-0-版本启动命令"><a href="#0-2-0-版本启动命令" class="headerlink" title="0.2.0 版本启动命令"></a>0.2.0 版本启动命令</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">java -cp KafkaOffsetMonitor-assembly-0.2.0.jar \ </span><br><span class="line">    com.quantifind.kafka.offsetapp.OffsetGetterWeb \ </span><br><span class="line">    --zk zk-server1,zk-server2 \ </span><br><span class="line">    --port 8080 \</span><br><span class="line">    --refresh 10.seconds \ </span><br><span class="line">    --retain 2.days</span><br></pre></td></tr></table></figure>

<p>The arguments are:</p>
<ul>
<li><strong>offsetStorage</strong> valid options are ‘’zookeeper’’, ‘’kafka’’ or ‘’storm’’. Anything else falls back to ‘’zookeeper’’ 【说明】0.2.1版本才有这个参数</li>
<li><strong>zk</strong> the ZooKeeper hosts</li>
<li><strong>port</strong> on what port will the app be available</li>
<li><strong>refresh</strong> how often should the app refresh and store a point in the DB</li>
<li><strong>retain</strong> how long should points be kept in the DB</li>
<li><strong>dbName</strong> where to store the history (default ‘offsetapp’)</li>
<li><strong>kafkaOffsetForceFromStart</strong> only applies to ‘’kafka’’ format. Force KafkaOffsetMonitor to scan the commit messages from start (see notes below)</li>
<li><strong>stormZKOffsetBase</strong> only applies to ‘’storm’’ format. Change the offset storage base in zookeeper, default to ‘’&#x2F;stormconsumers’’ (see notes below)</li>
<li><strong>pluginsArgs</strong> additional arguments used by extensions (see below)</li>
</ul>
<p>启动后就可以在浏览器中访问了：<a target="_blank" rel="noopener" href="http://localhost:8080/">http://localhost:8080</a></p>
<h1 id="Spring-中使用"><a href="#Spring-中使用" class="headerlink" title="Spring 中使用"></a>Spring 中使用</h1><p>spring 官网学习文档：<a target="_blank" rel="noopener" href="https://docs.spring.io/spring-kafka/docs/2.2.8.RELEASE/reference/html/#introduction">https://docs.spring.io/spring-kafka/docs/2.2.8.RELEASE/reference/html/#introduction</a></p>
<h1 id="java客户端"><a href="#java客户端" class="headerlink" title="java客户端"></a>java客户端</h1><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span> </span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.kafka<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span> </span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>kafka-clients<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span> </span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.3.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span> </span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h2 id="核心API"><a href="#核心API" class="headerlink" title="核心API"></a>核心API</h2><h3 id="Topic及管理"><a href="#Topic及管理" class="headerlink" title="Topic及管理"></a>Topic及管理</h3><ul>
<li>AdminClient</li>
<li>NewTopic</li>
</ul>
<h3 id="Client"><a href="#Client" class="headerlink" title="Client"></a>Client</h3><ul>
<li>CommonClientConfigs</li>
</ul>
<h3 id="Producer"><a href="#Producer" class="headerlink" title="Producer"></a>Producer</h3><ul>
<li><p>KafkaProducer</p>
</li>
<li><p>ProducerRecord</p>
</li>
<li><p>ProducerConfifig</p>
</li>
<li><p>Serializer</p>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211031094613.png"></p>
</li>
</ul>
<h3 id="Consumer"><a href="#Consumer" class="headerlink" title="Consumer"></a>Consumer</h3><ul>
<li><p>KafkaConsumer</p>
</li>
<li><p>ConsumerConfifig</p>
</li>
<li><p>ConsumerRecord</p>
</li>
<li><p>Deserializer</p>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211031094838.png"></p>
</li>
</ul>
<h1 id="核心概念"><a href="#核心概念" class="headerlink" title="核心概念"></a>核心概念</h1><h2 id="Topic"><a href="#Topic" class="headerlink" title="Topic"></a>Topic</h2><h3 id="Topic说明"><a href="#Topic说明" class="headerlink" title="Topic说明"></a>Topic说明</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost latest]# bin/kafka-topics.sh --create --bootstrap-server 192.168.254.159:9092 --replication-factor 3 --partitions 3 --topic my-33-topic</span><br><span class="line"></span><br><span class="line">[root@localhost latest]# bin/kafka-topics.sh --describe --bootstrap-server 192.168.254.159:9092 --topic my-33-topic</span><br><span class="line">Topic:my-33-topic	PartitionCount:3	ReplicationFactor:3	Configs:segment.bytes=1073741824</span><br><span class="line">	Topic: my-33-topic	Partition: 0	Leader: 0	Replicas: 0,2,1	Isr: 0,2,1</span><br><span class="line">	Topic: my-33-topic	Partition: 1	Leader: 2	Replicas: 2,1,0	Isr: 2,1,0</span><br><span class="line">	Topic: my-33-topic	Partition: 2	Leader: 1	Replicas: 1,0,2	Isr: 1,0,2</span><br></pre></td></tr></table></figure>

<ul>
<li><strong>Topic</strong> ：主题 ，一类消息（数据）</li>
<li><strong>partition</strong>：一个Topic可以分成多个分片来分布式存放数据，分片以顺序号来编号。</li>
<li><strong>Replicas</strong>：为保证数据存储的可靠性，一个分片可以存储多个副本（一般3个），副本被自动均衡分布在集群节点上</li>
<li><strong>Leader</strong>：一个分片的多个副本中自动选举一个作为Leader，通过Leader操作数据，Leader同步给其他副本，以此来保证一致性。当Leader挂了时，自动选择一个做Leader。 </li>
<li>Topic的这些元信息存储在Zookeeper上。</li>
<li>Replicas: 0,2,1 副本在哪些broker上</li>
<li>isr: 0,2,1 副本 存活且同步的broker</li>
</ul>
<h3 id="leader选举"><a href="#leader选举" class="headerlink" title="leader选举"></a>leader选举</h3><p>【问题】每个分片的Leader如何产生？</p>
<p>怎么进行Leader选举？</p>
<p>zk leader 选举的原理是什么？</p>
<p>临时节点 + watch</p>
<p>因为惊群效应，Kafka没有直接使用zk来进行分片的Leader选举</p>
<p>Kafka中增加一个角色： Controller ， 由集群的一个broker来担任这个角色</p>
<p>谁来当Controller 怎么定？ 挂了怎么办？</p>
<p>这里就是真正用的zk 来进行 Controller的选举。</p>
<p>然后所有的 主题的分片的副本的分布、leader的选定都由Controller来完成。</p>
<p>controller_epoch：controller选举的次数</p>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211031095659.png"></p>
<h3 id="消息的分片选择"><a href="#消息的分片选择" class="headerlink" title="消息的分片选择"></a>消息的分片选择</h3><p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211031095829.png"></p>
<p>消息该发往哪个分片？</p>
<p><strong>Producer</strong>客户端负责消息的分发</p>
<ul>
<li>kafka集群中的任何一个broker都可以向producer提供metadata信息,这些metadata中包含”集群中存活的servers列表”&#x2F;”partitions leader列表”等信息；</li>
<li>当producer获取到metadata信息之后, producer将会和Topic下所有partition leader保持socket连接；</li>
<li>消息由producer直接通过socket发送到broker，中间不会经过任何”路由层”，事实上，消息被路由到哪个partition上由producer客户端决定；比如可以采用”random”“key-hash”“轮询”等,如果一个topic中有多个partitions，那么在producer端实现”消息均衡分发”是必要的。</li>
</ul>
<p><strong>消息的分片选择规则：</strong></p>
<ul>
<li>用户给定了分片号且正确有效，则发到给定分片；</li>
<li>未指定分片，指定了Key，则对Key取Hash 求余决定目标分片</li>
<li>未指定分片，也未提供key，则采用轮询</li>
</ul>
<p>ProducerRecord&lt;K, V&gt;</p>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211031100807.png"></p>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211031100901.png"></p>
<h3 id="分片数据持久化原理"><a href="#分片数据持久化原理" class="headerlink" title="分片数据持久化原理"></a>分片数据持久化原理</h3><p>【问题】 副本怎么存储消息数据？</p>
<p>Kafka 是采用文件来存储数据</p>
<p>数据量大</p>
<h4 id="磁盘文件组织方式"><a href="#磁盘文件组织方式" class="headerlink" title="磁盘文件组织方式"></a>磁盘文件组织方式</h4><p>Kafka是一个分布式的流数据存储平台，它<strong>将流数据以日志的方式顺序存储在磁盘文件</strong>中。数据文件的数据存放目录下的组织方式为：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">drwxr-xr-x. 2 root root 141 10月 31 09:53 my-33-topic-0</span><br><span class="line">drwxr-xr-x. 2 root root 141 10月 31 09:53 my-33-topic-1</span><br><span class="line">drwxr-xr-x. 2 root root 141 10月 31 09:53 my-33-topic-2</span><br><span class="line"></span><br><span class="line">[root@localhost kafka-logs]# ll my-33-topic-0</span><br><span class="line">总用量 4</span><br><span class="line">-rw-r--r--. 1 root root 10485760 10月 31 09:53 00000000000000000000.index</span><br><span class="line">-rw-r--r--. 1 root root        0 10月 31 09:53 00000000000000000000.log</span><br><span class="line">-rw-r--r--. 1 root root 10485756 10月 31 09:53 00000000000000000000.timeindex</span><br><span class="line">-rw-r--r--. 1 root root        8 10月 31 09:53 leader-epoch-checkpoint</span><br><span class="line"></span><br><span class="line">[root@localhost kafka-logs]# ll my-33-topic-1</span><br><span class="line">总用量 0</span><br><span class="line">-rw-r--r--. 1 root root 10485760 10月 31 09:53 00000000000000000000.index</span><br><span class="line">-rw-r--r--. 1 root root        0 10月 31 09:53 00000000000000000000.log</span><br><span class="line">-rw-r--r--. 1 root root 10485756 10月 31 09:53 00000000000000000000.timeindex</span><br><span class="line">-rw-r--r--. 1 root root        0 10月 31 09:53 leader-epoch-checkpoint</span><br></pre></td></tr></table></figure>

<p><strong>【说明】消息数据是顺序追加到.log文件中，这用写入速度非常快。</strong></p>
<p>像写日志一样，追加流数据。</p>
<p>文件的顺序写 远快于 随机写</p>
<p>【问题1】日志文件名 这串 00000000000000000000 表示什么意思？ 为什么这么命名？</p>
<p>【问题2】在这个日志文件中怎么存储数据？怎么知道一条消息的结尾。</p>
<p>存得都是字节</p>
<p>offset 偏移量 记录</p>
<p>4个字节（消息内容的长度） + 消息内容（字节序列）</p>
<h4 id="日志文件数据存储格式"><a href="#日志文件数据存储格式" class="headerlink" title="日志文件数据存储格式"></a>日志文件数据存储格式</h4><p>每个日志文件都是“log entries”序列，每一个log entry包含一个4字节整型数（值为N），其后跟N个字节的消息体。每条消息都有一个当前partition下唯一的64字节的offset，它指明了这条消息的起始位置。磁盘上存储的消息格式如下：</p>
<p><strong>消息长度: 4 bytes (value: 1 + 4 + n)</strong><br><strong>版本号: 1 byte</strong><br><strong>CRC 校验码: 4 bytes</strong><br><strong>具体的消息: n bytes</strong></p>
<p>【问题】 每条消息都有一个当前partition下唯一的64字节的offset，它指明了这条消息的起始位置。那这个offset值存哪里？</p>
<p>下面是一个消息的偏移量图示</p>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211031101841.png"></p>
<p>这个信息存储到.index索引文件中</p>
<h4 id="索引"><a href="#索引" class="headerlink" title="索引"></a>索引</h4><p>【思考】索引中存储的数据的结构是怎样的？</p>
<p>{消息序号，存储偏移地址}</p>
<p>【思考】有必要在索引中存储每一条消息的偏移地址吗？</p>
<p>稀疏索引</p>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211031101948.png"></p>
<p>kafka用的稀疏索引。</p>
<p>【思考】 kafka作为一个分布式的流数据存储平台，它能存储海量的消息数据，那一个分片的数据可能会很大吗？</p>
<p>一个很大的分片，也即一个很大的文件，操作方便吗</p>
<h4 id="Segment-段"><a href="#Segment-段" class="headerlink" title="Segment 段"></a>Segment 段</h4><p>分片分成多个段（一个.log文件）来存储，段的大小固定（可以指定）</p>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211031102057.png"></p>
<p>每个partion(目录)相当于一个巨型文件被平均分配到多个大小相等segment(段)数据文件中，每个segment文件名为该segment第一条消息的offset和“.log”组成。但每个段segment file消息数量不一定相等，这种特性方便old segment fifile快速被删除。（默认情况下每个文件大小为1G）</p>
<p>每个partiton只需要支持顺序读写就行了，segment文件生命周期由服务端配置参数决定。</p>
<p>这样做的好处就是能快速删除无用文件，有效提高磁盘利用率。</p>
<h4 id="消息什么时候删除"><a href="#消息什么时候删除" class="headerlink" title="消息什么时候删除"></a>消息什么时候删除</h4><ul>
<li>通过在server.properties文件中配置全局默认的日志保留策略来控制：</li>
</ul>
<p>支持两种策略：时间 和 大小 。 可多策略，哪个达到即哪个生效。</p>
<figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">############################# Log Retention Policy #############################</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment"># The following configurations control the disposal of log segments. The policy can</span></span><br><span class="line"><span class="comment"># be set to delete segments after a period of time, or after a given size has accumulated.</span></span><br><span class="line"><span class="comment"># A segment will be deleted whenever *either* of these criteria are met. Deletion always happens</span></span><br><span class="line"><span class="comment"># from the end of the log.</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment"># The minimum age of a log file to be eligible for deletion due to age</span></span><br><span class="line"><span class="comment"># 策略1 留存多长时间</span></span><br><span class="line"><span class="attr">log.retention.hours</span>=<span class="string">168</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment"># A size-based retention policy for logs. Segments are pruned from the log unless the remaining</span></span><br><span class="line"><span class="comment"># segments drop below log.retention.bytes. Functions independently of log.retention.hours.</span></span><br><span class="line"><span class="comment"># 策略2 留存多少字节</span></span><br><span class="line"><span class="comment">#log.retention.bytes=1073741824</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment"># The maximum size of a log segment file. When this size is reached a new log segment will be created.</span></span><br><span class="line"><span class="attr">log.segment.bytes</span>=<span class="string">1073741824</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment"># The interval at which log segments are checked to see if they can be deleted according</span></span><br><span class="line"><span class="comment"># to the retention policies</span></span><br><span class="line"><span class="attr">log.retention.check.interval.ms</span>=<span class="string">300000</span></span><br></pre></td></tr></table></figure>

<ul>
<li><p>在定义Topic时指定对应参数</p>
<p>文档：<a target="_blank" rel="noopener" href="https://kafka.apachecn.org/documentation.html#topicconfigs">https://kafka.apachecn.org/documentation.html#topicconfigs</a></p>
</li>
</ul>
<table>
<thead>
<tr>
<th>名称</th>
<th>描述</th>
<th>类型</th>
<th>默认值</th>
<th>有效值</th>
<th>服务器默认属性</th>
<th>重要性</th>
</tr>
</thead>
<tbody><tr>
<td>retention.bytes</td>
<td>如果使用“delete”保留策略，此配置控制分区(由日志段组成)在放弃旧日志段以释放空间之前的最大大小。默认情况下，没有大小限制，只有时间限制。由于此限制是在分区级别强制执行的，因此，将其乘以分区数，计算出topic保留值，以字节为单位。</td>
<td>long</td>
<td>-1</td>
<td></td>
<td>log.retention.bytes</td>
<td>medium</td>
</tr>
<tr>
<td>retention.ms</td>
<td>如果使用“delete”保留策略，此配置控制保留日志的最长时间，然后将旧日志段丢弃以释放空间。这代表了用户读取数据的速度的SLA。</td>
<td>long</td>
<td>604800000</td>
<td></td>
<td>log.retention.ms</td>
<td>medium</td>
</tr>
<tr>
<td>segment.bytes</td>
<td>此配置控制日志的段文件大小。保留和清理总是一次完成一个文件，所以更大的段大小意味着更少的文件，但对保留的粒度控制更少。</td>
<td>int</td>
<td>1073741824</td>
<td>[14,…]</td>
<td>log.segment.bytes</td>
<td>medium</td>
</tr>
</tbody></table>
<h4 id="消息的序号"><a href="#消息的序号" class="headerlink" title="消息的序号"></a>消息的序号</h4><p>在每个分片中会给每条消息一个<strong>递增的序号</strong></p>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211113172943.png"></p>
<p>【重点】消费者的offset （序号偏移量）</p>
<h4 id="TimeIndex-时间索引"><a href="#TimeIndex-时间索引" class="headerlink" title="TimeIndex 时间索引"></a>TimeIndex 时间索引</h4><p>【问题】如果我们想要消费从某时刻开始的消息，该怎么办？</p>
<ul>
<li>消息要有时间戳</li>
<li>建立时间索引 .timeindex 数据结构 {时间戳，序号}</li>
</ul>
<p>当要获取某时刻开始的消息时，根据时间戳到时间索引中获得 &gt;&#x3D; 该时刻的第一个消息序号；然后从该序号开始拉取数据。</p>
<p><strong>Broker中关于Timestamp的全局默认配置参数：</strong></p>
<table>
<thead>
<tr>
<th>名称</th>
<th>描述</th>
<th>类型</th>
<th>默认值</th>
<th>有效值</th>
<th>重要性</th>
</tr>
</thead>
<tbody><tr>
<td>log.message.timestamp.difference.max.ms</td>
<td>broker收到消息时的时间戳和消息中指定的时间戳之间允许的最大差异。当log.message.timestamp.type&#x3D;CreateTime,如果时间差超过这个阈值，消息将被拒绝。如果log.message.timestamp.type &#x3D; logappendtime，则该配置将被忽略。允许的最大时间戳差值，不应大于log.retention.ms，以避免不必要的频繁日志滚动。</td>
<td>long</td>
<td>9223372036854775807</td>
<td></td>
<td>中</td>
</tr>
<tr>
<td>log.message.timestamp.type</td>
<td>定义消息中的时间戳是消息创建时间还是日志追加时间。 该值应该是“createtime”或“logappendtime”。</td>
<td>string</td>
<td>CreateTime</td>
<td>[CreateTime, LogAppendTime]</td>
<td>中</td>
</tr>
</tbody></table>
<p><strong>Topic中关于Timestamp的配置参数：</strong></p>
<table>
<thead>
<tr>
<th>名称</th>
<th>描述</th>
<th>类型</th>
<th>默认值</th>
<th>有效值</th>
<th></th>
<th>重要性</th>
</tr>
</thead>
<tbody><tr>
<td>message.timestamp.difference.max.ms</td>
<td>broker接收消息时所允许的时间戳与消息中指定的时间戳之间的最大差异。如果message.timestamp.type&#x3D;CreateTime，则如果时间戳的差异超过此阈值，则将拒绝消息。如果message.timestamp.type&#x3D;LogAppendTime，则忽略此配置。</td>
<td>long</td>
<td>9223372036854775807</td>
<td>[0,…]</td>
<td>log.message.timestamp.difference.max.ms</td>
<td>medium</td>
</tr>
<tr>
<td>message.timestamp.type</td>
<td>定义消息中的时间戳是消息创建时间还是日志附加时间。值应该是“CreateTime”或“LogAppendTime”</td>
<td>string</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody></table>
<h4 id="Topic的配置参数-【了解】"><a href="#Topic的配置参数-【了解】" class="headerlink" title="Topic的配置参数 【了解】"></a>Topic的配置参数 【了解】</h4><h5 id="参数列表"><a href="#参数列表" class="headerlink" title="参数列表"></a>参数列表</h5><p><a target="_blank" rel="noopener" href="https://kafka.apachecn.org/documentation.html#topicconfigs">https://kafka.apachecn.org/documentation.html#topicconfigs</a></p>
<h5 id="参数修改"><a href="#参数修改" class="headerlink" title="参数修改"></a>参数修改</h5><p>与Topic相关的配置既包含服务器默认值，也包含可选的每个Topic覆盖值。 如果没有给出每个Topic的配置，那么服务器默认值就会被使用。 通过提供一个或多个 <code>--config</code> 选项，可以在创建Topic时设置覆盖值。 本示例使用自定义的最大消息大小和刷新率创建了一个名为 <em>my-topic</em> 的topic:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt; bin/kafka-topics.sh --zookeeper localhost:2181 --create --topic my-topic --partitions 1 --replication-factor 1 --config max.message.bytes=64000 --config flush.messages=1</span><br></pre></td></tr></table></figure>

<p>也可以在使用alter configs命令稍后更改或设置覆盖值. 本示例重置<em>my-topic</em>的最大消息的大小:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt; bin/kafka-configs.sh --zookeeper localhost:2181 --entity-type topics --entity-name my-topic --alter --add-config max.message.bytes=128000</span><br></pre></td></tr></table></figure>

<p>您可以执行如下操作来检查topic设置的覆盖值</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt; bin/kafka-configs.sh --zookeeper localhost:2181 --entity-type topics --entity-name my-topic --describe</span><br></pre></td></tr></table></figure>

<p>您可以执行如下操作来删除一个覆盖值</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt; bin/kafka-configs.sh --zookeeper localhost:2181  --entity-type topics --entity-name my-topic --alter --delete-config max.message.bytes</span><br></pre></td></tr></table></figure>

<h4 id="删除Topic"><a href="#删除Topic" class="headerlink" title="删除Topic"></a>删除Topic</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-topics.sh --create --bootstrap-server localhost:9092 --replication-factor 1 --partitions 1 --topic test-1</span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-console-producer.sh --broker-list localhost:9092 --topic test-1</span><br></pre></td></tr></table></figure>

<p>删除Topic</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-topics.sh --delete --bootstrap-server localhost:9092 --topic test-1</span><br></pre></td></tr></table></figure>

<p><strong>说明：</strong></p>
<p>当Topic是新创建的空Topic时，元信息和Topic的存储目录都会删除。</p>
<p>当Topic已有数据时，元信息在zookeeper上被删除，数据存储目录被标识为delete</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">drwxr-xr-x. 2 root root 141 11月 13 23:33 test-1-0.ba94171b0a724f36ab7d85af5ab8d18a-delete</span><br></pre></td></tr></table></figure>

<h2 id="Producer-1"><a href="#Producer-1" class="headerlink" title="Producer"></a>Producer</h2><p>生产者可以将数据发布到所选择的topic（主题）中。生产者负责将记录分配到topic的哪一个partition（分片）中。</p>
<h3 id="生产者消息发布确认机制"><a href="#生产者消息发布确认机制" class="headerlink" title="生产者消息发布确认机制"></a>生产者消息发布确认机制</h3><p>设置发送数据是否需要服务端的反馈,有三个值 [all,0,1,-1 ] ，默认 1</p>
<ul>
<li>0: producer不会等待broker发送ack</li>
<li>1: 当leader接收到消息之后发送ack</li>
<li>all：leader接收到消息后，等待所有in-sync的副本同步完成之后 发送ack。这样可以提供最好的可靠性。This means the leader will wait for the full set of in-sync replicas to acknowledge the record. This guarantees that the record will not be lost as long as at least one in-sync replica remains alive. This is the strongest available guarantee. This is equivalent to the acks&#x3D;-1 setting.</li>
<li>-1: 等价于 all</li>
</ul>
<h4 id="Callback异步处理的确认结果"><a href="#Callback异步处理的确认结果" class="headerlink" title="Callback异步处理的确认结果"></a>Callback异步处理的确认结果</h4><p>在发送时，你可以完全非阻塞，通过指定回调来处理发送结果，但当使用事务时，则没必要使用Callback，因为结果在事务方法中处理。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> Future&lt;RecordMetadata&gt; <span class="title function_">send</span><span class="params">(ProducerRecord&lt;K, V&gt; record, Callback callback)</span></span><br></pre></td></tr></table></figure>

<p>发送到同一分片的结果处理将按顺序执行回调。请详细阅读它的注释。</p>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211114114620.png"></p>
<h4 id="spring中的处理方式"><a href="#spring中的处理方式" class="headerlink" title="spring中的处理方式"></a>spring中的处理方式</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Set a &#123;<span class="doctag">@link</span> ProducerListener&#125; which will be invoked when Kafka acknowledges</span></span><br><span class="line"><span class="comment"> * a send operation. By default a &#123;<span class="doctag">@link</span> LoggingProducerListener&#125; is configured</span></span><br><span class="line"><span class="comment"> * which logs errors only.</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> producerListener the listener; may be &#123;<span class="doctag">@code</span> null&#125;.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">setProducerListener</span><span class="params">(<span class="meta">@Nullable</span> ProducerListener&lt;K, V&gt; producerListener)</span> &#123;</span><br><span class="line">	<span class="built_in">this</span>.producerListener = producerListener;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="ProducerInterceptor"><a href="#ProducerInterceptor" class="headerlink" title="ProducerInterceptor"></a>ProducerInterceptor</h4><p>如果想对消息的发送过程增加额外的统一处理逻辑可以提供 ProducerInterceptor 实现</p>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211114115059.png"></p>
<p>请详细了解它的注释说明。</p>
<p><strong>使用方式</strong> ，通过interceptor.classes指定实现类名：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 配置 ProducerInterceptor</span></span><br><span class="line">props.put(ProducerConfig.INTERCEPTOR_CLASSES_CONFIG,</span><br><span class="line">                <span class="string">&quot;com.study.kafka.client.s02_pub_ack.MyProducerInterceptor1,&quot;</span> + <span class="string">&quot;com.study.kafka.client.s02_pub_ack.MyProducerInterceptor2&quot;</span>);</span><br></pre></td></tr></table></figure>

<p>参数的说明</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">String</span> <span class="variable">INTERCEPTOR_CLASSES_CONFIG</span> <span class="operator">=</span> <span class="string">&quot;interceptor.classes&quot;</span>;</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">String</span> <span class="variable">INTERCEPTOR_CLASSES_DOC</span> <span class="operator">=</span> <span class="string">&quot;A list of classes to use as interceptors. &quot;</span></span><br><span class="line">                                                        + <span class="string">&quot;Implementing the &lt;code&gt;org.apache.kafka.clients.producer.ProducerInterceptor&lt;/code&gt; interface allows you to intercept (and possibly mutate) the records &quot;</span></span><br><span class="line">                                                        + <span class="string">&quot;received by the producer before they are published to the Kafka cluster. By default, there are no interceptors.&quot;</span>;</span><br></pre></td></tr></table></figure>

<h3 id="幂等模式-【了解】"><a href="#幂等模式-【了解】" class="headerlink" title="幂等模式 【了解】"></a>幂等模式 【了解】</h3><p>Kafka 0.11.0 支持幂等和事务性能力。幂等传递确保消息在单个生产者的生命周期内仅给特定的主题分区传递一次。事务交付允许生产者给多个分区发送数据，这样所有的消息都会被传递成功或失败。这些功能使Kafka符合“恰好一次语义”。</p>
<p>配置生产者参数enable.idempotence 为 true 。 retries acks 都不要配置了，因为会有自动的默认值：Integer.MAX_VALUE all 。 不可以业务上重发相同的业务数据。</p>
<h3 id="事务模式"><a href="#事务模式" class="headerlink" title="事务模式"></a>事务模式</h3><p>事务模式是为了让发往多个分片、多个Topic的多条消息具有原子性。</p>
<p><strong>事务模式要求：</strong></p>
<ul>
<li>要使用事务模式和对应的api，必须设置 transactional.id 属性。 transactional.id 配置后，将自动启用幂等性，同时启用幂等性所依赖的生成器配置。transactional.id 是事务标识，用于跨单个生产者实例的多个会话启用事务恢复。对于分区应用程序中运行的每个生成器实例，它应该是惟一的。</li>
<li>Kafka集群需要有至少3个节点</li>
<li>为了从端到端实现事务保证，还必须将消费者配置为只<strong>读取提交的消息</strong>。</li>
</ul>
<p>所有事务API是同步阻塞的。</p>
<p>Producer是线程安全。</p>
<h2 id="Consumer-1"><a href="#Consumer-1" class="headerlink" title="Consumer"></a>Consumer</h2><p>Kafka中消费者是采用 poll 拉模式来获取消息。</p>
<h3 id="Consumer示例"><a href="#Consumer示例" class="headerlink" title="Consumer示例"></a>Consumer示例</h3><p>【注意】 Consumer是非线程安全的。 多线程中的情况下，一个线程一个Consumer。</p>
<h3 id="消费组"><a href="#消费组" class="headerlink" title="消费组"></a>消费组</h3><h4 id="消费组的说明"><a href="#消费组的说明" class="headerlink" title="消费组的说明"></a>消费组的说明</h4><p>消费者使用一个 消费组 名称来进行标识，发布到topic中的每条记录被分配给订阅消费组中的一个消费者实例.消费者实例可以分布在多个进程中或者多个机器上。</p>
<p>如果所有的消费者实例在同一消费组中，消息记录会负载平衡到每一个消费者实例.</p>
<p>如果所有的消费者实例在不同的消费组中，每条消息记录会广播到所有的消费者进程.</p>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211119200722.png"></p>
<p>通常情况下，每个 topic 都会有一些消费组，一个消费组对应一个”逻辑订阅者”。一个消费组由许多消费者实例组成，便于扩展和容错。这就是发布和订阅的概念，只不过订阅者是一组消费者而不是单个的进程。</p>
<p>在Kafka中实现消费的方式是将日志中的分片划分到每一个消费者实例上，以便在任何时间，每个实例都是分片唯一的消费者。维护消费组中的消费关系由Kafka协议动态处理。如果新的实例加入组，他们将从组中其他成员处接管一些 partition 分区;如果一个实例消失，拥有的分区将被分发到剩余的实例。</p>
<p>Kafka 只保证分区内的记录是有序的，而不保证主题中不同分区的顺序。如果你希望所有消息都有序消费，可使用仅有一个分区的主题来实现，这意味着每个消费者组只有一个消费者进程。</p>
<h4 id="group-rebalance-消费组重平衡"><a href="#group-rebalance-消费组重平衡" class="headerlink" title="group rebalance 消费组重平衡"></a>group rebalance 消费组重平衡</h4><p>【注意】消费组的重平衡延时参数设置</p>
<figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">############################# Group Coordinator Settings #############################</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment"># The following configuration specifies the time, in milliseconds, that the GroupCoordinator will delay the initial consumer rebalance.</span></span><br><span class="line"><span class="comment"># The rebalance will be further delayed by the value of group.initial.rebalance.delay.ms as new members join the group, up to a maximum of max.poll.interval.ms.</span></span><br><span class="line"><span class="comment"># The default value for this is 3 seconds.</span></span><br><span class="line"><span class="comment"># We override this to 0 here as it makes for a better out-of-the-box experience for development and testing.</span></span><br><span class="line"><span class="comment"># However, in production environments the default value of 3 seconds is more suitable as this will help to avoid unnecessary, and potentially expensive, rebalances during application startup.</span></span><br><span class="line"><span class="comment"># 消费组的重平衡延时（单位毫秒），【注意】生产环境请恢复为默认值3秒，或根据实际需要增大</span></span><br><span class="line"><span class="attr">group.initial.rebalance.delay.ms</span>=<span class="string">0</span></span><br></pre></td></tr></table></figure>

<p>触发重平衡的事件：</p>
<ul>
<li>消费者数量变化</li>
<li>分片的增减</li>
</ul>
<p>【思考】Kafka怎么知道消费者离线了，需要rebalance了？</p>
<p>消费者与partition leader保持长连接，通过心跳机制检测消费者是否离线</p>
<h4 id="Beatheart-Session"><a href="#Beatheart-Session" class="headerlink" title="Beatheart Session"></a>Beatheart Session</h4><table>
<thead>
<tr>
<th>Name</th>
<th>Description</th>
<th>Type</th>
<th>Default</th>
<th>Valid Values</th>
<th>Importance</th>
</tr>
</thead>
<tbody><tr>
<td>heartbeat.interval.ms</td>
<td>The expected time between heartbeats to the consumer coordinator when using Kafka’s group management facilities. Heartbeats are used to ensure that the consumer’s session stays active and to facilitate rebalancing when new consumers join or leave the group. The value must be set lower than <code>session.timeout.ms</code>, but typically should be set no higher than 1&#x2F;3 of that value. It can be adjusted even lower to control the expected time for normal rebalances.</td>
<td>int</td>
<td>3000</td>
<td></td>
<td>high</td>
</tr>
<tr>
<td>session.timeout.ms</td>
<td>The timeout used to detect consumer failures when using Kafka’s group management facility. The consumer sends periodic heartbeats to indicate its liveness to the broker. If no heartbeats are received by the broker before the expiration of this session timeout, then the broker will remove this consumer from the group and initiate a rebalance. Note that the value must be in the allowable range as configured in the broker configuration by <code>group.min.session.timeout.ms</code> and <code>group.max.session.timeout.ms</code>.</td>
<td>int</td>
<td>10000</td>
<td></td>
<td>high</td>
</tr>
<tr>
<td>max.poll.interval.ms</td>
<td>The maximum delay between invocations of poll() when using consumer group management. This places an upper bound on the amount of time that the consumer can be idle before fetching more records. If poll() is not called before expiration of this timeout, then the consumer is considered failed and the group will rebalance in order to reassign the partitions to another member.</td>
<td>int</td>
<td>300000</td>
<td>[1,…]</td>
<td>medium</td>
</tr>
</tbody></table>
<h3 id="消费offset"><a href="#消费offset" class="headerlink" title="消费offset"></a>消费offset</h3><p>【思考】消费者启动时从哪里开始消费消息？</p>
<h4 id="auto-offset-reset"><a href="#auto-offset-reset" class="headerlink" title="auto.offset.reset"></a>auto.offset.reset</h4><p>如果zookeeper上没有消费者的offset，或保存的消费者offset被删除了，消费者启动时从哪里开始消费？</p>
<table>
<thead>
<tr>
<th>PROPERTY</th>
<th>DEFAULT</th>
<th>DESCRIPTION</th>
</tr>
</thead>
<tbody><tr>
<td>auto.offset.reset</td>
<td>largest</td>
<td>What to do when there is no initial offset in ZooKeeper or if an offset is out of range:<br/>* smallest(最小偏移量) : automatically reset the offset to the smallest offset<br/>* largest(最大偏移量) : automatically reset the offset to the largest offset<br/>* anything else(抛异常): throw exception to the consumer</td>
</tr>
</tbody></table>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//设置创建的消费者从何处开始消费消息 </span></span><br><span class="line">props.put(<span class="string">&quot;auto.offset.reset&quot;</span>, <span class="string">&quot;smallest&quot;</span>);</span><br></pre></td></tr></table></figure>

<p>【思考】每次启动都从头开始消费吗？如何从上次结束的位置开始？</p>
<h4 id="自动提交"><a href="#自动提交" class="headerlink" title="自动提交"></a>自动提交</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 开启自动消费offset提交</span></span><br><span class="line"><span class="comment">// 如果此值设置为true，consumer会周期性的把当前消费的offset值保存到zookeeper。当consumer失败重启之后将会使用此值作为新开始消费的值。</span></span><br><span class="line">props.put(<span class="string">&quot;enable.auto.commit&quot;</span>, <span class="string">&quot;true&quot;</span>);</span><br><span class="line"><span class="comment">// 自动消费offset提交的间隔时间</span></span><br><span class="line">props.put(<span class="string">&quot;auto.commit.interval.ms&quot;</span>, <span class="string">&quot;1000&quot;</span>);</span><br></pre></td></tr></table></figure>

<p>存在问题：</p>
<ul>
<li>重复消费</li>
<li>丢数据（丢消息，只是拉取到数据还没处理完成就自动提交offset了）</li>
</ul>
<h4 id="手动提交"><a href="#手动提交" class="headerlink" title="手动提交"></a>手动提交</h4><h5 id="代码示例"><a href="#代码示例" class="headerlink" title="代码示例"></a>代码示例</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//设置手动提交</span></span><br><span class="line">props.put(<span class="string">&quot;enable.auto.commit&quot;</span>, <span class="string">&quot;false&quot;</span>);</span><br><span class="line">...</span><br><span class="line"><span class="comment">//手动同步提交消费者offset到zookeeper</span></span><br><span class="line">consumer.commitSync();</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerRecord;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerRecords;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.KafkaConsumer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.time.Duration;</span><br><span class="line"><span class="keyword">import</span> java.util.ArrayList;</span><br><span class="line"><span class="keyword">import</span> java.util.Arrays;</span><br><span class="line"><span class="keyword">import</span> java.util.List;</span><br><span class="line"><span class="keyword">import</span> java.util.Properties;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">ManualCommitConsumerDemo</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        <span class="type">Properties</span> <span class="variable">props</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Properties</span>();</span><br><span class="line">        props.put(<span class="string">&quot;bootstrap.servers&quot;</span>, <span class="string">&quot;192.168.254.159:9092&quot;</span>);</span><br><span class="line">        props.put(<span class="string">&quot;group.id&quot;</span>, <span class="string">&quot;test&quot;</span>);</span><br><span class="line">        <span class="comment">// 设置手动提交消费offset</span></span><br><span class="line">        props.put(<span class="string">&quot;enable.auto.commit&quot;</span>, <span class="string">&quot;false&quot;</span>);</span><br><span class="line">        props.put(<span class="string">&quot;key.deserializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringDeserializer&quot;</span>);</span><br><span class="line">        props.put(<span class="string">&quot;value.deserializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringDeserializer&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">try</span> (KafkaConsumer&lt;String, String&gt; consumer = <span class="keyword">new</span> <span class="title class_">KafkaConsumer</span>&lt;&gt;(props);) &#123;</span><br><span class="line">            consumer.subscribe(Arrays.asList(<span class="string">&quot;test&quot;</span>, <span class="string">&quot;test-group&quot;</span>));</span><br><span class="line">            <span class="keyword">final</span> <span class="type">int</span> <span class="variable">minBatchSize</span> <span class="operator">=</span> <span class="number">10</span>;</span><br><span class="line">            List&lt;ConsumerRecord&lt;String, String&gt;&gt; buffer = <span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;&gt;();</span><br><span class="line">            <span class="keyword">while</span> (<span class="literal">true</span>) &#123;</span><br><span class="line">                ConsumerRecords&lt;String, String&gt; records = consumer.poll(Duration.ofMillis(<span class="number">200L</span>));</span><br><span class="line">                <span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; record : records) &#123;</span><br><span class="line">                    System.out.printf(<span class="string">&quot;offset = %d, key = %s, value = %s%n&quot;</span>, record.offset(), record.key(), record.value());</span><br><span class="line">                    buffer.add(record);</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="keyword">if</span> (buffer.size() &gt;= minBatchSize) &#123;</span><br><span class="line">                    insertIntoDb(buffer);</span><br><span class="line">                    <span class="comment">// 手动同步提交消费者offset到zookeeper</span></span><br><span class="line">                    consumer.commitSync();</span><br><span class="line">                    buffer.clear();</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">insertIntoDb</span><span class="params">(List&lt;ConsumerRecord&lt;String, String&gt;&gt; buffer)</span> &#123;</span><br><span class="line">        <span class="comment">// Insert into db</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h5 id="Consumer-的提交方法"><a href="#Consumer-的提交方法" class="headerlink" title="Consumer 的提交方法"></a>Consumer 的提交方法</h5><p>有同步、异步的方法，还有获取上次提交的offset数据的方法</p>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211121115638.png"></p>
<h5 id="灵活按分片提交消费offset值"><a href="#灵活按分片提交消费offset值" class="headerlink" title="灵活按分片提交消费offset值"></a>灵活按分片提交消费offset值</h5><p>当你订阅了多个topic的消息，一次拉取可能会返回多个主题多个分片的消息集，上面的手动提交是所有</p>
<p>的分片一起提交，如有需要，我们可以更细粒度地控制提交，下面的代码示例展示了按分片处理消息，</p>
<p>没处理完一个则提交该分片的消费offset值。<strong>【注意】 offset值是下次拉取的起始值（lastOffset + 1)</strong></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerRecord;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerRecords;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.KafkaConsumer;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.OffsetAndMetadata;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.common.TopicPartition;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.time.Duration;</span><br><span class="line"><span class="keyword">import</span> java.util.*;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">ManualPartitionCommitConsumer</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        <span class="type">Properties</span> <span class="variable">props</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Properties</span>();</span><br><span class="line">        props.put(<span class="string">&quot;bootstrap.servers&quot;</span>, <span class="string">&quot;192.168.254.170:9092&quot;</span>);</span><br><span class="line">        props.put(<span class="string">&quot;group.id&quot;</span>, <span class="string">&quot;test&quot;</span>);</span><br><span class="line">        <span class="comment">// 设置手动提交消费offset</span></span><br><span class="line">        props.put(<span class="string">&quot;enable.auto.commit&quot;</span>, <span class="string">&quot;false&quot;</span>);</span><br><span class="line">        props.put(<span class="string">&quot;key.deserializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringDeserializer&quot;</span>);</span><br><span class="line">        props.put(<span class="string">&quot;value.deserializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringDeserializer&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">try</span> (KafkaConsumer&lt;String, String&gt; consumer = <span class="keyword">new</span> <span class="title class_">KafkaConsumer</span>&lt;&gt;(props);) &#123;</span><br><span class="line">            consumer.subscribe(Arrays.asList(<span class="string">&quot;test&quot;</span>, <span class="string">&quot;test-group&quot;</span>));</span><br><span class="line">            <span class="keyword">final</span> <span class="type">int</span> <span class="variable">minBatchSize</span> <span class="operator">=</span> <span class="number">10</span>;</span><br><span class="line">            List&lt;ConsumerRecord&lt;String, String&gt;&gt; buffer = <span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;&gt;();</span><br><span class="line">            <span class="keyword">while</span> (<span class="literal">true</span>) &#123;</span><br><span class="line">                ConsumerRecords&lt;String, String&gt; records = consumer.poll(Duration.ofMillis(<span class="number">200L</span>));</span><br><span class="line">                <span class="comment">// 按分片来处理得到的数据</span></span><br><span class="line">                <span class="keyword">for</span> (TopicPartition partition : records.partitions()) &#123;</span><br><span class="line">                    System.out.println(<span class="string">&quot;************* partition:&quot;</span> + partition);</span><br><span class="line">                    <span class="comment">// 遍历处理分片的数据</span></span><br><span class="line">                    List&lt;ConsumerRecord&lt;String, String&gt;&gt; partitionRecords = records.records(partition);</span><br><span class="line">                    <span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; record : partitionRecords) &#123;</span><br><span class="line">                        System.out.printf(<span class="string">&quot;offset = %d, key = %s, value = %s%n&quot;</span>, record.offset(), record.key(), record.value());</span><br><span class="line">                    &#125;</span><br><span class="line">                    <span class="comment">// 取到的分片的最后一条数据的offset值</span></span><br><span class="line">                    <span class="type">long</span> <span class="variable">lastOffset</span> <span class="operator">=</span> partitionRecords.get(partitionRecords.size() - <span class="number">1</span>).offset();</span><br><span class="line">                    <span class="comment">// 提交该分片的消费offset = lastOffset + 1 .</span></span><br><span class="line">                    consumer.commitSync(Collections.singletonMap(partition, <span class="keyword">new</span> <span class="title class_">OffsetAndMetadata</span>(lastOffset + <span class="number">1</span>)));</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="控制消费的位置"><a href="#控制消费的位置" class="headerlink" title="控制消费的位置"></a>控制消费的位置</h4><p>有两种情况需要控制消费的位置：</p>
<ul>
<li>消费者程序在某个时刻停止了运行，重启后继续消费从那个时刻以来的消息。</li>
<li>消费者在本地可能存储了消费的消息，但这份本地存储坏了，想重新取一份到本地。</li>
</ul>
<p>控制消费位置的情况的下一般使用 指定消费的分片方式来进行消费</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">String</span> <span class="variable">topic</span> <span class="operator">=</span> <span class="string">&quot;foo&quot;</span>; </span><br><span class="line"><span class="type">TopicPartition</span> <span class="variable">partition0</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">TopicPartition</span>(topic, <span class="number">0</span>); </span><br><span class="line"><span class="type">TopicPartition</span> <span class="variable">partition1</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">TopicPartition</span>(topic, <span class="number">1</span>);</span><br><span class="line">consumer.assign(Arrays.asList(partition0, partition1));</span><br></pre></td></tr></table></figure>

<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211121121020.png"></p>
<h3 id="subscribe-vs-assign"><a href="#subscribe-vs-assign" class="headerlink" title="subscribe vs assign"></a>subscribe vs assign</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 不是订阅Topic</span></span><br><span class="line"><span class="comment">// consumer.subscribe(Arrays.asList(&quot;test&quot;, &quot;test-group&quot;));</span></span><br><span class="line"><span class="comment">// 而是直接分配该消费者读取某些分片 subscribe 和 assign 只能用其一，assign 时不受 rebalance影响。</span></span><br><span class="line"><span class="type">TopicPartition</span> <span class="variable">partition</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">TopicPartition</span>(<span class="string">&quot;test&quot;</span>, <span class="number">0</span>);</span><br><span class="line">consumer.assign(Arrays.asList(partition));</span><br></pre></td></tr></table></figure>

<ul>
<li>subscribe ：该消费者消费的是哪些分片是由Kafka根据消费组动态指定，并可以动态rebalance</li>
<li>assign ：则是由用户指定要消费哪些分片，不受rebalance影响。</li>
</ul>
<p>了解 ConsumerRebalanceListener</p>
<h3 id="poll设置"><a href="#poll设置" class="headerlink" title="poll设置"></a>poll设置</h3><p>【思考】 一次拉取，返回多少数据？</p>
<p>可以通过下面的消费者参数来指定。</p>
<table>
<thead>
<tr>
<th>名称</th>
<th>描述</th>
<th>类型</th>
<th>默认值</th>
<th>有效值</th>
<th>重要性</th>
</tr>
</thead>
<tbody><tr>
<td>fetch.min.bytes</td>
<td>The minimum amount of data the server should return for a fetch request. If insufficient data is available the request will wait for that much data to accumulate before answering the request. The default setting of 1 byte means that fetch requests are answered as soon as a single byte of data is available or the fetch request times out waiting for data to arrive. Setting this to something greater than 1 will cause the server to wait for larger amounts of data to accumulate which can improve server throughput a bit at the cost of some additional latency.</td>
<td>int</td>
<td>1</td>
<td>[0,…]</td>
<td>high</td>
</tr>
<tr>
<td>max.partition.fetch.bytes</td>
<td>The maximum amount of data per-partition the server will return. Records are fetched in batches by the consumer. If the first record batch in the first non-empty partition of the fetch is larger than this limit, the batch will still be returned to ensure that the consumer can make progress. The maximum record batch size accepted by the broker is defined via <code>message.max.bytes</code> (broker config) or <code>max.message.bytes</code> (topic config). See fetch.max.bytes for limiting the consumer request size.</td>
<td>int</td>
<td>1048576</td>
<td>[0,…]</td>
<td>high</td>
</tr>
<tr>
<td>fetch.max.bytes</td>
<td>The maximum amount of data the server should return for a fetch request. Records are fetched in batches by the consumer, and if the first record batch in the first non-empty partition of the fetch is larger than this value, the record batch will still be returned to ensure that the consumer can make progress. As such, this is not a absolute maximum. The maximum record batch size accepted by the broker is defined via <code>message.max.bytes</code> (broker config) or <code>max.message.bytes</code> (topic config). Note that the consumer performs multiple fetches in parallel.</td>
<td>int</td>
<td>52428800</td>
<td>[0,…]</td>
<td>medium</td>
</tr>
<tr>
<td>max.poll.records</td>
<td>The maximum number of records returned in a single call to poll().</td>
<td>int</td>
<td>500</td>
<td>[1,…]</td>
<td>medium</td>
</tr>
</tbody></table>
<h3 id="消费的流速控制-Flow-Control"><a href="#消费的流速控制-Flow-Control" class="headerlink" title="消费的流速控制 Flow Control"></a>消费的流速控制 Flow Control</h3><p>当我们给消费者指定从多个分片取数据时，一次poll它会同时从所有指定的分片中拉取数据。但在有些情况下我们可能需要先全速消费指定分片的一个子集，当这个子集只有少量或没有数据时再开始消费其他的分片。</p>
<p>这样的场景如：</p>
<ul>
<li>在流式处理中，程序要对两个Topic中的流数据执行join操作，而一个Topic的生产速度快与另一个，此时就需要降低快的topic的消费速度来匹配慢的。</li>
<li>另一个场景：启动消费者时，已经有大量消息堆积在这些指定的Topic中，程序需要优先处理包含最新数据的topic，再处理老旧数据的topic。</li>
</ul>
<p>Kafka的Consumer支持通过 pause(Collection) 和 resume(Collection) 来动态控制消费流速。</p>
<ul>
<li>pause(Collection partitions) 暂停一个子集的拉取</li>
<li>resume(Collection partitions) 恢复子集的拉取</li>
</ul>
<p>下次调用 poll(Duration) 时它们生效。</p>
<h3 id="事务"><a href="#事务" class="headerlink" title="事务"></a>事务</h3><p>和 生产者使用事务相关。重点就是隔离级别。</p>
<h3 id="消费者的配置参数"><a href="#消费者的配置参数" class="headerlink" title="消费者的配置参数"></a>消费者的配置参数</h3><p><a target="_blank" rel="noopener" href="https://kafka.apachecn.org/documentation.html#newconsumerconfigs">https://kafka.apachecn.org/documentation.html#newconsumerconfigs</a></p>
<p>建议看英文最新版的。</p>
<p><a target="_blank" rel="noopener" href="https://kafka.apache.org/documentation/#consumerconfigs">https://kafka.apache.org/documentation/#consumerconfigs</a></p>
<h3 id="Spring-API-【了解】"><a href="#Spring-API-【了解】" class="headerlink" title="Spring API 【了解】"></a>Spring API 【了解】</h3><ul>
<li>@KafkaListener</li>
<li>@TopicPartition</li>
<li>@PartitionOffset</li>
<li>KafkaListenerErrorHandler</li>
</ul>
<h3 id="ConsumerInterceptor-【知道】"><a href="#ConsumerInterceptor-【知道】" class="headerlink" title="ConsumerInterceptor 【知道】"></a>ConsumerInterceptor 【知道】</h3><p>配置参数：interceptor.classes</p>
<h1 id="Kafka-Streams-process"><a href="#Kafka-Streams-process" class="headerlink" title="Kafka Streams process"></a>Kafka Streams process</h1><h2 id="流式计算说明"><a href="#流式计算说明" class="headerlink" title="流式计算说明"></a>流式计算说明</h2><p>示例：</p>
<ul>
<li>双十一时实时滚动的订单量、成交总金额。</li>
<li>每十分钟的成交额</li>
<li>股票交易看板</li>
</ul>
<p>流式数据 –&gt; 流式计算</p>
<p>流式计算的特点：</p>
<ul>
<li>数据是随时间不断产生的，没有界限，数据是不能变更的。</li>
<li>计算也是不断进行的，是近实时的计算。</li>
<li>计算的结果是不断更新的，每次计算产生最新的结果</li>
</ul>
<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211121123058.png"></p>
<p>Kafka中提供了 <strong>Kafka-Streams</strong> 客户端库，让我们可以非常轻松地编写流式计算程序，来对Kafka集群中存储的流数据进行实时计算、分析处理。</p>
<p><strong>Kafka-Streams的特点：</strong></p>
<ul>
<li>作为一个简单的轻量级客户端库设计，它可以很容易地嵌入到任何Java应用程序中。</li>
<li>除Apache Kafka本身作为内部消息层外，对系统没有外部依赖;值得注意的是，它使用Kafka的分片模型进行水平伸缩处理，同时保持了强大的顺序保证。</li>
<li>支持容错的本地状态，这支持非常快速和高效的有状态操作，比如窗口连接和聚合。</li>
<li>支持精确的一次处理语义，以确保每条记录将被处理一次，且仅被处理一次，即使流客户机或Kafka代理在处理过程中出现故障也是如此。</li>
<li>使用一次一个记录的处理来实现毫秒级的处理延迟，并支持基于事件时间的窗口操作和记录的延迟到达。</li>
<li>提供必要的流处理原语，以及高级流DSL和低级处理器API。</li>
</ul>
<h2 id="一个流式计算程序示例"><a href="#一个流式计算程序示例" class="headerlink" title="一个流式计算程序示例"></a>一个流式计算程序示例</h2><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span> </span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.kafka<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span> </span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>kafka-streams<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span> </span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.3.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span> </span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>【需求】编写一个统计单词在消息总出现次数统计的流计算程序</p>
<p>【思考】 需要做哪些事情来完成单词统计。</p>
<ul>
<li>不断消费消息</li>
<li>分割消息为单词</li>
<li>累计单词的出现次数</li>
<li>每隔一分钟输出统计结果</li>
</ul>
<h2 id="Kafka-Streams-Low-level-processor-API-和-核心概念"><a href="#Kafka-Streams-Low-level-processor-API-和-核心概念" class="headerlink" title="Kafka Streams Low-level processor API 和 核心概念"></a>Kafka Streams Low-level processor API 和 核心概念</h2><h3 id="Processor-处理器"><a href="#Processor-处理器" class="headerlink" title="Processor 处理器"></a>Processor 处理器</h3><p>我们实现Processor接口提供我们的数据处理逻辑。<strong>要掌握的知识点</strong>：</p>
<ol>
<li>掌握Processor的三个方法的用途</li>
<li>掌握ProcessorContext的用途</li>
<li>重新认识Kafka中的数据为什么是Key 、Value 对结构的。</li>
</ol>
<h3 id="Processor-Topology-处理器拓扑结构"><a href="#Processor-Topology-处理器拓扑结构" class="headerlink" title="Processor Topology 处理器拓扑结构"></a>Processor Topology 处理器拓扑结构</h3><p>处理器拓扑结构，即流计算的流程。Kafka-streams API中提供了Topology这个API来让我们组合多个流处理步骤来构成一个复杂的流计算流程。看下面的Topoloy组合示例代码：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">Topology</span> <span class="variable">topology</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Topology</span>(); </span><br><span class="line"></span><br><span class="line">topology.addSource(<span class="string">&quot;SOURCE&quot;</span>, <span class="string">&quot;src-topic&quot;</span>) </span><br><span class="line"><span class="comment">// add &quot;PROCESS1&quot; node which takes the source processor &quot;SOURCE&quot; as its upstream processor </span></span><br><span class="line">.addProcessor(<span class="string">&quot;PROCESS1&quot;</span>, () -&gt; <span class="keyword">new</span> <span class="title class_">MyProcessor1</span>(), <span class="string">&quot;SOURCE&quot;</span>) </span><br><span class="line"></span><br><span class="line"><span class="comment">// add &quot;PROCESS2&quot; node which takes &quot;PROCESS1&quot; as its upstream processor </span></span><br><span class="line">.addProcessor(<span class="string">&quot;PROCESS2&quot;</span>, () -&gt; <span class="keyword">new</span> <span class="title class_">MyProcessor2</span>(), <span class="string">&quot;PROCESS1&quot;</span>) </span><br><span class="line"></span><br><span class="line"><span class="comment">// add &quot;PROCESS3&quot; node which takes &quot;SOURCE&quot; as its upstream processor </span></span><br><span class="line">.addProcessor(<span class="string">&quot;PROCESS3&quot;</span>, () -&gt; <span class="keyword">new</span> <span class="title class_">MyProcessor3</span>(), <span class="string">&quot;SOURCE&quot;</span>) </span><br><span class="line"></span><br><span class="line"><span class="comment">// add the sink processor node &quot;SINK1&quot; that takes Kafka topic &quot;sink-topic1&quot; </span></span><br><span class="line"><span class="comment">// as output and the &quot;PROCESS1&quot; node as its upstream processor </span></span><br><span class="line">.addSink(<span class="string">&quot;SINK1&quot;</span>, <span class="string">&quot;sink-topic1&quot;</span>, <span class="string">&quot;PROCESS1&quot;</span>) </span><br><span class="line"></span><br><span class="line"><span class="comment">// add the sink processor node &quot;SINK2&quot; that takes Kafka topic &quot;sink-topic2&quot; </span></span><br><span class="line"><span class="comment">// as output and the &quot;PROCESS2&quot; node as its upstream processor </span></span><br><span class="line">.addSink(<span class="string">&quot;SINK2&quot;</span>, <span class="string">&quot;sink-topic2&quot;</span>, <span class="string">&quot;PROCESS2&quot;</span>) </span><br><span class="line"></span><br><span class="line"><span class="comment">// add the sink processor node &quot;SINK3&quot; that takes Kafka topic &quot;sink-topic3&quot; </span></span><br><span class="line"><span class="comment">// as output and the &quot;PROCESS3&quot; node as its upstream processor </span></span><br><span class="line">.addSink(<span class="string">&quot;SINK3&quot;</span>, <span class="string">&quot;sink-topic3&quot;</span>, <span class="string">&quot;PROCESS3&quot;</span>);</span><br></pre></td></tr></table></figure>

<p><img src="https://raw.githubusercontent.com/chengchen901/img/master/blog/20211121125000.png"></p>
<p><strong>拓扑结构中有两种特殊的处理器：</strong></p>
<ul>
<li><strong>Source Processor</strong> ： Source Processor 是一种没有前置节点的特殊流处理器。它从一个或者多个 Kafka Topic 消费数据并产出一个输入流给到拓扑结构的后续处理节点。</li>
<li><strong>Sink Processor</strong> ： sink processor 是一种特殊的流处理器，没有处理器需要依赖于它。 它从前置流处理器接收数据并传输给指定的 Kafka Topic</li>
</ul>
<h3 id="State-Store"><a href="#State-Store" class="headerlink" title="State Store"></a>State Store</h3><p>【思考】流处理程序启动后能停吗？</p>
<p>重启时怎么恢复到停前的计算状态？</p>
<p>得要能保存计算过程中的状态（结算的结果，特别是中间环节的计算结果）</p>
<p>【思考】为了使流计算过程能容错，我们需要存储计算状态，那可以存储到哪里呢？</p>
<p>内存、磁盘 、db</p>
<p>存储到本机可靠吗？</p>
<p>如果机器故障了，为了容错，需要能将计算迁移到其他机器上继续，存储到本机就不合适了。</p>
<p>那存到哪里合适？</p>
<p>Topic</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">KeyValueBytesStoreSupplier</span> <span class="variable">countStoreSupplier</span> <span class="operator">=</span> Stores.inMemoryKeyValueStore(<span class="string">&quot;Counts&quot;</span>); StoreBuilder&lt;KeyValueStore&lt;String, Long&gt;&gt; builder = Stores.keyValueStoreBuilder(countStoreSupplier, Serdes.String(), Serdes.Long()); </span><br><span class="line"><span class="comment">// add the count store associated with the WordCountProcessor processor </span></span><br><span class="line"><span class="comment">// 在topoly中关联Processor要使用的state store </span></span><br><span class="line">topology.addStateStore(builder, <span class="string">&quot;Process&quot;</span>)</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.kafka.streams.state.StoreBuilder; </span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.streams.state.Stores; </span><br><span class="line"></span><br><span class="line">StoreBuilder&lt;KeyValueStore&lt;String, Long&gt;&gt; countStoreSupplier = Stores.keyValueStoreBuilder( 		Stores.persistentKeyValueStore(<span class="string">&quot;Counts&quot;</span>), </span><br><span class="line">	Serdes.String(), </span><br><span class="line">	Serdes.Long())</span><br><span class="line">    .withLoggingDisabled(); <span class="comment">// disable backing up the store to a changelog topic</span></span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.kafka.streams.state.StoreBuilder; </span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.streams.state.Stores; </span><br><span class="line"><span class="comment">// 存储 state changelog 的topic的配置 </span></span><br><span class="line">Map&lt;String, String&gt; changelogConfig = <span class="keyword">new</span> <span class="title class_">HashMap</span>(); </span><br><span class="line"><span class="comment">// override min.insync.replicas </span></span><br><span class="line">changelogConfig.put(<span class="string">&quot;min.insyc.replicas&quot;</span>, <span class="string">&quot;1&quot;</span>);</span><br><span class="line"></span><br><span class="line">StoreBuilder&lt;KeyValueStore&lt;String, Long&gt;&gt; countStoreSupplier = Stores.keyValueStoreBuilder( 		Stores.persistentKeyValueStore(<span class="string">&quot;Counts&quot;</span>), </span><br><span class="line">	Serdes.String(), </span><br><span class="line">	Serdes.Long())</span><br><span class="line">	.withLoggingEnabled(changlogConfig); <span class="comment">// enable changelogging, with custom changelog settings</span></span><br></pre></td></tr></table></figure>

<p>容错而记录变更日志 (state stroe 变更日志topic )，默认是开启的。</p>
<p>存储变更日志的topic名称为 流计算应用名-存储名-changelog ，如 my-stream-processing-application-Counts-changelog</p>
<h2 id="DSL-High-Level-API"><a href="#DSL-High-Level-API" class="headerlink" title="DSL High-Level API"></a>DSL High-Level API</h2><p>【思考】流计算一般都计算些什么结果？或做些什么计算？</p>
<p>聚合计算 数据转换</p>
<p>kafka-streams 提供了一种更高级的简便DSL，为我们定义好了很多聚合函数，方面我们快速开发流计算程序。</p>
<p>详细的API</p>
<p>KStream KTable</p>
<p><a target="_blank" rel="noopener" href="http://kafka.apache.org/23/documentation/streams/developer-guide/dsl-api.html">http://kafka.apache.org/23/documentation/streams/developer-guide/dsl-api.html</a></p>
<h1 id="Connect"><a href="#Connect" class="headerlink" title="Connect"></a>Connect</h1><p><a target="_blank" rel="noopener" href="https://kafka.apachecn.org/documentation.html#connect">https://kafka.apachecn.org/documentation.html#connect</a></p>
<p>Kafka Connect 是一款可扩展并且可靠地在 Apache Kafka 和其他系统之间进行数据传输的工具。 可以很简单的快速定义 <em>connectors</em> 将大量数据从 Kafka 移入和移出. Kafka Connect 可以摄取数据库数据或者收集应用程序的 metrics 存储到 Kafka topics，使得数据可以用于低延迟的流处理。 一个导出的 job 可以将来自 Kafka topic 的数据传输到二级存储，用于系统查询或者批量进行离线分析。</p>
<p>Kafka Connect 功能包括:</p>
<ul>
<li><strong>Kafka connectors 通用框架：</strong> - Kafka Connect 将其他数据系统和Kafka集成标准化,简化了 connector 的开发,部署和管理</li>
<li><strong>分布式和单机模式</strong> - 可以扩展成一个集中式的管理服务，也可以单机方便的开发,测试和生产环境小型的部署。</li>
<li><strong>REST 接口</strong> - submit and manage connectors to your Kafka Connect cluster via an easy to use REST API</li>
<li><strong>offset 自动管理</strong> - 只需要connectors 的一些信息，Kafka Connect 可以自动管理offset 提交的过程，因此开发人员无需担心开发中offset提交出错的这部分。</li>
<li><strong>分布式的并且可扩展</strong> - Kafka Connect 构建在现有的 group 管理协议上。Kafka Connect 集群可以扩展添加更多的workers。</li>
<li><strong>整合流处理&#x2F;批处理</strong> - 利用 Kafka 已有的功能，Kafka Connect 是一个桥接stream 和批处理系统理想的方式。</li>
</ul>
<h2 id="运行-Kafka-Connect"><a href="#运行-Kafka-Connect" class="headerlink" title="运行 Kafka Connect"></a>运行 Kafka Connect</h2><p>Kafka Connect 当前支持两种执行方式: 单机 (单个进程) 和 分布式.</p>
<p>在单机模式下所有的工作都是在一个进程中运行的。connect的配置项很容易配置和开始使用，当只有一台机器(worker)的时候也是可用的(例如，收集日志文件到kafka)，但是不利于Kafka Connect 的容错。你可以通过下面的命令启动一个单机进程:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt; bin/connect-standalone.sh config/connect-standalone.properties connector1.properties [connector2.properties ...]</span><br></pre></td></tr></table></figure>

<h3 id="connect-standalone-properties"><a href="#connect-standalone-properties" class="headerlink" title="connect-standalone.properties"></a>connect-standalone.properties</h3><figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Licensed to the Apache Software Foundation (ASF) under one or more</span></span><br><span class="line"><span class="comment"># contributor license agreements.  See the NOTICE file distributed with</span></span><br><span class="line"><span class="comment"># this work for additional information regarding copyright ownership.</span></span><br><span class="line"><span class="comment"># The ASF licenses this file to You under the Apache License, Version 2.0</span></span><br><span class="line"><span class="comment"># (the &quot;License&quot;); you may not use this file except in compliance with</span></span><br><span class="line"><span class="comment"># the License.  You may obtain a copy of the License at</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment">#    http://www.apache.org/licenses/LICENSE-2.0</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># Unless required by applicable law or agreed to in writing, software</span></span><br><span class="line"><span class="comment"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span></span><br><span class="line"><span class="comment"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span></span><br><span class="line"><span class="comment"># See the License for the specific language governing permissions and</span></span><br><span class="line"><span class="comment"># limitations under the License.</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment"># These are defaults. This file just demonstrates how to override some settings.</span></span><br><span class="line"><span class="attr">bootstrap.servers</span>=<span class="string">localhost:9092</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment"># The converters specify the format of data in Kafka and how to translate it into Connect data. Every Connect user will</span></span><br><span class="line"><span class="comment"># need to configure these based on the format they want their data in when loaded from or stored into Kafka</span></span><br><span class="line"><span class="attr">key.converter</span>=<span class="string">org.apache.kafka.connect.json.JsonConverter</span></span><br><span class="line"><span class="attr">value.converter</span>=<span class="string">org.apache.kafka.connect.json.JsonConverter</span></span><br><span class="line"><span class="comment"># Converter-specific settings can be passed in by prefixing the Converter&#x27;s setting with the converter we want to apply</span></span><br><span class="line"><span class="comment"># it to</span></span><br><span class="line"><span class="attr">key.converter.schemas.enable</span>=<span class="string">true</span></span><br><span class="line"><span class="attr">value.converter.schemas.enable</span>=<span class="string">true</span></span><br><span class="line"></span><br><span class="line"><span class="attr">offset.storage.file.filename</span>=<span class="string">/tmp/connect.offsets</span></span><br><span class="line"><span class="comment"># Flush much faster than normal, which is useful for testing/debugging</span></span><br><span class="line"><span class="attr">offset.flush.interval.ms</span>=<span class="string">10000</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment"># Set to a list of filesystem paths separated by commas (,) to enable class loading isolation for plugins</span></span><br><span class="line"><span class="comment"># (connectors, converters, transformations). The list should consist of top level directories that include </span></span><br><span class="line"><span class="comment"># any combination of: </span></span><br><span class="line"><span class="comment"># a) directories immediately containing jars with plugins and their dependencies</span></span><br><span class="line"><span class="comment"># b) uber-jars with plugins and their dependencies</span></span><br><span class="line"><span class="comment"># c) directories immediately containing the package directory structure of classes of plugins and their dependencies</span></span><br><span class="line"><span class="comment"># <span class="doctag">Note:</span> symlinks will be followed to discover dependencies or plugins.</span></span><br><span class="line"><span class="comment"># Examples: </span></span><br><span class="line"><span class="comment"># plugin.path=/usr/local/share/java,/usr/local/share/kafka/plugins,/opt/connectors,</span></span><br><span class="line"><span class="comment">#plugin.path=</span></span><br></pre></td></tr></table></figure>

<p>第一个参数是 worker 的配置文件. 其中包括 Kafka connection 参数，序列化格式，和如何频繁的提交offsets。 所提供的示例可以在本地良好的运行，使用默认提供的配置 <code>config/server.properties</code> 。它需要调整以配合不同的配置或生产环境部署。所有的workers（独立和分布式）都需要一些配置 :</p>
<ul>
<li><code>bootstrap.servers</code> - List of Kafka servers used to bootstrap connections to Kafka</li>
<li><code>key.converter</code> - Converter class used to convert between Kafka Connect format and the serialized form that is written to Kafka. This controls the format of the keys in messages written to or read from Kafka, and since this is independent of connectors it allows any connector to work with any serialization format. Examples of common formats include JSON and Avro.</li>
<li><code>value.converter</code> - Converter class used to convert between Kafka Connect format and the serialized form that is written to Kafka. This controls the format of the values in messages written to or read from Kafka, and since this is independent of connectors it allows any connector to work with any serialization format. Examples of common formats include JSON and Avro.</li>
</ul>
<p>单机模式的重要配置如下:</p>
<ul>
<li><code>offset.storage.file.filename</code> - 存储 offset 数据的文件</li>
</ul>
<p>此处配置的参数适用于由Kafka Connect使用的 producer 和 consumer 访问配置，offset 和 status topic。对于 Kafka source和 sink 任务的配置，可以使用相同的参数，但必须以<code>consumer.</code> 和 <code>producer.</code> 作为前缀。 此外，从 worker 配置中继承的参数只有一个，就是 <code>bootstrap.servers</code>。大多数情况下，这是足够的，因为同一个集群通常用于所有的场景。但是需要注意的是一个安全集群，需要额外的参数才能允许连接。这些参数需要在 worker 配置中设置三次，一次用于管理访问，一次用于 Kafka sinks，还有一次用于 Kafka source。</p>
<p>其余参数用于 connector 的配置文件，你可以导入尽可能多的配置文件，但是所有的配置文件都将在同一个进程内(在不同的线程上)执行。</p>
<p>分布式模式下会自动进行负载均衡，允许动态的扩缩容，并提供对 active task，以及这个任务对应的配置和offset提交记录的容错。分布式执行方式和单机模式非常相似:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt; bin/connect-distributed.sh config/connect-distributed.properties</span><br></pre></td></tr></table></figure>

<h3 id="connect-distributed-properties"><a href="#connect-distributed-properties" class="headerlink" title="connect-distributed.properties"></a>connect-distributed.properties</h3><figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">##</span></span><br><span class="line"><span class="comment"># Licensed to the Apache Software Foundation (ASF) under one or more</span></span><br><span class="line"><span class="comment"># contributor license agreements.  See the NOTICE file distributed with</span></span><br><span class="line"><span class="comment"># this work for additional information regarding copyright ownership.</span></span><br><span class="line"><span class="comment"># The ASF licenses this file to You under the Apache License, Version 2.0</span></span><br><span class="line"><span class="comment"># (the &quot;License&quot;); you may not use this file except in compliance with</span></span><br><span class="line"><span class="comment"># the License.  You may obtain a copy of the License at</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment">#    http://www.apache.org/licenses/LICENSE-2.0</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># Unless required by applicable law or agreed to in writing, software</span></span><br><span class="line"><span class="comment"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span></span><br><span class="line"><span class="comment"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span></span><br><span class="line"><span class="comment"># See the License for the specific language governing permissions and</span></span><br><span class="line"><span class="comment"># limitations under the License.</span></span><br><span class="line"><span class="comment">##</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment"># This file contains some of the configurations for the Kafka Connect distributed worker. This file is intended</span></span><br><span class="line"><span class="comment"># to be used with the examples, and some settings may differ from those used in a production system, especially</span></span><br><span class="line"><span class="comment"># the `bootstrap.servers` and those specifying replication factors.</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment"># A list of host/port pairs to use for establishing the initial connection to the Kafka cluster.</span></span><br><span class="line"><span class="attr">bootstrap.servers</span>=<span class="string">localhost:9092</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment"># unique name for the cluster, used in forming the Connect cluster group. Note that this must not conflict with consumer group IDs</span></span><br><span class="line"><span class="attr">group.id</span>=<span class="string">connect-cluster</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment"># The converters specify the format of data in Kafka and how to translate it into Connect data. Every Connect user will</span></span><br><span class="line"><span class="comment"># need to configure these based on the format they want their data in when loaded from or stored into Kafka</span></span><br><span class="line"><span class="attr">key.converter</span>=<span class="string">org.apache.kafka.connect.json.JsonConverter</span></span><br><span class="line"><span class="attr">value.converter</span>=<span class="string">org.apache.kafka.connect.json.JsonConverter</span></span><br><span class="line"><span class="comment"># Converter-specific settings can be passed in by prefixing the Converter&#x27;s setting with the converter we want to apply</span></span><br><span class="line"><span class="comment"># it to</span></span><br><span class="line"><span class="attr">key.converter.schemas.enable</span>=<span class="string">true</span></span><br><span class="line"><span class="attr">value.converter.schemas.enable</span>=<span class="string">true</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment"># Topic to use for storing offsets. This topic should have many partitions and be replicated and compacted.</span></span><br><span class="line"><span class="comment"># Kafka Connect will attempt to create the topic automatically when needed, but you can always manually create</span></span><br><span class="line"><span class="comment"># the topic before starting Kafka Connect if a specific topic configuration is needed.</span></span><br><span class="line"><span class="comment"># Most users will want to use the built-in default replication factor of 3 or in some cases even specify a larger value.</span></span><br><span class="line"><span class="comment"># Since this means there must be at least as many brokers as the maximum replication factor used, we&#x27;d like to be able</span></span><br><span class="line"><span class="comment"># to run this example on a single-broker cluster and so here we instead set the replication factor to 1.</span></span><br><span class="line"><span class="attr">offset.storage.topic</span>=<span class="string">connect-offsets</span></span><br><span class="line"><span class="attr">offset.storage.replication.factor</span>=<span class="string">1</span></span><br><span class="line"><span class="comment">#offset.storage.partitions=25</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment"># Topic to use for storing connector and task configurations; note that this should be a single partition, highly replicated,</span></span><br><span class="line"><span class="comment"># and compacted topic. Kafka Connect will attempt to create the topic automatically when needed, but you can always manually create</span></span><br><span class="line"><span class="comment"># the topic before starting Kafka Connect if a specific topic configuration is needed.</span></span><br><span class="line"><span class="comment"># Most users will want to use the built-in default replication factor of 3 or in some cases even specify a larger value.</span></span><br><span class="line"><span class="comment"># Since this means there must be at least as many brokers as the maximum replication factor used, we&#x27;d like to be able</span></span><br><span class="line"><span class="comment"># to run this example on a single-broker cluster and so here we instead set the replication factor to 1.</span></span><br><span class="line"><span class="attr">config.storage.topic</span>=<span class="string">connect-configs</span></span><br><span class="line"><span class="attr">config.storage.replication.factor</span>=<span class="string">1</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment"># Topic to use for storing statuses. This topic can have multiple partitions and should be replicated and compacted.</span></span><br><span class="line"><span class="comment"># Kafka Connect will attempt to create the topic automatically when needed, but you can always manually create</span></span><br><span class="line"><span class="comment"># the topic before starting Kafka Connect if a specific topic configuration is needed.</span></span><br><span class="line"><span class="comment"># Most users will want to use the built-in default replication factor of 3 or in some cases even specify a larger value.</span></span><br><span class="line"><span class="comment"># Since this means there must be at least as many brokers as the maximum replication factor used, we&#x27;d like to be able</span></span><br><span class="line"><span class="comment"># to run this example on a single-broker cluster and so here we instead set the replication factor to 1.</span></span><br><span class="line"><span class="attr">status.storage.topic</span>=<span class="string">connect-status</span></span><br><span class="line"><span class="attr">status.storage.replication.factor</span>=<span class="string">1</span></span><br><span class="line"><span class="comment">#status.storage.partitions=5</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment"># Flush much faster than normal, which is useful for testing/debugging</span></span><br><span class="line"><span class="attr">offset.flush.interval.ms</span>=<span class="string">10000</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment"># These are provided to inform the user about the presence of the REST host and port configs </span></span><br><span class="line"><span class="comment"># Hostname &amp; Port for the REST API to listen on. If this is set, it will bind to the interface used to listen to requests.</span></span><br><span class="line"><span class="comment">#rest.host.name=</span></span><br><span class="line"><span class="comment">#rest.port=8083</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment"># The Hostname &amp; Port that will be given out to other workers to connect to i.e. URLs that are routable from other servers.</span></span><br><span class="line"><span class="comment">#rest.advertised.host.name=</span></span><br><span class="line"><span class="comment">#rest.advertised.port=</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment"># Set to a list of filesystem paths separated by commas (,) to enable class loading isolation for plugins</span></span><br><span class="line"><span class="comment"># (connectors, converters, transformations). The list should consist of top level directories that include </span></span><br><span class="line"><span class="comment"># any combination of: </span></span><br><span class="line"><span class="comment"># a) directories immediately containing jars with plugins and their dependencies</span></span><br><span class="line"><span class="comment"># b) uber-jars with plugins and their dependencies</span></span><br><span class="line"><span class="comment"># c) directories immediately containing the package directory structure of classes of plugins and their dependencies</span></span><br><span class="line"><span class="comment"># Examples: </span></span><br><span class="line"><span class="comment"># plugin.path=/usr/local/share/java,/usr/local/share/kafka/plugins,/opt/connectors,</span></span><br><span class="line"><span class="comment">#plugin.path=</span></span><br></pre></td></tr></table></figure>

<p>和单机模式不同在于启动的实现类和决定 Kafka connect 进程如何工作的配置参数，如何分配 work,offsets 存储在哪里和任务状态。在分布式模式中，Kafka Connect 存储 offsets,配置和存储在 Kafka topic中的任务状态。建议手动创建Kafka 的 offsets,配置和状态，以实现自己所期望的分区数和备份因子。如果启动Kafka Connect之前没有创建 topic，则会使用默认分区数和复制因子自动创建创建 topic，但这可能不是最适合的。</p>
<p>特别是，除了上面提到的常用设置之外，以下配置参数在启动集群之前至关重要:</p>
<ul>
<li><code>group.id</code> (default <code>connect-cluster</code>) - unique name for the cluster, used in forming the Connect cluster group; note that this <strong>must not conflict</strong> with consumer group IDs</li>
<li><code>config.storage.topic</code> (default <code>connect-configs</code>) - topic to use for storing connector and task configurations; note that this should be a single partition, highly replicated, compacted topic. You may need to manually create the topic to ensure the correct configuration as auto created topics may have multiple partitions or be automatically configured for deletion rather than compaction</li>
<li><code>offset.storage.topic</code> (default <code>connect-offsets</code>) - topic to use for storing offsets; this topic should have many partitions, be replicated, and be configured for compaction</li>
<li><code>status.storage.topic</code> (default <code>connect-status</code>) - topic to use for storing statuses; this topic can have multiple partitions, and should be replicated and configured for compaction</li>
</ul>
<p>注意在分布式模式下 connector 配置不会通过命令行传递。相反，会使用下面提到的 REST API来创建，修改和销毁 connectors。</p>
<h2 id="Configuring-Connectors"><a href="#Configuring-Connectors" class="headerlink" title="Configuring Connectors"></a>Configuring Connectors</h2><p>Connector 配置是简单的key-value 映射的格式。对于单机模式，这些配置会在 properties 文件中定义，并通过命令行传递给 Connect 进程。在分布式模式中，它们将被包含在创建（或修改）connector 的请求的JSON格式串中。</p>
<p>大多数配置都依赖于 connectors,所以在这里不能概述。但是，有几个常见选项可以看一下:</p>
<ul>
<li><code>name</code> - Unique name for the connector. Attempting to register again with the same name will fail.</li>
<li><code>connector.class</code> - The Java class for the connector</li>
<li><code>tasks.max</code> - The maximum number of tasks that should be created for this connector. The connector may create fewer tasks if it cannot achieve this level of parallelism.</li>
<li><code>key.converter</code> - (optional) Override the default key converter set by the worker.</li>
<li><code>value.converter</code> - (optional) Override the default value converter set by the worker.</li>
</ul>
<p><code>connector.class</code> 配置支持多种名称格式：这个 connector class 的全名或者别名。如果 connector 是 org.apache.kafka.connect.file.FileStreamSinkConnector，则可以指定全名，也可以使用FileStreamSink 或 FileStreamSinkConnector 来简化配置。</p>
<p>Sink connectors 还有一个额外的选项来控制他的输出:</p>
<ul>
<li><code>topics</code> - A list of topics to use as input for this connector</li>
</ul>
<p>对于任何其他选项，你应该查阅 connector的文档.</p>
<h2 id="Transformations"><a href="#Transformations" class="headerlink" title="Transformations"></a>Transformations</h2><p>connectors可以配置 transformations 操作，实现轻量级的消息单次修改，他们可以方便地用于数据修改和事件路由。</p>
<p>A transformation chain 可以在connector 配置中指定。</p>
<ul>
<li><code>transforms</code> - List of aliases for the transformation, specifying the order in which the transformations will be applied.</li>
<li><code>transforms.$alias.type</code> - Fully qualified class name for the transformation.</li>
<li><code>transforms.$alias.$transformationSpecificConfig</code> Configuration properties for the transformation</li>
</ul>
<p>例如，让我们使用内置的 file soucre connector，并使用 transformation 来添加静态字段。</p>
<p>这个例子中，我们会使用 schemaless json 数据格式。为了使用 schemaless 格式，我们将 <code>connect-standalone.properties</code> 文件中下面两行从true改成false:</p>
<figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">key.converter.schemas.enable</span></span><br><span class="line"><span class="attr">value.converter.schemas.enable</span></span><br></pre></td></tr></table></figure>

<p>这个 file source connector 读取每行数据作为一个字符串。我们会将每行数据包装进一个 Map 数据结构,然后添加一个二级字段来标识事件的来源。做这样一个操作，我们使用两个 transformations:</p>
<ul>
<li><strong>HoistField</strong> to place the input line inside a Map</li>
<li><strong>InsertField</strong> to add the static field. In this example we’ll indicate that the record came from a file connector</li>
</ul>
<p>添加完 transformations, <code>connect-file-source.properties</code> 文件像下面这样:</p>
<figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">name</span>=<span class="string">local-file-source</span></span><br><span class="line"><span class="attr">connector.class</span>=<span class="string">FileStreamSource</span></span><br><span class="line"><span class="attr">tasks.max</span>=<span class="string">1</span></span><br><span class="line"><span class="attr">file</span>=<span class="string">test.txt</span></span><br><span class="line"><span class="attr">topic</span>=<span class="string">connect-test</span></span><br><span class="line"><span class="attr">transforms</span>=<span class="string">MakeMap, InsertSource</span></span><br><span class="line"><span class="attr">transforms.MakeMap.type</span>=<span class="string">org.apache.kafka.connect.transforms.HoistField$Value</span></span><br><span class="line"><span class="attr">transforms.MakeMap.field</span>=<span class="string">line</span></span><br><span class="line"><span class="attr">transforms.InsertSource.type</span>=<span class="string">org.apache.kafka.connect.transforms.InsertField$Value</span></span><br><span class="line"><span class="attr">transforms.InsertSource.static.field</span>=<span class="string">data_source</span></span><br><span class="line"><span class="attr">transforms.InsertSource.static.value</span>=<span class="string">test-file-source</span></span><br></pre></td></tr></table></figure>

<p>所有以<code>transforms</code> 为开头的行都将被添加了静态字段用于 transformations 。 你可以看到我们创建的两个 transformations: “InsertSource” 和 “MakeMap” 是我们给的 transformations 的别称. transformation 类型基于下面给的一系列内嵌 transformations。每个 transformation 类型都有额外的配置: HoistField 需要一个配置叫做 “field”,这是 map中原始字符串的字段名称。InsertField transformation 让我们指定字段名称和我们要添加的内容。</p>
<p>当我们对一个 sample file 运行 file source connector 操作，不做transformations 操作，然后使用<code>kafka-console-consumer.sh</code> 读取数据，结果如下:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&quot;foo&quot;</span><br><span class="line">&quot;bar&quot;</span><br><span class="line">&quot;hello world&quot;</span><br></pre></td></tr></table></figure>

<p>然后我们创建一个新的file connector,然后将这个transformations 添加到配置文件中。这次结果如下:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&#123;&quot;line&quot;:&quot;foo&quot;,&quot;data_source&quot;:&quot;test-file-source&quot;&#125;</span><br><span class="line">&#123;&quot;line&quot;:&quot;bar&quot;,&quot;data_source&quot;:&quot;test-file-source&quot;&#125;</span><br><span class="line">&#123;&quot;line&quot;:&quot;hello world&quot;,&quot;data_source&quot;:&quot;test-file-source&quot;&#125;</span><br></pre></td></tr></table></figure>

<p>你可以看到我们读取的行现在JSON map的一部分，并且还有一个静态值的额外字段。这只是用 transformations 做的一个简单的例子。</p>
<p>Kafka Connect 包含几个广泛适用的数据和 routing transformations</p>
<ul>
<li>InsertField - Add a field using either static data or record metadata</li>
<li>ReplaceField - Filter or rename fields</li>
<li>MaskField - Replace field with valid null value for the type (0, empty string, etc)</li>
<li>ValueToKey</li>
<li>HoistField - Wrap the entire event as a single field inside a Struct or a Map</li>
<li>ExtractField - Extract a specific field from Struct and Map and include only this field in results</li>
<li>SetSchemaMetadata - modify the schema name or version</li>
<li>TimestampRouter - Modify the topic of a record based on original topic and timestamp. Useful when using a sink that needs to write to different tables or indexes based on timestamps</li>
<li>RegexRouter - modify the topic of a record based on original topic, replacement string and a regular expression</li>
</ul>
<h2 id="REST-API"><a href="#REST-API" class="headerlink" title="REST API"></a>REST API</h2><p>由于Kafka Connect 旨在作为服务运行，它还提供了一个用于管理 connectors 的REST API。默认情况下，此服务在端口8083上运行。以下是当前支持的功能:</p>
<ul>
<li><code>GET /connectors</code> - return a list of active connectors</li>
<li><code>POST /connectors</code> - create a new connector; the request body should be a JSON object containing a string <code>name</code> field and an object <code>config</code> field with the connector configuration parameters</li>
<li><code>GET /connectors/&#123;name&#125;</code> - get information about a specific connector</li>
<li><code>GET /connectors/&#123;name&#125;/config</code> - get the configuration parameters for a specific connector</li>
<li><code>PUT /connectors/&#123;name&#125;/config</code> - update the configuration parameters for a specific connector</li>
<li><code>GET /connectors/&#123;name&#125;/status</code> - get current status of the connector, including if it is running, failed, paused, etc., which worker it is assigned to, error information if it has failed, and the state of all its tasks</li>
<li><code>GET /connectors/&#123;name&#125;/tasks</code> - get a list of tasks currently running for a connector</li>
<li><code>GET /connectors/&#123;name&#125;/tasks/&#123;taskid&#125;/status</code> - get current status of the task, including if it is running, failed, paused, etc., which worker it is assigned to, and error information if it has failed</li>
<li><code>PUT /connectors/&#123;name&#125;/pause</code> - pause the connector and its tasks, which stops message processing until the connector is resumed</li>
<li><code>PUT /connectors/&#123;name&#125;/resume</code> - resume a paused connector (or do nothing if the connector is not paused)</li>
<li><code>POST /connectors/&#123;name&#125;/restart</code> - restart a connector (typically because it has failed)</li>
<li><code>POST /connectors/&#123;name&#125;/tasks/&#123;taskId&#125;/restart</code> - restart an individual task (typically because it has failed)</li>
<li><code>DELETE /connectors/&#123;name&#125;</code> - delete a connector, halting all tasks and deleting its configuration</li>
</ul>
<p>Kafka Connect还提供用于获取有关 connector plugin 信息的REST API:</p>
<ul>
<li><code>GET /connector-plugins</code>- return a list of connector plugins installed in the Kafka Connect cluster. Note that the API only checks for connectors on the worker that handles the request, which means you may see inconsistent results, especially during a rolling upgrade if you add new connector jars</li>
<li><code>PUT /connector-plugins/&#123;connector-type&#125;/config/validate</code> - validate the provided configuration values against the configuration definition. This API performs per config validation, returns suggested values and error messages during validation.</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2020/11/29/08-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/03-Kafka/01-Kafka%E5%85%A5%E9%97%A8/" data-id="clmcxed2b00b8u8wa6x2u6gf0" data-title="Kafka入门" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Kafka/" rel="tag">Kafka</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2020/11/29/08-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/02-RabbitMQ/06-RabbitMQ%E5%8D%95%E6%9C%BA%E5%88%B0%E9%9B%86%E7%BE%A4%E5%AE%8C%E6%95%B4%E6%90%AD%E5%BB%BA/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          RabbitMQ单机到集群完整搭建
        
      </div>
    </a>
  
  
    <a href="/2020/11/15/08-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/02-RabbitMQ/05-MQ%E5%BA%94%E7%94%A8-%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">MQ应用-分布式事务</div>
    </a>
  
</nav>

  
</article>


</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/ActiveMQ/">ActiveMQ</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Docker/">Docker</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Dubbo/">Dubbo</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Golang/">Golang</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Java%E5%9F%BA%E7%A1%80/">Java基础</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/K8s/">K8s</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Kafka/">Kafka</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/MQ/">MQ</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/MySQL/">MySQL</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Mybatis/">Mybatis</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Netty/">Netty</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/RPC/">RPC</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/RabbitMQ/">RabbitMQ</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Redis/">Redis</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Socker/">Socker</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Spring/">Spring</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/SpringBoot/">SpringBoot</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/SpringCloud/">SpringCloud</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Tomcat/">Tomcat</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/ZooKeeper/">ZooKeeper</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Zookeeper/">Zookeeper</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/">大数据</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/">数据结构</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E9%9D%A2%E8%AF%95%E9%A2%98/">面试题</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/ActiveMQ/" rel="tag">ActiveMQ</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/CDH/" rel="tag">CDH</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Docker/" rel="tag">Docker</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Dubbo/" rel="tag">Dubbo</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Golang/" rel="tag">Golang</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/HDFS/" rel="tag">HDFS</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/K8s/" rel="tag">K8s</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/KDTree/" rel="tag">KDTree</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Kafka/" rel="tag">Kafka</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/MQ/" rel="tag">MQ</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/MapReduce/" rel="tag">MapReduce</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/MySQL/" rel="tag">MySQL</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Mybatis/" rel="tag">Mybatis</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Netty/" rel="tag">Netty</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/RPC/" rel="tag">RPC</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/RabbitMQ/" rel="tag">RabbitMQ</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Redis/" rel="tag">Redis</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Socker/" rel="tag">Socker</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Spark/" rel="tag">Spark</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Spring/" rel="tag">Spring</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/SpringBoot/" rel="tag">SpringBoot</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/SpringCloud/" rel="tag">SpringCloud</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Tomcat/" rel="tag">Tomcat</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/YARN/" rel="tag">YARN</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ZooKeeper/" rel="tag">ZooKeeper</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%A4%9A%E7%BA%BF%E7%A8%8B/" rel="tag">多线程</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%B3%A8%E8%A7%A3/" rel="tag">注解</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/" rel="tag">设计模式</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E9%9B%86%E5%90%88%E6%BA%90%E7%A0%81/" rel="tag">集合源码</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E9%9D%A2%E8%AF%95%E9%A2%98/" rel="tag">面试题</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/ActiveMQ/" style="font-size: 10px;">ActiveMQ</a> <a href="/tags/CDH/" style="font-size: 10px;">CDH</a> <a href="/tags/Docker/" style="font-size: 18px;">Docker</a> <a href="/tags/Dubbo/" style="font-size: 13px;">Dubbo</a> <a href="/tags/Golang/" style="font-size: 19px;">Golang</a> <a href="/tags/HDFS/" style="font-size: 10px;">HDFS</a> <a href="/tags/K8s/" style="font-size: 10px;">K8s</a> <a href="/tags/KDTree/" style="font-size: 10px;">KDTree</a> <a href="/tags/Kafka/" style="font-size: 10px;">Kafka</a> <a href="/tags/MQ/" style="font-size: 10px;">MQ</a> <a href="/tags/MapReduce/" style="font-size: 10px;">MapReduce</a> <a href="/tags/MySQL/" style="font-size: 10px;">MySQL</a> <a href="/tags/Mybatis/" style="font-size: 14px;">Mybatis</a> <a href="/tags/Netty/" style="font-size: 14px;">Netty</a> <a href="/tags/RPC/" style="font-size: 11px;">RPC</a> <a href="/tags/RabbitMQ/" style="font-size: 15px;">RabbitMQ</a> <a href="/tags/Redis/" style="font-size: 17px;">Redis</a> <a href="/tags/Socker/" style="font-size: 10px;">Socker</a> <a href="/tags/Spark/" style="font-size: 11px;">Spark</a> <a href="/tags/Spring/" style="font-size: 20px;">Spring</a> <a href="/tags/SpringBoot/" style="font-size: 14px;">SpringBoot</a> <a href="/tags/SpringCloud/" style="font-size: 10px;">SpringCloud</a> <a href="/tags/Tomcat/" style="font-size: 13px;">Tomcat</a> <a href="/tags/YARN/" style="font-size: 11px;">YARN</a> <a href="/tags/ZooKeeper/" style="font-size: 12px;">ZooKeeper</a> <a href="/tags/%E5%A4%9A%E7%BA%BF%E7%A8%8B/" style="font-size: 16px;">多线程</a> <a href="/tags/%E6%B3%A8%E8%A7%A3/" style="font-size: 10px;">注解</a> <a href="/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/" style="font-size: 11px;">设计模式</a> <a href="/tags/%E9%9B%86%E5%90%88%E6%BA%90%E7%A0%81/" style="font-size: 13px;">集合源码</a> <a href="/tags/%E9%9D%A2%E8%AF%95%E9%A2%98/" style="font-size: 10px;">面试题</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/01/">January 2022</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/12/">December 2021</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/05/">May 2021</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/04/">April 2021</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/03/">March 2021</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/02/">February 2021</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/01/">January 2021</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/12/">December 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/11/">November 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/10/">October 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/08/">August 2020</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2023/09/10/11-MySQL/04-MySQL%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%8A%E8%AF%BB%E5%86%99%E5%88%86%E7%A6%BB/05-%E9%AB%98%E5%8F%AF%E7%94%A8%E6%95%B0%E6%8D%AE%E5%BA%93%E9%9B%86%E7%BE%A4%E5%AE%89%E8%A3%85%E6%90%AD%E5%BB%BA%E6%89%8B%E5%86%8C/">(no title)</a>
          </li>
        
          <li>
            <a href="/2023/09/10/11-MySQL/04-MySQL%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%8A%E8%AF%BB%E5%86%99%E5%88%86%E7%A6%BB/04-Sharding-JDBC%E8%AF%BB%E5%86%99%E5%88%86%E7%A6%BB/">(no title)</a>
          </li>
        
          <li>
            <a href="/2023/09/10/11-MySQL/04-MySQL%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%8A%E8%AF%BB%E5%86%99%E5%88%86%E7%A6%BB/03-MyCat%E8%AF%BB%E5%86%99%E5%88%86%E7%A6%BB/">(no title)</a>
          </li>
        
          <li>
            <a href="/2023/09/10/11-MySQL/04-MySQL%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%8A%E8%AF%BB%E5%86%99%E5%88%86%E7%A6%BB/02-MySQL%E4%B8%BB%E4%BB%8E%E5%A4%8D%E5%88%B6/">(no title)</a>
          </li>
        
          <li>
            <a href="/2023/09/10/11-MySQL/04-MySQL%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%8A%E8%AF%BB%E5%86%99%E5%88%86%E7%A6%BB/01-MySQL%E4%B8%BB%E4%BB%8E%E5%A4%8D%E5%88%B6%E5%92%8C%E9%AB%98%E5%8F%AF%E7%94%A8/">(no title)</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2023 John Doe<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.6.4.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
</body>
</html>